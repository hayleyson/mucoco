import logging
import math
import os
import sys
import re
import torch
import numpy as np
import pandas as pd
import transformers
import gc
import time
import json
import os
import joblib

from transformers import AutoTokenizer, AutoConfig
from sentence_transformers import SentenceTransformer # lib for embeddings

from mucoco.utils import TargetProbability, TargetEmbeddings, TargetSimplex, Lambda, Optimizer, OptimizerLE, get_epsilon
import mucoco.losses as lossbuilder
import mucoco.options as options
import mucoco.utils as utils
import torch.nn.functional as F

def set_global_logging_level(level=logging.ERROR, prefices=[""]):
    
    """
    Override logging levels of different modules based on their name as a prefix.
    """
    prefix_re = re.compile(fr'^(?:{ "|".join(prefices) })') ## ì •í™•íˆ ì–´ë–¤ ê±¸ ë§¤ì¹­í•˜ëŠ”ì§€ ì˜ ëª¨ë¥´ê² ëŠ”ë°...
    for name in logging.root.manager.loggerDict:
        if re.match(prefix_re, name):
            logging.getLogger(name).setLevel(level)

def sentence_completion(prompt, tokens, lossfn):
    
    # max output lengthë¥¼ 10ê°œ ëŠ˜ë¦°ë‹¤.
    lossfn.args.max_output_length = lossfn.args.max_output_length + 10
    
    # ì™ ì§€ëŠ” ëª¨ë¥´ê² ì§€ë§Œ ã…  promptë‘ tokensë¥¼ ë¶™ì¸ ë‹¤ìŒì— ë‚˜ë¨¸ì§€ë¥¼ ìƒì„±í•˜ê²Œ í•¨ ... í .... promptì™€ token ë’¤ì— 10ê°œë¥¼ ë” ìƒì„±í•˜ë¼ ì´ëŸ°ê±´ê°€?
    new_tokens = lossfn.generate(torch.cat([prompt, torch.LongTensor([tokens]).to(lossfn.device)]))
    
    # ë‹¤ì‹œ max_output_lengthë¥¼ ì›ë³µí•œë‹¤.
    lossfn.args.max_output_length = lossfn.args.max_output_legnth - 10

    return tokens + new_tokens[0].tolist()
    
def clean_output(tokens, eos_token_id, return_tensors=False, allow_first_eos=False, skip_special_tokens=[], prompt=None, sentence_complete=False, lossfn=None):
    """
    skip_special_tokensë¥¼ ì œì™¸í•œ tokenë§Œ ë¦¬í„´.
    eosë¡œ ì‹œì‘í•˜ë©´ []ë¥¼ ë¦¬í„´
    """
    
    if sentence_complete: # ??? ì–´ë–¨ ë•Œ ì“°ëŠ” ì˜µì…˜ì¼ê¹Œ?
        tokens = sentence_completion(prompt, tokens, lossfn)
    new_tokens = []
    for i, tok in enumerate(tokens):
        if tok == eos_token_id and (not allow_first_eos or i > 0):
            break # ë§Œì•½ì— ì²«ë²ˆì§¸ í† í°ì´ eos ì´ë©´ ê·¸ëƒ¥ [] ë¥¼ ë¦¬í„´
        
        if (tok not in skip_special_tokens): # special_tokenì´ ì•„ë‹Œê²ƒë§Œ ê³¨ë¼ë‚´ê² ë‹¤ê°€ ê³¨ìì„.
            new_tokens.append(tok)
            
    if return_tensors:
        return torch.LongTensor([new_tokens])
    return new_tokens
    
def main(args):
    
    logging.basicConfig(
        format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        level=logging.ERROR,
        stream=sys.stdout,
    )
    logger = logging.getLogger("mucoco")
    logger.setLevel(logging.ERROR)
    logger.info(args)
    
    if args.outfile is not None:
        outf = open(args.outfile, "w")
        outallsatf = open(args.outfile + ".allsat", "w")
        outf2 = open(args.outfile + ".intermediate", "w")
        
    if args.seed is not None:
        np.random.seed(args.seed)
        torch.manual_seed(args.seed)
        
    use_cuda = torch.cuda.is_available() and not args.cpu
    
    name2tokenizer = {}
    name2model = {}
    name2config = {}
    loss2modelname = {}
    loss2tokenizer = {}
    embed_luts = []
    embed_scales = []
    
    betas = []
    model_paths = args.model.split(":")
    tokenizer_paths = args.tokenizer.split(":")
    
    cur_lr = args.lr
    args.jsonl_tokenized = args.jsonl_tokenized == "true"
    
    if args.model_types is not None:
        model_types = args.model_types.split(":")
    else:
        model_types = [AutoModel for _ in model_paths]
    
    losses = args.loss.split(":")
    if args.lossabbr is not None:
        lossabbr = args.lossabbr.split(":")
    else:
        lossabbr = [x for x in losses]
        
    if args.label_id is None or args.label_id == "none":
        label_ids = [1 for _ in losses]
    else:
        label_ids = [int(i) for i in args.label_id.split(":")]
    
    # ??? ì™œ ë‚˜ì¤‘ì— ì–´ì§œí”¼ ë‹¤ì‹œ ì„¤ì •í• ê±°ë¥¼ ì—¬ê¸°ì„œ ì´ë ‡ê²Œ í•˜ëŠ”ê±¸ê¹Œ?...
    # ê·¸ëƒ¥ initializeí•˜ëŠ”ê±°ë¼ê³  ìƒê°í•˜ë©´ ë ê¹Œ??ã… .ã… 
    if args.keywords is None or args.keywords == "none":
        keywords = ["the" for _ in losses]
    elif args.keywords in ["_roc_", "_commongen_", "_commongenunique_"]:
        keywords = ["" for _ in losses]
    else:
        keywords = args.keywords.split(":")
        if len(keywords) == 1:
            keywords = [f"_topic_:{args.keywords[0]}" for _ in losses]

    # options related to how to select final output.
    # what's beta? ë‚´ ì˜ˆìƒìœ¼ë¡œëŠ”,,, lambda? nontoxic ì¼ë•Œì˜ 
    # nontoxicì¼ ë•Œ selection_criterion='mrr_allsat' ì„, betas='0.8:0.2' ì„.
    # !!! ê·¸ëŸ¬ë©´ ê²°êµ­ì—ëŠ” betas = [1.0, 0.0] ì¸ê±°ì„.
    if "allsat" in args.selection_criterion: # allsat = all satisfied? 
        # with this flag, the output which minimized the primary objective while satisfying all objectives is selected. In case all constraints are not satisfied (e.g when constraints are competing or optimization fails), this will predict the default output (Using an autoregressive decoding setup: beam search in this case)
        betas = [1.0] + [0.0 for _ in range(len(losses)-1)]
    elif (args.selection_criterion == "weighted_sum" and args.betas is not None) or (args.selection_criterion == "last"):
        betas = [float(beta) for beta in args.betas.split(":")]
    else:
        raise ValueError("correct selection_criterion or betas needs to be specified")
    
    # omit assert statemments
    
    prev_vocab_size = None
    vocab_size = None
    primary_vocab_size = None
    
    # start for loop - model, tokenizer, embed_lut ì½ì–´ì˜¤ëŠ” ë¶€ë¶„
    for i, model_path in enumerate(model_paths):
        if model_path not in name2model:
            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(tokenizer_paths[i], cache_dir = args.cache_dir, use_fast=True)
            name2config[model_path] = AutoConfig.from_pretrained(model_path, cache_dir=args.cache_dir)
            
            # modelì„ ì½ì–´ì˜¤ëŠ”ë°, loss ëª¨ë“ˆì˜ model wrapperë¥¼ ì´ìš©í•œë‹¤ëŠ” ì ì´ ì¢€ ì½”ë“œê°€ ì–´ë ¤ì›Œë³´ì´ê²Œ ë§Œë“ ë‹¤.
            if model_types[i] == "sentence-transformer":
                name2model[model_path] = lossbuilder.ModelWrapper(SentenceTransformer(model_path))
            elif "Custom" in model_types[i]:
                name2model[model_path] = lossbuilder.ModelWrapper(getattr(utils, model_types[i]).from_pretrained(model_path, config=name2config[model_path], cache_dir = args.cache_dir))
            else:
                name2model[model_path] = lossbuilder.ModelWrapper(getattr(transformers, model_types[i]).from_pretrained(model_path, config=name2config[model_path], cache_dir=args.cache_dir))
            # getattr: stringì„ í†µí•´ì„œ classì˜ memberë¥¼ ê°€ì ¸ì˜¨ë‹¤. stringìœ¼ë¡œ memberë¥¼ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ì¥ì . ì½”ë“œë¥¼ ê°„ê²°í™”í•  ìˆ˜ ìˆë‹¤. # https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=siniphia&logNo=221796316521
            
            if not args.show_warnings:
                
                set_global_logging_level(logging.ERROR, [name2model[model_path].__module__]) 
                # __module__: The name of the module the function was defined in, or None if unavailable.
                # https://stackoverflow.com/questions/10113892/semantics-of-module
                
            name2model[model_path].eval() # set as eval mode. turn off dropout ...
            
            # new_vocab_sizeë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ì„œ í•˜ëŠ” step
            embed_lut_ = name2model[model_path].get_input_embeddings() # transformers pretrained model class ì˜ method
            if isinstance(embed_lut_, torch.nn.Sequential):
                new_vocab_size = embed_lut_[0].num_embeddings
            else:
                new_vocab_size = embed_lut_.num_embeddings
                
            # prev_vocab_size -> ì—¬ëŸ¬ê°œì˜ modelì„ ì“°ëŠ” ê²½ìš°, for loop ì˜ ì´ì „ iterationì˜ modelì˜ vocab_size
            # lossì— ì‚¬ìš©ë˜ëŠ” ëª¨ë“  ëª¨ë¸ì´ ê°™ì€ vocabulary sizeë¥¼ ê°€ì§€ë„ë¡ í•˜ëŠ” step
            if prev_vocab_size is None:
                vocab_size = new_vocab_size
            if new_vocab_size != prev_vocab_size and prev_vocab_size is not None:
                if not args.allow_diff_vocab:
                    raise ValueError(f"all models should have the same vocabulary {new_vocab_size} != {vocab_size}")
                else:
                    logger.warning("all models don't have the same vocabulary and we are still proceeding")
                
            prev_vocab_size = vocab_size
            
        # ë§Œì•½ decoderì™€ encoderì˜ tokenizerê°€ ë‹¤ë¥´ë©´, decoderì˜ input_embedding ì‚¬ìš©. ì•„ë‹ˆë©´ encoderì˜ input_embedding ì‚¬ìš©.
        if args.target_tokenize_different:
            embed_luts.append(name2model[model_path].get_decoder().get_input_embeddings())
        else:
            input_embeds = name2model[model_path].get_input_embeddings()
            if isinstance(input_embeds, torch.nn.Sequential):
                input_embeds = input_embeds[0]
            embed_luts.append(input_embeds)
            
        # ë§Œì•½ì— target_type ì€ ì˜ì€ ëª¨ë¥´ì§€ë§Œ,,
        # updateí•˜ëŠ” ëŒ€ìƒì´ embeddingì¸ì§€, simplexì¸ì§€, probabilityì¸ì§€ ì¸ê²ƒ ê°™ë‹¤.
        # ê·¼ë° ì™œ target_typeì´ë¼ê³  ë¶€ë¥¸ê±¸ê¹Œ? ê·¸ë¦¬ê³ , simplexëŠ” ì •í™•íˆ ë­˜ê¹Œ?
        if args.target_type == "embeds":
            embed_luts[-1].requires_grad = False # embedding weightsì— ëŒ€í•´ì„œ, gradient descentë¥¼ í•˜ì§€ ì•Šìœ¼ë ¤ëŠ” ê±¸ê¹Œ?
            
        if i == 0 :
            primary_vocab_size = vocab_size
            primary_embed_dim = embed_luts[-1].embedding_dim
            
        # I'm not sure but, for some transformer-based models, authors scale token embeddings before adding with positional embedding... 
        # https://datascience.stackexchange.com/questions/87906/transformer-model-why-are-word-embeddings-scaled-before-adding-positional-encod
        if getattr(name2model[model_path], "get_decoder", None) is None:
            embed_scales.append(1.0)
        else:
            embed_scales.append(getattr(name2model[model_path].get_decoder(), "embed_scel", 1.0))
    # end for loop
    
    if use_cuda:
        for name, model in name2model.items():
            model.cuda()
    
    # losses - models ëŠ” 1:1 ë§¤í•‘ì´ ëœë‹¤. 
    # lossë¥¼ build í•´ì„œ lossfnsì— ì¶”ê°€í•œë‹¤.
    lossfns = []
    for i, loss in enumerate(losses):
        lossfns.append(lossbuilder.build_loss(loss, name2model[model_paths[i]], name2tokenizer[model_paths[i]], args))
        loss2modelname[loss] = model_paths[i]
        loss2tokenizer[loss] = name2tokenizer[model_paths[i]]
    primary_tokenizer = loss2tokenizer[losses[0]]

    if args.model_dtype == "fp16":
        for name, model in name2model.items():
            model.half()
    
    # mucola ë…¼ë¬¸ì—ì„œ epsilonê³¼ lambdaëŠ” constraint ë§ˆë‹¤ ì¡´ì¬í•œë‹¤.
    # ê²½ì œí•™ì—ì„œ ìƒê°í•´ë³´ë©´, epsilonì€ ì£¼ì–´ì§„ "ì˜ˆì‚°"ì´ê³ , lambdaëŠ” ì˜ˆì‚°ì´ 1 ì¦ê°€í•  ë•Œ, optimal íš¨ìš©ì´ ëª‡ ë‹¨ìœ„ ì¦ê°€í•˜ëŠ”ì§€ì´ë‹¤.
    # quote : ëŒë‹¤(Î») ê°’ì€ ê²½ì œí•™ì  ì˜ë¯¸ì—ì„œ ë­ëƒ? ëŒë‹¤ëŠ” ì˜ˆì‚°ì œì•½ì´ 1ë‹¨ìœ„ ì¦ê°€í•  ë•Œ ëª©ì í•¨ìˆ˜ì˜ ìµœì ê°’ì´ ì–¼ë§ˆë‚˜ ì¦ê°€í•˜ëŠ”ì§€ë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤. ì¦‰, ì˜ˆì‚°ì´ 1ì›(ë˜ëŠ” 1ë‹¬ëŸ¬) ì¦ê°€í•  ê²½ìš°, ëª©ì í•¨ìˆ˜ì¸ íš¨ìš©í•¨ìˆ˜ì˜ ìµœì ê°’(optimal utility)ì´ Î»ë‹¨ìœ„ë§Œí¼ ì¦ê°€í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.
    # https://electronicsdo.tistory.com/entry/Lagrangian-%EB%9D%BC%EA%B7%B8%EB%9E%91%EC%A7%80%EC%96%B8
    if args.epsilons is not None and args.epsilon != "none":
        epsilons = [float(eps) for eps in args.epsilons.split(":")]
        if args.min_epsilons is not None:
            min_epsilons = [float(eps) for eps in args.min_epsilons.split(":")]
            epsilon_warmup_steps = [int(steps) for steps in args.epsilon_warmup_steps.split(":")]
            epsilon_cooldown_steps = [int(steps) for steps in args.epsilon_cooldown_steps.split(":")]
            epsilon_decay_functions = [f for f in args.epsilon_decay_functions.split(":")]
        else:
            min_epsilons = [float(eps) for eps in args.epsilons.split(":")]
            epsilon_warmup_steps = [1 for eps in min_epsilons]
            epsilon_cooldown_steps = [2 for eps in min_epsilons]
            epsilon_decay_functions = ["none" for eps in min_epsilons]
        min_epsilons = [eps + getattr(lossfns[i+1], "epsilon_additive", 0) for i, eps in enumerate(min_epsilons)]
        
    else:
        epsilons = []
        min_epsilons = []
        decay_function = []
        epsilon_warmup_steps = []
        epsilon_colldown_steps = []
    
    source_dataset = None
    target_dataset = None
    additional_dataset = None # additionalì´ ë“¤ì–´ê°€ëŠ” ê±°ëŠ” hard constraint(lexical constraint) ê´€ë ¨ì´ë¼ ìƒê°í•˜ê¸°
    args.use_context = args.use_context == "true"
    # setting data paths
    if args.data is not None:
        data_paths = args.data.split(":")
        if len(data_paths) == 1:
            source_data = data_paths[0]
            target_data = data_paths[0] # not used
            context_data = data_paths[0] # not used
        elif len(data_paths) == 2:
            source_data = data_paths[0]
            target_data = data_paths[1] # for debugging
            context_data = data_paths[1] # not used
        else:
            source_data = data_paths[0]
            target_data = data_paths[1] # for debugging
            context_data = data_paths[2] # tsv file
        
        additional_data = args.additional_data
        if additional_data is None or additional_data == "none":
            additional_data = source_data # used for STRAP example (Krishna et al 2020)
    
    elif args.additional_data is not None and additional_data != "none":
        source_data = args.additional_data
        target_data = args.additional_data
        additional_data = args.additional_data
    
    else:
        source_dataset = sys.stdin
        start_idx = 0
        end_idx = 1000000
        
    if source_dataset is None: # ìœ„ if else ì˜ ë§ˆì§€ë§‰ ì¼€ì´ìŠ¤ê°€ ì•„ë‹Œ ê²½ìš°. # ì˜µì…˜ì— ë”°ë¼ì„œ, ë‹¤ë¥´ê²Œ ìƒê¸´ ë°ì´í„°ì…‹ì„ ì½ì–´ì˜¤ëŠ” ì½”ë“œë¥¼ ì‹¤í–‰
        if args.datastyle == "text":
            source_dataset = [l.strip() for l in open(source_data)]
            target_dataset = [l.strip() for l in open(target_data)]
            context_dataset = []
            import csv
            with open(context_data) as csvfile:
                reader = csv.reader(csvfile, delimiter="\t")
                for row in reader:
                    context_dataset.append(row)
            additional_dataset = [l.strip() for l in open(additional_data)]
        elif args.datastyle == "jsonl":
            source_dataset = [json.loads(l)[args.jsonl_primary_key] for l in open(source_data)]
            target_dataset = [json.loads(l)[args.jsonl_primary_key] for l in open(target_data)]
            additional_dataset = [json.loads(l)[args.jsonl_primary_key] for l in open(additional_data)]
            
            # dataset í˜•íƒœê°€ nested ëœ json ì¼ ê²½ìš°ì—.. (e.g. {"contents": {"text": ...}})
            if args.jsonl_secondary_key is not None and args.jsonl_secondary_key != "none":
                source_dataset = [x[args.jsonl_secondary_key] for x in source_dataset]
                target_dataset = [x[args.jsonl_secondary_key] for x in target_dataset]
                additional_dataset = [x[args.jsonl_secondary_key] for x in additional_dataset]
            
            context_dataset = [None] * len(source_dataset)
            if args.use_context:
                context_dataset = [json.loads(l)[args.jsonl_primary_key] for l in open(context_data)]
                if args.jsonl_secondary_key is not None and args.jsonl_secondary_key != "none":
                    context_dataset = [x[args.jsonl_secondary_key] for x in context_dataset]
        elif args.datastyle == "single-jsonl": # one jsonl file has all the information 
            source_dataset = [json.loads(l)[args.jsonl_primary_key] for l in open(source_data)]
            target_dataset = [json.loads(l)[args.jsonl_secondary_key] for l in open(target_data)]
            additional_dataset = [json.loads(l)[args.jsonl_secondary_key] for l in open(additional_data)]
            
            context_dataset = [None] * len(source_dataset)
            if args.use_context:
                context_dataset = [[json.loads(l)[args.jsonl_secondary_key]] for l in open(context_data)] #meaningful
        
        # ì½ì–´ì˜¨ ë°ì´í„°ì—ì„œ ëª‡ë²ˆì§¸ ìƒ˜í”Œë¶€í„° ëª‡ë²ˆì§¸ ìƒ˜í”Œê¹Œì§€ ì²˜ë¦¬í• ê²ƒì¸ì§€ ê²°ì •
        start_idx = args.start_idx 
        end_idx = (len(source_dataset) + args.end_idx) % len(source_dataset) 

    # batchë¥¼ initialize í•¨
    source_batch, target_batch, additional_batch, for_predicted_source_batch, predicted_batch, context_batch = [], [], [], [], [], []
    batch_size = args.batch_size 
    
    device = "cuda" if use_cuda else "cpu"
    c = 0 # counting # of examples that are subject to constrained decoding.
    
    # initialize lists to keep track of losses
    losslists = [[] for _ in range(len(losses))] 
    predictedlosslists = [[] for _ in range(len(losses))]
    source_primarylosslist = []
    all_stepcounts = []
    avg_time = 0
    
    # ì´ê²ƒì´ ë¬´ì—‡ì¼ê¹Œ..ã…ã… // len(losses) -1 ì˜ ê°œìˆ˜ë§Œí¼ì˜ "true" or "false"
    # if "true", then use the base lm loss as the epsilon for loss i
    if args.gold_loss_epsilons is not None and args.gold_loss_epsilons != "none":
        args.gold_loss_epsilons = args.gold_loss_epsilons.lower().split(":")
        assert len(args.gold_loss_epsilons) == len(losses) - 1
    else:
        args.gold_loss_epsilons = ["false" for _ in range(len(losses) - 1)]
    
    # ëª¨ë“  test setì„ ì²˜ë¦¬í•˜ì§€ ì•Šê³ , ì¼ë¶€ë§Œ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ì„œ ì•„ë˜ë¥¼ ì •ì˜
    # args.random_example -> ì“°ì´ì§€ ì•ŠìŒ
    # args.num_examples -> 0 ë³´ë‹¤ í° ìˆ«ìì´ë©´, args.num_examples ë§Œí¼ë§Œ í…ŒìŠ¤íŠ¸í•˜ë„ë¡ í•¨
    example_p = 1.0
    args.random_example = args.random_example == "true" ## ë”±íˆ ì–´ë””ì— ì“°ì´ì§€ ì•ŠëŠ” ì˜µì…˜ì„. ê·¸ëƒ¥ defaultë¡œ sample í•¨..
    if args.num_examples > 0 and target_dataset is not None: # source_datasetì´ sys.stdinì´ ì•„ë‹Œ ê²½ìš°ì—
        example_p = args.num_examples * 1.0 / len(source_dataset)
    
    for text_id, source_text in enumerate(source_dataset):
        
        # ì§€ì •í•œ start_idx, end_idx ì‚¬ì´ì˜ ìƒ˜í”Œë§Œ ì‚¬ìš©
        if text_id < start_idx or text_id > end_idx: 
            continue 
        
        # ì‹¤ì œë¡œ test í•œ ìƒ˜í”Œê°œìˆ˜ê°€ c ì¸ë°, ê·¸ê²ƒì´ num_examples ì™€ ê°™ìœ¼ë©´ ì¤‘ì§€
        if args.num_examples > 0 and c > 0 and c == args.num_examples:
            break
        
        # ì „ì²´ test sample ì¤‘ì—ì„œ example_p ì˜ í™•ë¥ ë¡œ testí•¨
        # np.random.rand() : uniform distribution over [0, 1]
        do_this_example = np.random.rand() <= example_p
        if not do_this_example:
            continue 
        
        # testí•œ ìƒ˜í”Œ ìˆ˜ë¥¼ 1ê°œ ëŠ˜ë¦¼
        c += 1 
        
        # lossfns[lossid].compute_gold_loss() í• ë•Œ kweightìœ¼ë¡œ ì¸ìë¡œ ë“¤ì–´ê°.
        # kweightì´ë¼ëŠ” ì˜µì…˜ì´ í€µí•˜ê²Œ ë³´ê¸°ë¡œëŠ” ì•ˆì“°ì´ëŠ” ê°’ê°™ë‹¤.
        new_kweight = args.kweight 
        if target_dataset is not None: # sys.stdin ì—ì„œ source_datasetì„ ë°›ì•„ì˜¤ëŠ”ê²ƒì´ ì•„ë‹Œ ê²½ìš°
            target_text = target_dataset[text_id]
            additional_text = additional_dataset[text_id]
            context_texts = context_dataset[text_id]
            
            # ë”°ë¡œ option ë“¤ì„ sys.stdin ì—ì„œ ë°›ì„ í•„ìš”ê°€ ì—†ëŠ” ê²½ìš°ì„
            # nontoxicity ì˜ ê²½ìš°,
            # args.init = "target" ## optionì˜ ì¢…ë¥˜: "zeros", "random", "source", "target", "targettarget", "random_vocab", "embedgd-zeros"
            # args.max_output_length = 20
            # args.max_length = 20
            # args.use_context = "false"
            # ??? ê³„ì†í•´ì„œ "source" ì™€ "target"ì˜ ê°œë…ì´ ë‚˜ì˜¤ëŠ”ë°,, ì´ê²ƒë“¤ì€ ë‹¤ ë­˜ê¹Œ?...
            
        else: # source_textê°¸ sys.stdinê³¼ ê°™ì€ ê²½ìš° ??? 
            args.jsonl_tokenized = False
            items = source_text.split("::")
            source_text = items[0].rstrip()
            target_text = items[1].rstrip()
            if target_text == "-":
                args.init = "zeros"
            elif target_text == "--":
                args.init = "target"
            else:
                args.init = "targettarget"
            additional_text = items[2]
            
            if len(items) > 3:
                args.max_output_length = int(items[3])
                args.max_length = int(items[3])
            if len(items) > 4:
                args.use_context = True
                context_texts = [items[4]].rstrip()
            else:
                args.use_context = False
                context_texts = []
            
            if len(items) > 5:
                new_kweight = float(items[5])

        # ì˜ì€ ëª¨ë¥´ëŠ”ë°, hard constraint, ì¦‰ íŠ¹ì • ë‹¨ì–´ë¥¼ ê¼­ ë„£ì–´ì•¼ í•˜ëŠ” lossì— ì“°ì´ëŠ” ì˜µì…˜ ê°™ë‹¤.
        if args.keywords == "_roc_": # roc -> dataset ì´ë¦„
            keywords = ["none"] + additional_text.split(", ")
        if args.keywords == "_rocunique_": # ??? uniqueê°€ ë¶™ì—ˆì„ë•Œ ë­ê°€ ë‹¬ë¼ì§€ëŠ”ê±´ì§€ ëª¨ë¦„.
            keywords = ["none"] + additional_text.split(", ") + ["none"]
        elif args.keywords == "_commongen_":
            keywords = ["none"] + json.loads(additional_text)['concept_set'].split("#")
        elif args.keywords == "_commongenunique_":
            keywords = ["none"] + json.loads(additional_text)['concept_set'].split("#") + ["none"]
        
        # debugging í•  ë•Œ ì¼ë˜ ì½”ë“œì¸ë“¯
        early_skip="n"
        if args.debug:
            early_skip = input(f"skip this example? {source_text} [yes(y)/maybe(m)/no(n)]")
            if early_skip == "y":
                continue
        
        if not args.jsonl_tokenized: # ëŒ€ì¶© guess í•˜ê¸°ë¡œëŠ” jsonl íŒŒì¼ì´ ì´ë¯¸ tokenize ëœ ì±„ë¡œ ì €ì¥ëœê²Œ ì•„ë‹ˆë©´
            if source_text == "": # promptê°€ ì—†ëŠ” ê²½ìš°ë¼ë©´
                source_text = primary_tokenizer.bos_token # bosë¡œ ì‹œì‘í•˜ê³ 
            source_indices = primary_tokenizer.encode(source_text, return_tensors="pt").to(device) # source_textë¥¼ encode
            source_indices_write = source_indices[0].tolist()
            
            additional_indices = primary_tokenizer.encode(additional_text, return_tensors="pt", add_special_tokens=False).to(device)
            
            eos_token_id = primary_tokenizer.eos_token_id
            bos_token_id = primary_tokenizer.bos_token_id
            
            context_indices= None ## ë³´í†µ contextë€ ë§ì€ QA í•  ë•Œ ì“°ëŠ”ê±°ê°™ì€ë°, ì—¬ê¸°ì„œëŠ”... nontoxic generation ì¼ ë•ŒëŠ” ì˜ ì•ˆì“°ëŠ” ë§ê°™ë‹¤.
            if args.target_tokenize_different:
                with primary_tokenizer.as_target_tokenizer():
                    eos_token_id = primary_tokenizer.eos_token_id
                    bos_token_id = primary_tokenizer.bos_token_id
                    if args.use_context:
                        context_indices = primary_tokenizer.encode(context_texts[0], return_tensors="pt")
            elif args.use_context:
                context_indices = primary_tokenizer.encode(context_texts[0], return_tensors="pt")
            
            # mucola ë…¼ë¬¸ì—ì„œ seq2seq ëª¨ë¸ì„ ì“´ ì¼€ì´ìŠ¤ -> appendixì— ë‚˜ì™€ìˆëŠ” entity-controlled summarization task ë¼ëŠ”ê²Œ ìˆì—ˆìŒ!
            # ë­ ì˜ì€ ëª¨ë¥´ê² ëŠ”ë° base LM is seq2seq ì´ë©´ target_tokenize_different settingì´ True ì—¬ì•¼ í•˜ë‚˜ë´„........ ì´ê²Œ ë¬´ì—‡ì´ëƒ........
            if not args.target_tokenize_different and "Seq2SeqLM" in model_paths[0]:
                logger.warning("you are using a seq2seq model for your primary loss but not tokenizing the target sentences with a different target tokenizer.")

            if args.target_tokenize_different:
                with primary_tokenizer.as_target_tokenizer():
                    for_predicted_source_indices = primary_tokenizer.encode(source_text, return_tensors="pt")
                    target_indices = primary_tokenizer.encode(target_text, return_tensors="pt", add_special_tokens=False).to(device)
            else:
                for_predicted_source_indices = source_indices
                target_indices = primary_tokenizer.encode(target_text, return_tensors="pt", add_special_tokens=False).to(device)
        else: # in case jsonl is already tokenized!
            source_indices_write = source_text # ..._text is already indices
            source_indices = source_text
            target_indices = target_text
            additional_indices = additional_text
            context_indices = context_texts # ??? contextëŠ” ë­˜ê¹Œ?
            if len(source_indices)==0: # if there's no prompt.
                source_indices.append(primary_tokenizer.bos_token_id)

            source_indices = torch.LongTensor([source_indices]).to(device) # tensorë¡œ ë°”ê¿ˆ
            additional_indices = torch.LongTensor([additional_indices]).to(device) # tensorë¡œ ë°”ê¿ˆ
            
            # for_predicted_source_indices => used for style transfer. 
            for_predicted_source_indices = source_indices
            target_indices = torch.LongTensor([target_indices]).to(device) # tensorë¡œ ë°”ê¿ˆ
            
            bos_token_id = primary_tokenizer.bos_token_id
            eos_token_id = primary_tokenizer.eos_token_id
            if args.target_tokenize_different: # ì´ëŸ° ì˜µì…˜ì´ ë§Œì•½ì— ìˆìœ¼ë©´, target tokenizerì— ë§ì¶°ì„œ tokenize í•´ì•¼ í•¨
                with primary_tokenizer.as_target_tokenizer():
                    bos_token_id = primary_tokenizer.bos_token_id
                    eos_token_id = primary_tokenizer.eos_token_id
                    
            source_text = primary_tokenizer.decode(source_indices[0].tolist()) # ì˜¤íˆë ¤ ì´ë¯¸ tokenizeëœ indicesë¥¼ textë¡œ ë‹¤ì‹œ ë°”ê¿ˆ
            
        #### tokenizeê°€ ëë‚˜ê³  listì— ë‹´ê¸° -> ê·¼ë° ì‚¬ì‹¤ ì¢€ ì›ƒê¸´ê²Œ .. ì§€ê¸ˆ ì½”ë“œê°€ ì–´ì§œí”¼ batch_size == 1 ì¼ë•Œë§Œ ëŒì•„ê°€ê°€ì§€êµ¬, [[ids]] ì´ëŸ°ì‹ìœ¼ë¡œ ìƒê¸´ í˜•íƒœì¼ ë¿ì´ë‹¤.
        source_batch.append(source_indices)
        target_batch.append(target_indices)
        for_predicted_source_batch.append(for_predicted_source_indices)
        additional_batch.append(additional_indices)
        context_batch.append(context_indices)
            
        # ì´ë²ˆ iterationì˜ text_id, source_text ì— ëŒ€í•´ì„œ ì•„ë˜ë¥¼ ì‹¤í–‰
        if len(source_batch) == batch_size: ## ì•½ê°„ ì´ë ‡ê²Œ í•œ ì´ìœ ëŠ” ë­˜ê¹Œ? í ... ë­”ê°€.... ìŒ........ëª¨ë¥´ê² ë‹¤ ã…
            
            source_batch = torch.cat(source_batch, dim=0).to(device)
            target_batch = torch.cat(target_batch, dim=0).to(device)
            additional_batch = torch.cat(additional_batch, dim=0).to(device)
            for_predicted_source_batch = torch.cat(for_predicted_source_batch, dim=0).to(device)
            
            if args.use_context:
                context_batch = torch.cat(context_batch, dim=0).to(device)
                print(context_batch)
            
            # generate AR generations.
            predicted_batches = []
            for batchidx in range(source_batch.size(0)):
                with torch.no_grad(): # modelì˜ gradientëŠ” í•„ìš”í•˜ì§€ ì•Šì•„ì„œ ì´ë ‡ê²Œ í•˜ëŠ”ê±¸ê¹Œ?
                    starttime = time.time() # num_samples ê°œìˆ˜ë§Œí¼ AutoRegressive predictionì„ ë§Œë“¤ì–´ë‚¸ë‹¤.
                    AR_predicted_all = \
                        lossfns[0].generate(
                            input_ids = source_batch[batchidx].unsqueeze(0),
                            additional_ids = additional_batch[batchidx].unsqueeze(0),
                            num_return_sequences = (args.restarts + 1) * args.num_samples
                        )
                    
                    AR_prediction_all = [] # ì´ë¦„ì„ ì—„ì²­ í—·ê°ˆë¦¬ê²Œ ì§€ì—ˆë‹¤ê³  ìƒê°í•˜ëŠ” ë¶€ë¶„..
                    for sample_idx in range(len(AR_predicted_all)):
                        AR_predicted_indices = \
                            clean_output(AR_predicted_all[sample_idx].tolist(),
                                         eos_token_id = eos_token_id,
                                         return_tensors = True,
                                         allow_first_eos=losses[0] == "bart",
                                         skip_special_tokens = [bos_token_id, eos_token_id])
                        
                        # cleaní•œ indexë¥¼ ë‹¤ì‹œ textë¡œ ë³€í™˜
                        if args.target_tokenize_different:
                            with primary_tokenizer.as_target_tokenizer():
                                AR_prediction = primary_tokenizer.decode(AR_predicted_indices[0].tolist())
                        else:
                            AR_prediction = primary_tokenizer.decode(AR_predicted_indices[0].tolist())
                        
                        # decodeí•œ textëŠ” AR_prediction_all ì— ì €ì¥
                        AR_prediction_all.append(AR_prediction)
                        
                        # indicesëŠ” predicted_batches ì— ì €ì¥
                        predicted_batches.append(AR_predicted_indices.to(device))

            broken_skip = False
            
            for sample_idx in range(args.num_samples):
                
                for restart_idx in range(args.restarts + 1): # args.restartsê°€ 0ë³´ë‹¤ í¬ë©´, AR sampleë„ ì¢€ë” ë§ì´ ë§Œë“¤ì–´ë‘  (??? ê·¸ëŸ´ë°”ì— ê·¸ëƒ¥ num_samplesë¥¼ ëŠ˜ë¦¬ì§€ì™œ....)
                    
                    # get one sample from predicted batch
                    predicted_batch = predicted_batches[sample_idx * (args.restarts + 1) + restart_idx]
                    AR_prediction = AR_prediction_all[sample_idx * (args.restarts + 1) + restart_idx]
                    
                    intermediate_result = {"prompt": source_text} # by hayley
                    intermediate_result.update({"sample_id": sample_idx, "original_text": AR_prediction})
                    
                    # initialize some options
                    skip = False
                    #predicted_allsat = False
                    lengthwise_best_prediction = [None] * batch_size # ??? ì™œ êµ³ì´ batch_size í¬ê¸°ë¡œ initialize í• ê¹Œ?
                    
                    # !!! important notes !!!
                    # !!! 1. AR outputì˜ loss ë³´ë‹¤ëŠ” ì¢‹ì•„ì•¼ì§€ë§Œ updateí•œ outputì„ ìµœì¢… ê²°ì •í•œë‹¤. ë§Œì•½ì— AR outputì´ ë” ì¢‹ìœ¼ë©´, ê·¸ê±¸ë¡œ ìµœì¢… ë±‰ëŠ”ë‹¤. losses of the autoregressive output: we should perform atleast as well as this. If we don't, we predict this output
                    # !!! 2. ë§Œì•½ì— AR outputì´ ì´ë¯¸ constraintë¥¼ ë§Œì¡±í•˜ë©´, mucola ë‹¨ê³„ë¥¼ êµ³ì´ ë°Ÿì§€ ì•ŠëŠ”ë‹¤. args.always_mucoco ë¼ëŠ” ì˜µì…˜ì´ true ì´ë©´ ë¬´ì¡°ê±´ mucola í•œë‹¤. Also, if the autoregressive output already satisfies the constraints, we skip mucoco unless, args.always_mucoco is true
                    predicted_labels = {}
                    total_predicted_loss = 0.0
                    predicted_allsat = True # ì´ê±´ ì§„ì§œ ì €ìê°€ ì˜ëª»í–ˆë‹¤ ã…‹
                    predictedlosses = []
                    
                    for lossid in range(len(losses)):
                        lossname = losses[lossid]

                        predicted_loss, predicted_lo = \
                            lossfns[lossids].compute_gold_loss(
                                (source_batch, target_batch),
                                additional_batch = additional_batch,
                                context_batch = context_bach,
                                use_context = args.use_context,
                                label_id = label_ids[lossid],
                                keyword = keywords[lossid],
                                kweight=new_kweight # ì•ˆì“°ëŠ” ì˜µì…˜ ê°™ë‹¤........!!!
                            )
                        
                        predictedlosses.append(predicted_loss.data.cpu()) # primary lossì™€ constraint lossë“¤ì„ listë¡œ ê°€ì§€ê³  ìˆìŒ.
                        predicted_loss = predicted_loss.sum().item()
                        intermediate_result.update({f"original_loss{lossid}": predicted_loss})
                        total_predicted_loss += betas[lossid] * predicted_loss # ì´ê±°ëŠ” ê²°êµ­ì— ë­ëƒë©´, gold_lossì˜ ê²½ìš°, primary_loss ì´ì™¸ì—ëŠ” weightë¥¼ ì£¼ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì˜ë¯¸...!>!>!>

                        if lossid > 0: # primary_lossê°€ ì•„ë‹ë•Œ
                            predicted_allsat = predicted_allsat and (predicted_loss <= min_epsilons[lossid-1])
                        
                        if "label_prediction" in predicted_lo:
                            predicted_labels[lossid] = predicted_lo['label_prediction']
                        else:
                            predicted_labels[lossid] = "NA"
                        
                        if lossid > 0 and args.gold_loss_epsilons[lossid-1] == "true": # primary_lossê°€ ì•„ë‹ˆê³ , í•´ë‹¹ lossì— ëŒ€í•´ì„œ gold_lossë¥¼ epsilonìœ¼ë¡œ ì“°ê¸° ì›í• ë•Œ
                            min_epsilons[lossid-1] = predicted_loss + getattr(lossfns[lossid], "epsilon_additive", 0)
                            epsilons[lossid-1] = predicted_loss + getattr(lossfns[lossid], "epsilon_additive", 0)
                            
                    predictedlosslists.append(predictedlosses)
                    
                    if args.only_mucoco == "false": # ??? ì´ê±°ë‘ always_mucocoë‘ ê³¼ì—° ã…‹ã…‹ ë­ê°€ ë‹¤ë¥¸ê±¸ê¹Œ.. ì–´ì¨Œë“ , only_mucocoê°€ ì•„ë‹Œ ì´ìƒ, AR outputì„ best predictionìœ¼ë¡œ ì €ì¥í•´ë†“ëŠ”ë‹¤.
                        lengthwise_best_prediction = [(AR_prediction, total_predicted_loss, predicted_allsat, predicted_batch[0].tolist(), -1)]
                    
                    skip = predicted_allsat # ë§Œì•½ì— AR predictionì´ all satisfying í•˜ë©´, skip = True ê°€ ëœë‹¤.
                    
                    definite_skip = False
                    ask_skip = ""
                    if args.debug and early_skip=="m": # debug ëª¨ë“œ ì´ë©´ì„œ early_skip ì¼ ë•Œ maybeë¼ê³  í–ˆë˜ ê²½ìš°...
                        print(f"new example: {source_text}\nautoregressive output: {AR_prediction}")
                        for lossid in range(len(losses)):
                            print(f"{lossabbr[lossid]} for desired label_id({label_ids[lossid]}): {predictedlosslists[-1][lossid]}; predicted label: {predicted_labels[lossid]}")
                        if predicted_allsat:
                            print(f"autoregressive output already satisfies the constraints")
                        ask_skip = input(f"skip this example? [y/n]") # ì‚¬ìš©ìì—ê²Œ skip í• ì§€ë¥¼ ë¬¼ì–´ë´„
                        definite_skip = ask_skip == "y" # ì‚¬ìš©ìê°€ skip í•˜ë¼ê³  í•˜ë©´ skip í•¨
                        
                    # ì‚¬ì‹¤ ë­.. ìœ„ì— skip = prediced_allsat ë•Œë¬¸ì—. skip and predicted allsatì„ êµ³ì´ ë‘ë²ˆ ì²´í¬í•˜ì§€ ì•Šê³ , ê·¸ëƒ¥ predicted_allsat ë§Œ ì²´í¬í•´ë„ ë ë“¯...
                    elif skip and predicted_allsat and (args.always_mucoco == "false"): 
                        # debugëª¨ë“œê°€ ì•„ë‹ˆê³ 
                        # !!! ë§Œì•½ì— always_mucoco ê°€ ì•„ë‹Œ ì´ìƒ, predicted_allsatì´ë©´ skip í•¨
                        definite_skip = True 
                    
                    # ğŸŒŸ IMPORTANT ğŸŒŸ #
                    if not definite_skip: # skip ì´ ì•„ë‹ ê²½ìš° == constrained decodingì„ í•  ê²½ìš° 
                        # set length_range
                        if (args.max_length is None or args.max_length == -1) and args.init not in ["source", "target"]:
                            predicted_length = predicted_batch.size(1)
                            length_range = [predicted_length + int(diff) for diff in args.length_diff.split(":")]
                            length_range = [x for x in length_range if x <= args.max_allowed_length and x >= 1]
                            if len(length_range) == 0:
                                length_range = [args.max_allowed_length]
                            length_range = sorted(list(set(length_range)))
                        elif args.init == "targettarget":
                            length_range = [target_batch.size(1)]
                        elif args.init == "target":
                            length_range = [predicted_batch.size(1)]
                        elif args.init == "source":
                            length_range = [source_batch.size(1)]
                        else:
                            length_range = [args.max_length]
                        
                        for sent_length_ in length_range: # ì‚¬ì‹¤ìƒ 1ê°œ length ë¡œë§Œ ëŒê³  ìˆë‹¤ ì§€ê¸ˆì€.
                            if args.prefix_length > 0:
                                sent_length = sent_length_ - args.prefix_length
                                target_prefix = predicted_batch[:, :args.prefix_length]
                            else:
                                sent_length = sent_length_
                                target_prefix = torch.empty((source_indices.size(0), 0)).long().to(device)
                                
                            if sent_length <= 0:
                                continue
                            if sent_length > args.max_allowed_length:
                                old_l = sent_length
                                sent_length = args.max_allowed_length
                                print(f"changed output length to {sent_length} from {old_l} to avoid GPU overflow. This is a temporary solution.")
                            else:
                                print("predicting a sentence length: ", sent_length)
                            
                            if args.target_type == "simplex": # use V sized real vector for each token and apply softmax before output
                                outputs = TargetSimplex(
                                    vocabsize = primary_vocab_size,
                                    sent_length = sent_length,
                                    batch_size = batch_size,
                                    device = device,
                                    temperature = args.decode_temperature, 
                                    st = args.st,
                                    init_value = source_batch[:, 1:-1] if args.init == "source" else None,
                                    random_init = args.init == "random",
                                    do_sample = args.expgd_do_sample,
                                    top_p = args.expgd_top_p,
                                    top_k = args.expgd_top_k,
                                    embed_scales = embed_scales
                                )
                            elif args.target_type == "probs": # use V sized vector which sums to one for each token and apply softmax before output
                                
                                init_value = None
                                break_after = False
                                if args.init == "source":
                                    init_value = source_batch
                                    target_prefix = torch.empty((source_indices.size(0), 0)).long().to(device)
                                    sent_length = init_value.size(1)
                                    break_after = True
                                elif args.init == "target":
                                    init_value = target_batch
                                    target_prefix = torch.empty((source_indices.size(0), 0)).long().to(device)
                                    sent_length = init_value.size(1)
                                    break_after = True
                                    
                                outputs = TargetProbability(
                                    vocabsize = primary_vocab_size,
                                    sent_length = sent_length,
                                    batch_size=batch_size,
                                    device = device,
                                    st = args.st,
                                    init_value = init_value,
                                    random_init = args.init == "random",
                                    do_sample = args.expgd_do_sample,
                                    top_p = args.expgd_top_p,
                                    top_k = args.expgd_top_k,
                                    embed_scales = embed_scales,
                                    max_steps = args.optim_steps
                                )
                            elif args.target_type == "embeds":
                                init_value = None
                                break_after = False
                                if args.init == "source": # initialize the target with the source
                                    init_value = embed_luts[0](source_batch)
                                    target_prefix = torch.empty((source_indices.size(0), 0)).long().to(device)
                                    sent_length = init_value.size(1)
                                    break_after = True
                                elif args.init == "targettarget": # initialize the target with given target
                                    init_value = embed_luts[0](target_batch)
                                    target_prefix = torch.empty((source_indices.size(0), 0)).long().to(device)
                                    sent_length = init_value.size(1)
                                    break_after = True
                                elif args.init == "target": # initialize the target with the autoregressive output 
                                    init_value = embed_luts[0](predicted_batch)
                                    target_prefix = torch.empty((source_indices.size(0), 0)).long().to(device)
                                    sent_length = init_value.size(1)
                                    break_after = True
                                elif args.init == "random_vocab": # uniformly and with replacement sample token from the entire vocabulary
                                    random_indices = torch.multinomial(torch.ones(primary_vocab_size,)/primary_vocab_size, num_samples = batch_size * sent_length, replacement = True).view(batch_size, sent_length).to(device)
                                    init_value = embed_luts[0](random_indices)
                                elif args.init == "embedgd-zeros": # initialize with eos_token_ids
                                    if args.target_tokenize_different:
                                        with primary_tokenizer.as_target_tokenizer():
                                            indices = torch.empty((batch_size, sent_length)).long().fill_(primary_tokenizer.eos_token_id).to(device)
                                    else:
                                        indices = torch.empty((batch_size,sent_length)).long().fill_(primary_tokenizer.eos_token_id)
                                    init_value = embed_luts[0](indices)
                                elif args.init == "zeros":
                                    indices = torch.zeros((batch_size, sent_length)).long().to(device)
                                    init_value = embed_luts[0](indices)
                                
                                final_bias = None
                                if args.final_bias:
                                    final_bias = lossfns[0].model.final_logits_bias
                                
                                outputs = TargetEmbeddings( # ì†”ì§íˆ ë§í•˜ë©´ ì¡¸ë¦¬ê¸°ë„ í•˜ê³ , ì´ê²Œ ì •í™•íˆ ì–´ë–¤ê±´ì§€ ëª¨ë¥´ê² ë‹¤. ê·¸ëƒ¥ embeddingì„ í•˜ëŠ”ê±´ê°€?
                                    embed_dim = primary_embed_dim, # 
                                    embed_lut = embed_luts[0], # look up table
                                    sent_length = sent_length, # sentench length
                                    batch_size = batch_size, # batch size
                                    device = device,
                                    st = args.st, # st ê°€ ë­˜ê¹Œ? straight-through. gradientë¥¼ ê³„ì‚°í•˜ê¸° ì–´ë ¤ìš´ ìŠ¤í…ì´ ìˆë‹¤ë©´, forwardì—ëŠ” ê·¸ ìŠ¤í…ì„ ì ìš©í•˜ì§€ë§Œ backwardì—ëŠ” ê·¸ ìŠ¤í…ì„ ì ìš©í•˜ì§€ ì•ŠìŒ. https://hassanaskary.medium.com/intuitive-explanation-of-straight-through-estimators-with-pytorch-implementation-71d99d25d9d0
                                    init_value = init_value,
                                    random_init = args.init == "random",
                                    sampling_strategy = args.sampling_strategy,
                                    sampling_strategy_k = args.sampling_strategy_k,
                                    embed_scales = embed_scales,
                                    metric = args.metric,
                                    same_embed = args.same_embeds,
                                    final_bias = final_bias,
                                    eos_token_id = primary_tokenizer.eos_token_id
                                )
                            else:
                                raise ValueError("Wrong target_type")
                            
                            # ì œì•½ì¡°ê±´ì´ ìˆì„ ê²½ìš°, Lambda ê°ì²´ë¥¼ ì •ì˜í•´ì„œ ì“´ë‹¤. (Lambda ê°ì²´ëŠ” constraintì— ê³±í•˜ëŠ” ê°’ì„)
                            if len(losses) > 1:
                                lambda_ = Lambda(count=len(epsilons)) # epsilonì˜ ê°œìˆ˜ë§Œí¼ 0.0ìœ¼ë¡œ initializeí•¨ # ì™œ ì´ê±°ëŠ” gradient ascent í•˜ëŠ”ê±¸ê¹Œ?
                                if use_cuda:
                                    lambda_.cuda()
                            
                            args.optim = "embedgd_le"
                            optimizer = OptimizerLE.from_opt(outputs, args)
                            optimizer.set_init_pred(predicted_batch)
                            
                            cur_lr = args.lr
                            
                            if len(losses) > 1: # ì œì•½ì¡°ê±´ì´ ìˆì„ ê²½ìš°ì—ëŠ”, lambdaë¥¼ gradient ascentí•˜ëŠ” optimizerë„ ì •ì˜í•œë‹¤.
                                old_optim = args.optim
                                args.optim = "gradascent"
                                old_lr = args.lr
                                args.lr = args.lambda_lr
                                optimizer_lambda = Optimizer.from_opt(lambda_, args)
                                args.optim = old_optim
                                args.lr = old_lr
                            
                            best_loss = [None] * batch_size
                            best_allsat = [False] * batch_size
                            best_repeat_count = [0] * batch_size
                            best_losses = [[None] * batch_size for _ in range(len(losses))]
                            best_step = -100
                            
                            best_pred_tokens = [None] * batch_size
                            best_prediction_set = [set() for _ in range(batch_size)]
                            best_pred_probs = [None] * batch_size
                            best_index = [-1 for i in range(batch_size)]
                            
                            scaler = None
                            if args.model_dtype == "fp16" and args.fp16_source == "pytorch":
                                scaler = torch.cuda.amp.GradScaler()
                                
                            for lossid, lossname in enumerate(losses):
                                losslists[lossid].append([])
                            
                            broken = False
                            prev_loss = None
                            dynamic_lambda_update_prev_loss = None
                            same_loss_count = 0
                            dynamic_loss_update_same_loss_count = 0
                            starttime = time.time()
                            repeat_counts = [0] * batch_size
                            
                            # locateí•˜ëŠ” ì½”ë“œë¥¼ ë„£ì„ ì¥ì†ŒëŠ” ì—¬ê¸°.
                            
                            # 
                            
                            # 200ê°œì˜ stepìœ¼ë¡œ í˜„ì¬ëŠ” ì§€ì •ë˜ì–´ ìˆìŒ.
                            for step in range(args.optim_steps):
                                try: # keyboard interruptê°€ ìˆì„ ë•Œ ê·¸ëƒ¥ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•˜ëŠ”ê²Œ ì•„ë‹ˆë¼, í•´ë‹¹ ìƒ˜í”Œì— ëŒ€í•œ optim_stepì„ ì¢…ë£Œí•˜ëŠ” ê²ƒìœ¼ë¡œ ë  ìˆ˜ ìˆë„ë¡. try~except~ë¬¸ êµ¬ì„±.
                                    with torch.cuda.amp.autocast():
                                        losses_for_backward = []
                                        logging_outputs = []
                                        
                                        # ??? ì´ë¶€ë¶„ì´ ì •í™•íˆ ì–´ë–¤ ê²ƒì„ í•˜ëŠ”ê±¸ê¹Œ? ì½”ë“œë¥¼ ì´í•´í•˜ì§€ëŠ” ëª»í–ˆë‹¤.
                                        # ë‹¤ë§Œ, ë³€ìˆ˜ëª…ì—ì„œ ë¯¸ë£¨ì–´ ì§ì‘í–ˆì„ ë•Œì—ëŠ” ì˜ˆì¸¡í•œ í† í°ì˜ ì„ë² ë”©ê°’, ì˜ˆì¸¡í•œ í† í°ê°’, ì˜ˆì¸¡í•œ í™•ë¥ ê°’ì„ ë¦¬í„´.
                                        # target_typeì´ TargetEmbedding ì¼ ë•ŒëŠ” ì•„ë˜ ìˆ˜í–‰.
                                        # return (pred_embs, self._pred_embeds), predictions, (pred_probs, softmax_pred_probs)
                                        pred_embeds, pred_tokens, pred_probs = outputs.forward_multiple(embed_luts, new_predictions = getattr(optimizer._optimizer, "new_predictions", None))
                                        
                                        def get_sent(tokens, tokenizer):
                                            batch = []
                                            if args.target_tokenize_different:
                                                with tokenizer.as_target_tokenizer():
                                                    for toks in tokens:
                                                        batch.append(tokenizer.decode(clean_output(toks.tolist(), -1, allow_first_eos=losses[0] == "bart")))
                                            else:
                                                for toks in tokens:
                                                    batch.append(tokenizer.decode(clean_output(toks.tolist(), -1, allow_first_eos=losses[0]=="bart")))
                                            return batch
                                        
                                        target_sents = get_sent(torch.cat([target_prefix, pred_tokens], dim=1), primary_tokenizer)
                                        if step % 10 == 0:
                                            intermediate_result.update({f"step_{step}_text": target_sents})
                                        
                                        original_preds = None
                                        if len(pred_embeds) > 1:
                                            original_preds = pred_embeds[1]
                                        
                                        # lossë¥¼ ê³„ì‚°í•œë‹¤. lossid = 0 ì€ autoregressive output ì´ë‹¤. 
                                        # lossid > 0 ì€ constraint ë“¤ì´ë‹¤.
                                        for lossid, lossname in enumerate(losses):
                                            lossvalue, logging_output =\
                                                lossfns[lossid].compute_loss(
                                                    [source_batch, target_prefix], # batch
                                                    [pred_tokens, pred_embeds[0][lossid], pred_probs], # preds
                                                    additional_batch = additional_batch,
                                                    context_batch = context_batch,
                                                    use_context = args.use_context,
                                                    embed_scale = embed_scales[lossid],
                                                    label_id = label_ids[lossid],
                                                    keyword = keywords[lossid], 
                                                    original_preds = original_preds, 
                                                    kweight = new_kweight,
                                                    step = step
                                                )
                                            losslists[lossid][-1].append(lossvalue.sum().item())
                                            losses_for_backward.append(lossvalue)
                                            logging_outputs.append(logging_output)
                                        optimizer.zero_grad(set_to_none = True)
                                        outputs.zero_grad()
                                        
                                        if len(losses) > 1:
                                            optimizer_lambda.zero_grad(set_to_none=True)
                                            lambda_.zero_grad()
                                        
                                        for model in name2model.values():
                                            model.zero_grad(set_to_none=True)
                                        
                                        # linear_scale == True ì¼ ë•Œ (cold ê°™ì€ ì„¸íŒ…ì¼ ë•Œ) betasë¥¼ ì‚¬ìš©í•´ì„œ weighted sumì„ í•œë‹¤.
                                        if args.linear_scale == "true":
                                            
                                            total_loss = 0
                                            cur_epsilons = []
                                            for sid in range(len(losses_for_backward)):
                                                total_loss = total_loss + betas[sid] * losses_for_backward[sid]
                                                cur_epsilons.append(0.0)
                                            
                                            total_batchloss = total_loss.sum()
                                            optimizer.backward(total_batchloss, retain_graph=False, scaler=scaler)
                                        # Lagrangianìœ¼ë¡œ ì‹ì„ ì„¸ì› ì„ ë•ŒëŠ” betaë¥¼ ì•ˆì“°ê³  lambda_.get_lossë¥¼ ì´ìš©í•œë‹¤.
                                        else:
                                            
                                            total_loss = 0.0
                                            total_loss = losses_for_backward[0] # perplexity (autoregressive loss)
                                            
                                            cur_epsilons = []
                                            
                                            constraint_values = []
                                            for sid in range(1, len(losses_for_backward)): # secondary losses or constraints
                                                cur_epsilon = get_epsilon(step, epsilons[sid-1], min_epsilons[sid-1], epsilon_warmup_steps[sid-1], epsilon_cooldown_steps[sid-1], epsilon_decay_functions[sid-1])
                                                # constraint value
                                                constraint_value = (cur_epsilon - losses_for_backward[sid]).detach()
                                                damp = args.dampness * constraint_value
                                                # ì™œ maskë¥¼ ì¼ë‹¨ ë§Œë“¤ê¹Œ? -> if constraint is satified and lambda < damp, then don't use lambdas to update thetas
                                                # ìœ„ ì„¤ëª…ì„ ë³´ê³  ì½”ë“œì— ëŒ€í•œ í•´ì„: 
                                                #    dampê°€ 0 ë³´ë‹¤ í¬ë©´ losses_for_backward[sid]ê°€ cur_epsilon ë³´ë‹¤ ì‘ì€ ê²ƒ. -> constraintë¥¼ ë§Œì¡±í•˜ëŠ” ê²ƒ.
                                                #    ??? lambdaê°€ damp ë³´ë‹¤ ì‘ë‹¤ëŠ” ê²ƒì€,, ì˜ ëª¨ë¥´ê² ë„¤ ã…‹ã…‹ã…‹ã…‹ ã… ã… ;; ì™œ ì € ì¡°ê±´ë„ í•„ìš”í•œê±¸ê¹Œ?
                                                #    ì•„ë¬´íŠ¼ 2ê°€ì§€ ì¡°ê±´ ëª¨ë‘ ë§Œì¡±í•˜ë©´ maskê°’ì´ 0ì´ ë¨.
                                                # def get_mask(self, i, damp):
                                                #     # if constraint is satified and lambda < damp, then don't use lambdas to update thetas
                                                #     return 1 - damp.ge(0.).float() * self.lambda_[i].data.le(damp).float()
                                                mask = lambda_.get_mask(sid-1, damp)
                                                
                                                # ê·¼ë° damp ì¸ìë¡œ damp * maskë¥¼ ë„£ì–´ì£¼ëŠ”ë°, ê·¸ëŸ¬ë©´ self.lambda[i] - 0 ì•„ë‹Œê°€? ì´ê²Œ ì–´ì§¸ì„œ don't use lambdas to update thetas ì¸ê±´ê°€..
                                                # def get_loss(self, i, damp, loss):
                                                #     return (self.lambda_[i] - damp) * loss
                                                closs_for_theta = lambda_.get_loss(sid-1, damp * mask, (cur_epsilon - losses_for_backward[sid]))
                                                total_loss = total_loss - closs_for_theta
                                                
                                                cur_epsilons.append(cur_epsilon)
                                                constraint_values.append(constraint_value.item())
                                            total_batchloss = total_loss.sum()
                                            optimizer.backward(total_batchloss, retain_graph=False, scaler=scaler)                            

                                    indices=[5,6] # indices to edit
                                    #indices = locate_indices_all[sample_idx]
                                    if logging_outputs[0].get('entropy', None) is not None:
                                        optimizer.step(indices, scaler=scaler, entropy=logging_outputs[0].get('entropy', None))
                                    else:
                                        optimizer.step(indices, scaler=scaler)
                                    
                                    # start : lambdaë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•œ ì½”ë“œ + EmbedGDì˜ learning rateë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•œ ì½”ë“œ
                                    update_lr_condition = "none"
                                    
                                    if args.linear_scale != "true" and len(losses) > 1:
                                        sats = torch.Tensor(constraint_values).ge(0.).to(device) # constraint_valuesê°€ 0 ë³´ë‹¤ í¬ë©´, epsilon - loss > 0 that is, epsilon > loss ì´ë¯€ë¡œ constraint ë§Œì¡±
                                        update_lambda_condition = (step % args.lambda_update == 0)
                                        lambda_mask = float(update_lambda_condition) * torch.ones_like(sats) # update_lambda_condition ì´ True ì´ë©´, lambda_maskëŠ” [1,1,1..,1] ì´ê³  ì•„ë‹ˆë©´ [0,0,0...,0]
                                                                                
                                        lambda_mask += (1 - sats.float()) * (lambda_.is_zero()) # constraintë¥¼ ë§Œì¡±í•œ constraintì—ëŠ” 0ì´ ë”í•´ì§€ê³ , ë§Œì¡±í•˜ì§€ ì•Šì€ constraintì—ëŠ” lambdaê°€ 0ì´ ì•„ë‹ˆë¼ë©´ update_lambda_conditionì´ Falseì—¬ë„ ì—…ë°ì´íŠ¸ í•œë‹¤.s
                                        
                                    total_batchlossitem = losses_for_backward[0].item()
                                    if dynamic_lambda_update_prev_loss is not None and abs(total_batchlossitem - dynamic_lambda_update_prev_loss) <= 1e-6:
                                        repeat_counts[0] += 1
                                        if args.linear_scale != "true" and len(losses) > 1 and args.dynamic_lambda_update:
                                            lambda_mask = (1 - sats.float()) # constraintë¥¼ ë§Œì¡±í•˜ì§€ ì•Šì€ constraintë§Œ 1ì„.
                                            
                                        if args.dynamic_lr_update and best_allsat[0] is not None and best_allsat[0]:
                                            update_lr_condition = "increase"
                                    else:
                                        repeat_counts[0] = 1
                                    
                                    dynamic_lambda_update_prev_loss = total_batchlossitem
                                    
                                    if update_lr_condition == "increase":
                                        cur_lr = optimizer._optimizer.update_lr(min(cur_lr + args.lr_update_size, args.max_lr))
                                        
                                    if args.linear_scale != "true" and len(losses) > 1:
                                        optimizer_lambda._optimizer.set_mask(lambda_mask.clamp(max = 1.0, min = 0.0)) # epsilon ê°’ì„ ë§Œì¡±í•˜ì§€ ì•Šì€ constraintì— ëŒ€í•´ì„œë§Œ, lambdaë¥¼ update.
                                        optimizer_lambda.step() # lambdaë¥¼ update.
                                        lambda_.make_positive()
                                    # end 
                                    
                                    gc.collect()
                                    
                                    # best...ë¥¼ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë¶€ë¶„
                                    cur_losses = []
                                    for b in range(batch_size):
                                        cur_loss = 0.0
                                        for beta, lossval in zip(betas, losses_for_backward):
                                            cur_loss = cur_loss + beta * lossval[b].item()
                                        cur_losses.append(cur_loss)
                                        
                                        constrained = []
                                        allsat = True
                                        for i in range(1, len(losses)):
                                            if losses_for_backward[i] <= min_epsilons[i-1]:
                                                constrained.append("sat")
                                            else:
                                                constrained.append("vio")
                                                allsat = False
                                                
                                        if args.show_all_outputs and len(losses) > 1 and allsat:
                                            best_prediction_set[b].add(target_sents[b])
                                        
                                        constrained = ",".join(constrained)
                                        
                                        modify_condition = \
                                            args.selection_criterion == "last" or\
                                                (best_loss[b] is None and args.selection_criterion == "weighted_sum") or\
                                                (best_loss[b] is not None and args.selection_criterion == "weighted_sum" and best_loss[b] > cur_loss)
                                        
                                        if not modify_condition and args.selection_criterion == "mrr_allsat":
                                            modify_condition = \
                                                (best_loss[b] is None and allsat and repeat_counts[b] == 2) or \
                                                (best_loss[b] is not None and best_allsat[b] and allsat and repeat_counts[b] == 2)
                                        elif not modify_condition and args.selection_criterion == "primary_allsat":
                                            modify_condition = \
                                                (best_loss[b] is None and allsat) or \
                                                (best_loss[b] is not None and not best_allsat[b] and allsat) or \
                                                (best_allsat[b] and allsat and best_loss[b] > cur_loss)
                                                
                                        if modify_condition:
                                            if args.dynamic_lr_update:
                                                print("resetting the learning rate and noise std, a constraint has been satisfied")
                                                cur_lr = optimizer._optimizer.update_lr(args.lr)
                                                optimizer._optimizer.set_begin_std(0.01)
                                            if args.selection_criterion != "last":
                                                print(f"modify condition @{step}", time.time() - starttime, end  ="\n")
                                            best_loss[b] = cur_loss
                                            best_allsat[b] = allsat
                                            best_repeat_count[b] = repeat_counts[b]
                                            for i in range(len(losses)):
                                                best_losses[i][b] = losses_for_backward[i][b].item()
                                                
                                            best_pred_tokens[b] = pred_tokens[b]
                                            best_index[b] = step
                                            
                                            best_constrained = constrained
                                            best_step = step 
                                            
                                    if not args.time and step > 0 and step % args.log_interval == 0:
                                        if len(losses) > 1:
                                            log = f"beam cons: {predicted_allsat};"
                                            log = f"Step {step}: lr: {cur_lr}; total_loss: {total_batchloss:.4f}; current [loss:{sum(cur_losses):.4f}; l:{','.join([f'{x:.4f}' for x in lambda_().tolist()])}; e:{','.join(f'{x:.4f}' for x in cur_epsilons)}]; cons: {constrained}; "
                                            for i in range(len(losslists)):
                                                log = log + f" {lossabbr[i]}:{losslists[i][-1][-1]:.4f}; "
                                            
                                            if best_loss[0] is not None:
                                                log = log[:-1] + f"] |||| best [cur_loss:{sum(best_loss):.4f}; cons: {best_constrained}; "
                                                for i in range(len(best_losses)):
                                                    log = log + f"{lossabbr[i]}:{sum(best_losses[i]):.4f}; "
                                                log = log[:-1] + f"@ step #{best_index[-1]}"
                                                log = log + "]"
                                            else: 
                                                log = log[:-1] + f"] |||| best [none of the generations so far satisfies constraints]"
                                        else:
                                            log = f"Step {step}: lr:{cur_lr}; loss:{total_batchloss:.4f}; current [loss:{sum(cur_losses):.4f};"
                                            for i in range(len(losslists)):
                                                log = log + f" {lossabbr[i]}:{losslists[i][-1][-1]:.4f}; "
                                                
                                            if best_loss[0] is not None:
                                                log = log[:-1] + f"] best [loss:{sum(best_loss):.4f} "
                                                for i in range(len(best_losses)):
                                                    log = log + f"{lossabbr[i]}:{sum(best_losses[i]):.4f}; "
                                                log = log[:-1] + f" at step {best_index[-1]}"
                                                log = log + "]"
                                            else:
                                                log = log[:-1] + f"] |||| best [none of the generations so far satisfies constraints]"

                                            print(log, end="\n")
                                    
                                    del losses_for_backward
                                    
                                    # early stopping ê´€ë ¨ëœ ë¡œì§. ì¡°ê±´ì— í•´ë‹¹í•˜ë©´ optim_stepsë§Œí¼ for loopì„ ëŒì§€ ì•Šì•„ë„ break.
                                    if args.early_stop_steps > 0:
                                        # selection_criterionì´ allsatì´ í¬í•¨ë˜ì–´ ìˆê³ , best_allsat[0]ì´ true ì¸ ê²½ìš°
                                        # selection_criterionì´ weighted_sumì´ë‚˜ last ì¸ ê²½ìš°
                                        # early_stop_condition = True ì´ë‹¤.
                                        early_stop_condition = \
                                            ("allsat" in args.selection_criterion and best_allsat[0]) or\
                                            (args.selection_criterion == "weighted sum") or \
                                            (args.selection_criterion == "last")
                                        
                                        # lossê°€ í° ë³€í™”ê°€ ì—†ìœ¼ë©´ patience count +1
                                        if prev_loss is not None and abs(cur_loss - prev_loss) <= 1e-6:
                                            same_loss_count += 1
                                            
                                        else: 
                                            same_loss_count = 0
                                        
                                        # conditionì„ ë§Œì¡±í•˜ë©´ì„œ, loss ë³€í™”ê°€ args.early_stop_steps ì´ìƒ ì—†ì—ˆë˜ ê²½ìš°
                                        if early_stop_condition and same_loss_count >= args.early_stop_steps:
                                            print(f"Early stop at @{step} with a loss value of {cur_loss} and satisfied constraints")
                                            break
                                        # conditionì„ ë§Œì¡±í•˜ì§€ëŠ” ì•Šì§€ë§Œ, loss ë³€í™”ê°€ args.early_stop_steps + 100 ì´ìƒì¸ ê²½ìš°
                                        elif same_loss_count >= args.early_stop_steps + 100:
                                            print(f"Early stop at @{step} with a loss value of {cur_loss} and unsatisfied constraints")
                                            break
                                        
                                        prev_loss = cur_loss
                                        
                                except KeyboardInterrupt:
                                    print("skipping remaining optimizing steps and showing the best option so far")
                                    broken = True
                                    break
                            # end for loop for gradient-based inference
                                
                            predictions = []
                            prediction_idss = []
                            broken_skip = False
                            skip_printing = False
                            for b, item in enumerate(best_pred_tokens):
                                if item is None and broken: # best_pred_tokensê°€ ì €ì¥ë˜ê¸°ë„ ì „ì— keyboard interruptê°€ ìˆì—ˆë˜ ê²½ìš°
                                    skip_printing = True
                                    if broken: 
                                        broken_skip = input("Skip this input entirely? yes(y)/no(continue)/press ctrl+c to exit")
                                        broken_skip = broken_skip.lower() == "y"
                                        break
                                # ê¼­ mucola outputì´ì–´ì•¼ í•œë‹¤ëŠ” ì¡°ê±´ì´ ì—†ê³ , best modificationë„ ì œì•½ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±í•˜ì§€ ëª»í–ˆì„ ë•Œ
                                # ë˜ëŠ” best modificationì´ë¼ê³  ì €ì¥ëœ ê²ƒì´ ì—†ì„ ë•Œ
                                # -> AR prediction ì‚¬ìš©
                                if (args.only_mucoco == "false" and not best_allsat[b]) or (item is None):
                                    prediction_ids = ", ".join([str(idx) for idx in AR_predicted_indices[0].tolist()])
                                    prediction_indices = AR_predicted_indices[0].tolist()
                                    prediction = AR_prediction
                                    
                                    lossvalue = 0.0
                                    for lossid in range(len(betas)):
                                        lossvalue += betas[lossid] * predictedlosslists[-1][lossid][b]
                                    print(f"best prediction is from beam search, all constraints were not satisfied, allsat={lengthwise_best_prediction[b][2]}")
                                else:
                                    # ì•„ë‹Œ ê²½ìš°ëŠ” best modificaitonì„ lengthwise_best_prediction ì— ì €ì¥..
                                    prediction_ids = ", ".join([str(x) for x in target_prefix[b].tolist()])
                                    prediction_ids += f'[{", ".join([str(x) for x in item.tolist()])}]'
                                    prediction_indices = target_prefix[b].tolist() + item.tolist()
                                    
                                    targets = clean_output(item.tolist(), primary_tokenizer.eos_token_id, allow_first_eos=losses[0]=="bart")
                                    if args.target_tokenize_different:
                                        with primary_tokenizer.as_target_tokenizer():
                                            prediction = primary_tokenizer.decode(target_prefix[b].tolist() + targets)
                                    else:
                                        prediction = primary_tokenizer.decode(target_prefix[b].tolist() + targets)
                                    
                                    print("best prediction at step", best_index[b])
                                    lossvalue = best_loss[b]
                                    
                                    modify_condition =\
                                        lengthwise_best_prediction[b] is None or \
                                        (args.selection_criterion == "weighted_sum" and lengthwise_best_prediction[b][1] > lossvalue)
                                    
                                    if not modify_condition and args.selection_criterion == "primary_allsat":
                                        modify_condition = \
                                            (not lengthwise_best_prediction[b][2] and best_allsat[b]) or \
                                            (lengthwise_best_prediction[b][2] and best_allsat[b] and lengthwise_best_prediction[b][1] > lossvalue)
                                    elif not modify_condition and args.selection_criterion == "mrr_allsat":
                                        modify_condition =\
                                            (not lengthwise_best_prediction[b][2] and best_allsat[b] and best_repeat_count[b] >= 2) or \
                                            (lengthwise_best_prediction[b][2] and lengthwise_best_prediction[b][4] >= 2 and lengthwise_best_prediction[b][1] > lossvalue)
                                    
                                    if modify_condition:
                                        if args.debug:
                                            print("modify condition satisfied", end="\n")
                                        else:
                                            outallsatf.write("modify_contition satisfied ")
                                        
                                        lengthwise_best_prediction[b] = (prediction, lossvalue, best_allsat[b], prediction_indices, best_repeat_count[b])
                                        intermediate_result.update({"best_step": best_index[b],
                                                                    "best_prediction": prediction})

                                prediction_idss.append(prediction_ids)
                                predictions.append(prediction)
                                
                            all_stepcounts += best_index
                            
                            optimizer.zero_grad(set_to_none = True)
                            del outputs
                            del optimizer
                            if len(losses) > 1:
                                optimizer_lambda.zero_grad()
                                del optimizer_lambda
                                del lambda_
                            for modelname in loss2modelname.values():
                                name2model[modelname].zero_grad(set_to_none=True)                                        
                            torch.cuda.empty_cache()
                        
                        # í•´ë‹¹ sampleì— ëŒ€í•´ì„œ lengthwise_best_predictionì„ íŒŒì¼ì— ì €ì¥
                        b = 0
                        if lengthwise_best_prediction[b] is None or not lengthwise_best_prediction[b][2]:
                            if restart_idx < args.restarts:
                                continue # skip printing and loop over
                            elif lengthwise_best_prediction[b] is None:
                                lengthwise_best_prediction = [("", -1, False, [], -1)]
                        
                        if args.debug:
                            if not skip_printing:
                                for b in range(batch_size):
                                    print("sample #" + str(sample_idx), f"repeat count: {lengthwise_best_prediction[b][4]}", "best prediction for all lengths: ", lengthwise_best_prediction[b][0].strip().replace("\n", " ") + "\n")
                        else:
                            if args.output_style == "text":
                                for b in range(batch_size):
                                    # recall: lengthwise_best_prediction = [(AR_prediction, total_predicted_loss, predicted_allsat, predicted_batch[0].tolist(), -1)]
                                    outf.write(lengthwise_best_prediction[b][0].strip().replace("\n", " ") + "\n") # AR_prediction
                                    outf.flush()
                                    outallsatf.write(str(lengthwise_best_prediction[b][2]) + "\n") # predicted_allsat
                                    outallsatf.flush()
                            else:
                                if sample_idx == 0: # í•œ prompt ì— ëŒ€í•´ì„œ ì²«ë²ˆì§¸ sample ì¼ ê²½ìš°
                                    output = {
                                        "prompt": {
                                            "text": source_text,
                                            "tokens": source_indices_write},
                                        "generations": [{
                                            "text": lengthwise_best_prediction[b][0],
                                            "tokens": lengthwise_best_prediction[b][3],
                                            "allsat": lengthwise_best_prediction[b][2],
                                            "repeat_count": lengthwise_best_prediction[b][4],
                                            "mucoco": True 
                                        }]
                                    }
                                else:
                                    output['generations'].append(
                                        {
                                            "text": lengthwise_best_prediction[b][0],
                                            "tokens": lengthwise_best_prediction[b][3],
                                            "allsat": lengthwise_best_prediction[b][2],
                                            "repeat_count": lengthwise_best_prediction[b][4],
                                            "mucoco": True
                                        }
                                    )
                                    
                                # í•œ prompt ì— ëŒ€í•´ì„œ ë§ˆì§€ë§‰ sample ì¼ ê²½ìš°, outputì— ì¶”ê°€í•œ í›„ dump ê¹Œì§€ í•´ì¤€ë‹¤.
                                if sample_idx + 1 == args.num_samples: 
                                    json.dump(output, outf)
                                    outf.write("\n")
                                    outf.flush()
                                    
                                    outallsatf.write(str(lengthwise_best_prediction[b][2]) + "\n")
                                    outallsatf.flush()
                                
                                output2 = intermediate_result
                                json.dump(output2, outf2)
                                outf2.write("\n")
                                outf2.flush()
                        print(f"required output achieved or number of restarts ran out at attempt #{restart_idx + 1}")
                        break
                            
                    else: # skip ì¼ ê²½ìš° == skipping mucola and writing beam search output 
                        if ask_skip != "y": # ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë´ì„œ skip í•˜ê¸°ë¡œ í•œê²Œ ì•„ë‹ˆë©´
                            ## args.debug ì¼ ë•ŒëŠ” ê³¼ê°íˆ ìƒëµ
                            print("Skipping this example. the beam search output already satisfies all the constraints or there's no constraints")
                            # output styleì— ë”°ë¼ ë‹¤ë¥¸ í¬ë§·ìœ¼ë¡œ ì €ì¥
                            if args.output_style == "text":
                                for b in range(batch_size):
                                    # recall: lengthwise_best_prediction = [(AR_prediction, total_predicted_loss, predicted_allsat, predicted_batch[0].tolist(), -1)]
                                    outf.write(lengthwise_best_prediction[b][0].strip().replace("\n", " ") + "\n") # AR_prediction
                                    outf.flush()
                                    outallsatf.write(str(lengthwise_best_prediction[b][2]) + "\n") # predicted_allsat
                                    outallsatf.flush()
                            else:
                                for b in range(batch_size):
                                    if sample_idx == 0: # í•œ prompt ì— ëŒ€í•´ì„œ ì²«ë²ˆì§¸ sample ì¼ ê²½ìš°
                                        output = {
                                            "prompt": {
                                                "text": source_text,
                                                "tokens": source_indices_write},
                                            "generations": [{
                                                "text": lengthwise_best_prediction[b][0],
                                                "tokens": lengthwise_best_prediction[b][3],
                                                "allsat": lengthwise_best_prediction[b][2],
                                                "mucoco": False # !!! AR output ì„ì´ í‘œì‹œë˜ì–´ ìˆêµ¬ë‚˜
                                            }]
                                        }
                                    else:
                                        output['generations'].append(
                                            {
                                                "text": lengthwise_best_prediction[b][0],
                                                "tokens": lengthwise_best_prediction[b][3],
                                                "allsat": lengthwise_best_prediction[b][2],
                                                "mucoco": False # !!! AR output ì„ì´ í‘œì‹œë˜ì–´ ìˆêµ¬ë‚˜
                                            }
                                        )
                                        
                                # í•œ prompt ì— ëŒ€í•´ì„œ ë§ˆì§€ë§‰ sample ì¼ ê²½ìš°, outputì— ì¶”ê°€í•œ í›„ dump ê¹Œì§€ í•´ì¤€ë‹¤.
                                if sample_idx + 1 == args.num_samples: 
                                    json.dump(output, outf)
                                    outf.write("\n")
                                    outf.flush()
                    break # restart í•˜ëŠ” loopì„ íƒˆì¶œ (don't restart)
        
        # 1ê°œ prompt ì— ëŒ€í•œ ì‘ì—…ì´ ë‹¤ ëë‚˜ë©´, source_batch ë“±ë“±ì„ ìƒˆë¡œ initialize í•©ë‹ˆë‹¤.
        del source_batch
        del target_batch
        del additional_batch
        del for_predicted_source_batch
        del predicted_batch
        del context_batch
        source_batch, target_batch, additional_batch, for_predicted_source_batch, predicted_batch, context_batch = [], [], [], [], [], []
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "197d83cc-bc2e-4f06-b0df-6020cee39916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/hyeryung/mucoco\")\n",
    "os.chdir(\"/home/hyeryung/mucoco\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM\n",
    "import wandb\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "import mucoco.losses as lossbuilder\n",
    "import mucoco.options as options\n",
    "import mucoco.utils as utils\n",
    "from mucoco.utils import TargetProbability, TargetEmbeddings, TargetSimplex, Lambda, Optimizer, OptimizerLE, get_epsilon, locate\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d91ed6-4832-4d53-82d4-cf15d46e4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyperparemeters\n",
    "config = dict(\n",
    "early_stopping_patience=-1,\n",
    "method='mlm',\n",
    "selection_criteria = \"weighted_sum\", #\"allsat_primary\",\n",
    "num_edit_token_per_step = 4,\n",
    "locate_unit = \"token\",\n",
    "k_per_location = 3,\n",
    "loss_weights = [0.5, 0.5],\n",
    "min_epsilons = [-3],\n",
    "n_iter = 2,\n",
    "num_samples = 1,#10,\n",
    "device = \"cuda\",\n",
    "target_type='embeds',\n",
    "cache_dir='hf_cache',\n",
    "jsonl_primary_key = \"prompt\",\n",
    "jsonl_secondary_key = \"text\",\n",
    "losses = ['gpt2', 'classification_no_prefix'],\n",
    "build_loss_dict={'coeff_steps': 200, \n",
    "                'coeff_pattern': 'constant',\n",
    "                'loss_type': 'dotplusplus',\n",
    "                'length_normalize': False,\n",
    "                'AR_temperature': 1.0,\n",
    "                'AR_top_k': 0,\n",
    "                'AR_top_p': 0.96,\n",
    "                'max_output_length': 20},\n",
    "model_paths=['gpt2-large',\n",
    "            'models/models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best'],\n",
    "tokenizer_paths=['gpt2-large',\n",
    "                'models/models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best'],\n",
    "model_types=['AutoModelForCausalLM',\n",
    "            'RobertaCustomForSequenceClassification'],\n",
    "source_data = 'new_module/toxicity-avoidance/data/testset_gpt2_2500.jsonl'\n",
    ")\n",
    "\n",
    "display_name = f\"tmp-mlm-{config['locate_unit']}-nps{config['num_edit_token_per_step']}-k{config['k_per_location']}-{config['selection_criteria']}-{config['loss_weights'][0]}-{config['loss_weights'][1]}-wandb\"\n",
    "# run = wandb.init(project=\"mucola\", config=config, name=display_name)\n",
    "\n",
    "class dummyArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "build_loss_args=dummyArgs(**config['build_loss_dict'])\n",
    "\n",
    "## logging-related\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(message)s')\n",
    "logger = logging.getLogger(\"le\")\n",
    "logger.setLevel(os.environ.get('LOGGING_LEVEL', logging.DEBUG))\n",
    "\n",
    "outdir = os.path.join('outputs/toxicity/mlm-reranking', display_name\n",
    "                      )\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "outfile = f\"{outdir}/outputs_epsilon{config['min_epsilons'][0]}.txt\"\n",
    "outf = open(outfile, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5e5cf1-81f8-4e9a-ab66-c955d1a2dc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/vocab.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/generation_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/vocab.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "## load data\n",
    "source_dataset = [json.loads(l)[config['jsonl_primary_key']][config['jsonl_secondary_key']] for l in open(config['source_data'])]\n",
    "generation_dataset = [json.loads(l)[\"generations\"] for l in open(config['source_data'])]\n",
    "# config['source_data = 'new_module/toxicity-avoidance/data/testset_jigsaw_1960.jsonl'\n",
    "# source_dataset = [\"\" for l in open(config['source_data'])]\n",
    "# generation_dataset = [json.loads(l)[\"source\"] for l in open(config['source_data'])]\n",
    "\n",
    "## load tokenizer, models, define losses\n",
    "name2tokenizer = {}\n",
    "name2model = {}\n",
    "name2config = {}\n",
    "loss2modelname = {}\n",
    "loss2tokenizer = {}\n",
    "embed_luts = []\n",
    "embed_scales = []\n",
    "prev_vocab_size = None\n",
    "vocab_size = None\n",
    "primary_vocab_size = None\n",
    "\n",
    "for i, model_path in enumerate(config['model_paths']):\n",
    "    if model_path not in name2model: #making sure we are not loading the model twice in case some constraints use the same model. \n",
    "        name2tokenizer[model_path] = AutoTokenizer.from_pretrained(config['tokenizer_paths'][i], cache_dir=config['cache_dir'],  use_fast=True)\n",
    "        name2config[model_path] = AutoConfig.from_pretrained(model_path, cache_dir=config['cache_dir'])\n",
    "\n",
    "        if config['model_types'][i] == \"sentence-transformer\":\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(SentenceTransformer(model_path))\n",
    "        elif \"Custom\" in config['model_types'][i]:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(getattr(utils, config['model_types'][i]).from_pretrained(model_path, config=name2config[model_path], cache_dir=config['cache_dir']))\n",
    "        else:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(getattr(transformers, config['model_types'][i]).from_pretrained(model_path, config=name2config[model_path], cache_dir=config['cache_dir']))\n",
    "        name2model[model_path].eval()\n",
    "        name2model[model_path].cuda()\n",
    "        embed_lut_ = name2model[model_path].get_input_embeddings()\n",
    "        if isinstance(embed_lut_, torch.nn.Sequential):\n",
    "            new_vocab_size = embed_lut_[0].num_embeddings\n",
    "        else:\n",
    "            new_vocab_size = embed_lut_.num_embeddings\n",
    "        if prev_vocab_size is None:\n",
    "            vocab_size=new_vocab_size\n",
    "        prev_vocab_size = vocab_size\n",
    "    \n",
    "    input_embeds = name2model[model_path].get_input_embeddings()\n",
    "    if isinstance(input_embeds, torch.nn.Sequential):\n",
    "        input_embeds = input_embeds[0]\n",
    "    embed_luts.append(input_embeds)\n",
    "    \n",
    "    if config['target_type'] == \"embeds\":\n",
    "        embed_luts[-1].requires_grad=False\n",
    "    \n",
    "    if i == 0:\n",
    "        primary_vocab_size = vocab_size\n",
    "        primary_embed_dim = embed_luts[-1].embedding_dim\n",
    "    \n",
    "    if getattr(name2model[model_path], \"get_decoder\", None) is None: #this is for MarianMT models which have a weird embedding_scale parameter\n",
    "        embed_scales.append(1.0)\n",
    "    else:\n",
    "        embed_scales.append(getattr(name2model[model_path].get_decoder(), \"embed_scale\", 1.0))\n",
    "\n",
    "lossfns = []\n",
    "for i, loss in enumerate(config['losses']):\n",
    "    lossfns.append(lossbuilder.build_loss(loss, name2model[config['model_paths'][i]], name2tokenizer[config['model_paths'][i]], build_loss_args))\n",
    "    loss2modelname[loss] = config['model_paths'][i]\n",
    "    loss2tokenizer[loss] = name2tokenizer[config['model_paths'][i]]\n",
    "primary_tokenizer = loss2tokenizer[config['losses'][0]]\n",
    "\n",
    "## for editing\n",
    "model_checkpoint = \"roberta-base\"\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "mlm = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "primary_tokenizer.add_special_tokens({'mask_token':mlm_tokenizer.mask_token})\n",
    "primary_mask_token_id = primary_tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d58868-5d80-489c-95cf-5c8ba2180030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Original output\n",
      "dirt. Unfortunately the majority of horses end up in shit that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?\n",
      "iter 0, sample_idx: 0\n",
      "located indices: [[0, 25, 10, 18]]\n",
      "located indices:  dirt myself shit My\n"
     ]
    }
   ],
   "source": [
    "## ------------------------- beginning of main logic ------------------------- ##\n",
    "text_id = 0\n",
    "source_text = source_dataset[text_id]\n",
    "source_indices = primary_tokenizer.encode(source_text, return_tensors=\"pt\").to(config['device'])\n",
    "source_batch = torch.cat([source_indices], dim=0).to(config['device'])\n",
    "\n",
    "predicted_batches = [x[\"tokens\"] for x in generation_dataset[text_id]]\n",
    "predicted_batches = [torch.tensor([x], dtype=torch.long, device=config['device']) for x in predicted_batches]\n",
    "AR_prediction_all = [x[\"text\"] for x in generation_dataset[text_id]]\n",
    "\n",
    "sample_idx = 0\n",
    "predicted_batch = predicted_batches[sample_idx].cuda()\n",
    "AR_prediction = AR_prediction_all[sample_idx]\n",
    "\n",
    "logger.critical(\"Original output\")\n",
    "logger.critical(AR_prediction)\n",
    "\n",
    "# ## check if toxicity less than threshold\n",
    "# gold_losses = []\n",
    "# label_ids = [0, 0]\n",
    "# keywords = [\"the\" for _ in config['losses']]\n",
    "# new_kweight = 5.0\n",
    "# use_context = 'false'\n",
    "# allsat = True\n",
    "# additional_batch = source_batch\n",
    "# context_batch = [None]\n",
    "# gold_losses = []\n",
    "# for lossid, lossname in enumerate(config['losses']):\n",
    "#     with torch.no_grad():\n",
    "#         lossvalue, logging_output =\\\n",
    "#             lossfns[lossid].compute_gold_loss(\n",
    "#                 # (source_batch, target_batch), # bug: if it's target_batch, we're inputting 2 copies of source_batch\n",
    "#                 (source_batch, predicted_batch), \n",
    "#                 additional_batch=additional_batch, \n",
    "#                 context_batch=context_batch,\n",
    "#                 use_context=use_context,\n",
    "#                 label_id=label_ids[lossid],\n",
    "#                 keyword=keywords[lossid],\n",
    "#                 kweight=new_kweight\n",
    "#             )\n",
    "#     gold_losses.append(lossvalue.squeeze().item())\n",
    "#     if (lossid >= 1) and (gold_losses[lossid] > config['min_epsilons'][lossid - 1]):\n",
    "#         allsat = False\n",
    "\n",
    "# if allsat:\n",
    "#     logger.info(f\"skipping this sample since it already satisfies constraint. {gold_losses}\")\n",
    "#     if sample_idx == 0:\n",
    "#         output = {\n",
    "#             \"prompt\":{\n",
    "#                 \"text\":source_text,\n",
    "#                 \"tokens\":source_indices.tolist()\n",
    "#                 }, \n",
    "#             \"generations\":[{\n",
    "#                 \"text\": \"\",\n",
    "#                 \"tokens\": [],\n",
    "#                 \"indices\": [[]], \n",
    "#                 \"allsat\": -1,\n",
    "#                 \"losses\": gold_losses,\n",
    "#                 \"weighted_loss\": -1\n",
    "#                 }]\n",
    "#         }\n",
    "#     else:\n",
    "#         output['generations'].append(\n",
    "#             {\n",
    "#                 \"text\": \"\",\n",
    "#                 \"tokens\": [],\n",
    "#                 \"indices\": [[]], \n",
    "#                 \"allsat\": -1,\n",
    "#                 \"losses\": gold_losses,\n",
    "#                 \"weighted_loss\": -1\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#     if sample_idx + 1 == config['num_samples']:\n",
    "#         json.dump(output, outf)\n",
    "#         outf.write(\"\\n\")\n",
    "#         outf.flush()\n",
    "    \n",
    "# else:\n",
    "    \n",
    "es_patience_count = 0\n",
    "best_ix, best_prediction, best_text, best_allsat, best_losses, best_weighted_loss = None, None, None, None, None, None\n",
    "\n",
    "_iter = 0\n",
    "# for _iter in range(config['n_iter']):\n",
    "## locate tokens to edit\n",
    "batch = {\"input_ids\": predicted_batch}\n",
    "indices = locate(name2model[config['model_paths'][1]], \n",
    "                name2tokenizer[config['model_paths'][1]], \n",
    "                batch, \n",
    "                max_num_tokens=config['num_edit_token_per_step'], \n",
    "                unit=config['locate_unit'], \n",
    "                use_cuda=True)\n",
    "logger.debug(f\"iter {_iter}, sample_idx: {sample_idx}\")\n",
    "logger.debug(f\"located indices: {indices}\")\n",
    "logger.debug(f\"located indices: {name2tokenizer[config['model_paths'][1]].decode(predicted_batch[:, indices].squeeze())}\")\n",
    "# logger.debug(f\"located indices: {predicted_batch[:, indices]}\")\n",
    "\n",
    "## replace tokens at the indices with mask tokens\n",
    "masked_sequence = predicted_batch.clone().detach()\n",
    "for i in indices:\n",
    "    masked_sequence[:, i] = primary_mask_token_id\n",
    "masked_sequence_text = primary_tokenizer.batch_decode(masked_sequence.tolist())\n",
    "inputs = mlm_tokenizer(masked_sequence_text, return_tensors=\"pt\")\n",
    "\n",
    "# ## c.f. check if spaces are preserved. -> preserved! checked.\n",
    "# logger.debug(inputs['input_ids'])\n",
    "# logger.debug(mlm_tokenizer.decode(inputs['input_ids'][0]))\n",
    "\n",
    "## make predictions for the masked indices\n",
    "with torch.no_grad():\n",
    "    logits = mlm(**inputs).logits\n",
    "mask_token_index = (inputs.input_ids == mlm_tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "## get top k tokens for each index \n",
    "predicted_token_ids = torch.topk(logits[0, mask_token_index], k=config['k_per_location'], dim=-1)\n",
    "# logger.debug(predicted_token_ids) # shape : (config['num_edit_token_per_step'],  config['k_per_location'])\n",
    "\n",
    "# logger.debug(predicted_token_ids)\n",
    "# torch.return_types.topk(\n",
    "# values=tensor([[16.153, 15.676, 15.537],\n",
    "#         [14.131, 13.642, 13.477],\n",
    "#         [12.802, 12.533, 12.361],\n",
    "#         [19.653, 16.148, 15.160]]),\n",
    "# indices=tensor([[32033,  1274,  5458],\n",
    "#         [ 3993,   697,   342],\n",
    "#         [   11,   106,    13],\n",
    "#         [  106,    24,    15]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2535cc-dd00-420e-bfaa-1ad31e53bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_losses = []\n",
    "label_ids = [0, 0]\n",
    "keywords = [\"the\" for _ in config['losses']]\n",
    "new_kweight = 5.0\n",
    "use_context = 'false'\n",
    "allsat = True\n",
    "additional_batch = source_batch\n",
    "context_batch = [None]\n",
    "gold_losses = []\n",
    "for lossid, lossname in enumerate(config['losses']):\n",
    "    with torch.no_grad():\n",
    "        lossvalue, logging_output =\\\n",
    "            lossfns[lossid].compute_gold_loss(\n",
    "                # (source_batch, target_batch), # bug: if it's target_batch, we're inputting 2 copies of source_batch\n",
    "                (source_batch, predicted_batch), \n",
    "                additional_batch=additional_batch, \n",
    "                context_batch=context_batch,\n",
    "                use_context=use_context,\n",
    "                label_id=label_ids[lossid],\n",
    "                keyword=keywords[lossid],\n",
    "                kweight=new_kweight\n",
    "            )\n",
    "    gold_losses.append(lossvalue.squeeze().item())\n",
    "    if (lossid >= 1) and (gold_losses[lossid] > config['min_epsilons'][lossid - 1]):\n",
    "        allsat = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af60d0a2-5d0f-412a-b54c-cc25e90fb30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13647,    13,  8989,   262,  3741,   286, 14260,   886,   510,   287,\n",
       "          7510,   326,   345,   550,   284,  3708,  3511,    13,  2011,   691,\n",
       "         38424,   318,   284,  3745,   340,  3589,    13,  1867,   561,   307,\n",
       "           262,  3772, 12838,   286,   616,  1204,   788,    30]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c46b4ea5-86b3-4e4d-a35d-bfddd28861a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let me just start by saying I hate horse dirt. Unfortunately the majority of horses end up in shit that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primary_tokenizer.batch_decode(torch.cat([source_batch, predicted_batch], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7002c45e-81df-4762-9351-eb4b29f2cce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<mask>. Unfortunately the majority of horses end up in<mask> that you had to drive yourself.<mask> only recourse is to feed it<mask>. What would be the happy tale of my life then?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_sequence_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c7acc99-e10f-425d-99f2-76f75dafb571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 1])\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/logging/__init__.py\", line 1085, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/logging/__init__.py\", line 929, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/logging/__init__.py\", line 668, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/logging/__init__.py\", line 371, in getMessage\n",
      "    msg = str(self.msg)\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/torch/_tensor.py\", line 431, in __repr__\n",
      "    return torch._tensor_str._str(self, tensor_contents=tensor_contents)\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/torch/_tensor_str.py\", line 664, in _str\n",
      "    return _str_intern(self, tensor_contents=tensor_contents)\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/torch/_tensor_str.py\", line 595, in _str_intern\n",
      "    tensor_str = _tensor_str(self, indent)\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/torch/_tensor_str.py\", line 347, in _tensor_str\n",
      "    formatter = _Formatter(get_summarized_data(self) if summarize else self)\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/torch/_tensor_str.py\", line 133, in __init__\n",
      "    value_str = f\"{value}\"\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/torch/_tensor.py\", line 933, in __format__\n",
      "    return self.item().__format__(format_spec)\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Call stack:\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 737, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/asyncio/events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 524, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 513, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 418, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 758, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 426, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/hyeryung/miniconda3/envs/loc-edit/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_412649/4005093706.py\", line 23, in <module>\n",
      "    logger.debug(source_batch)\n",
      "Unable to print the message and arguments - possible formatting error.\n",
      "Use the traceback above to help find the error.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(torch\u001b[38;5;241m.\u001b[39mtensor(hyp)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     23\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(source_batch)\n\u001b[0;32m---> 24\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(torch\u001b[38;5;241m.\u001b[39mcat([source_batch, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     25\u001b[0m     full_hypotheses\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mcat([source_batch, \n\u001b[1;32m     26\u001b[0m                                       torch\u001b[38;5;241m.\u001b[39mtensor(hyp)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     27\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(full_hypotheses)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "Hypothesis = namedtuple('Hypothesis', ['value', 'score'])\n",
    "\n",
    "        \n",
    "beam_size = 5\n",
    "hypotheses = [[]]\n",
    "max_decoding_time_step= predicted_batch.size(-1)\n",
    "\n",
    "for i in range(max_decoding_time_step):\n",
    "    logger.debug(i)\n",
    "    \n",
    "    if i not in indices[0]:\n",
    "        for hyp in hypotheses:\n",
    "            hyp.append(predicted_batch[:, i])\n",
    "    else:\n",
    "        new_hypotheses = []\n",
    "        for hyp in hypotheses:\n",
    "            for tok_cand_id in range(config['k_per_location']):\n",
    "                new_hypotheses.append(hyp + [predicted_token_ids.indices[indices[0].index(i), tok_cand_id].item()])\n",
    "\n",
    "        full_hypotheses = []\n",
    "        for hyp in new_hypotheses:\n",
    "            logger.debug(torch.tensor(hyp).unsqueeze(0).shape)\n",
    "            logger.debug(source_batch)\n",
    "            logger.debug(torch.cat([source_batch, torch.tensor(hyp).unsqueeze(0).cuda()], dim=-1))\n",
    "            full_hypotheses.append(torch.cat([source_batch, \n",
    "                                              torch.tensor(hyp).unsqueeze(0).cuda()], dim=-1))\n",
    "        logger.debug(full_hypotheses)\n",
    "        for hyp in full_hypotheses:\n",
    "            primary_tokenizer.batch_decode(hyp)\n",
    "        full_hypotheses = torch.stack(full_hypotheses)\n",
    "        logger.debug(full_hypotheses.size())\n",
    "        with torch.no_grad():\n",
    "            lm_score = name2model['gpt2-large'](full_hypotheses, labels=full_hypotheses)\n",
    "            lm_score_prompt = name2model['gpt2-large'](source_batch, labels=source_batch)\n",
    "        logger.debug(lm_score.logits.shape)\n",
    "        logger.debug(lm_score_prompt.logits.shape)\n",
    "\n",
    "        logger.debug(lm_score.logits[full_hypotheses.unsqueeze(-1)].size())\n",
    "        # lm_score = lm_score - lm_score_prompt\n",
    "        # c_score = name2model['models/models_mucola/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best'](full_hypotheses)\n",
    "        # c_score_prompt = name2model['models/models_mucola/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best'](source_batch)\n",
    "        # c_score = c_score - c_score_prompt\n",
    "        \n",
    "        # full_score = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c68af7a-77e4-4657-a06b-509c79343de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_ids.indices[indices[0].index(i), tok_cand_id].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde27c8-9d60-47c6-8f84-3df99db4f1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a541e1f8-13ac-4667-8c0b-afa7515f2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_hypotheses)\n",
    "raise\n",
    "    # hyp_num = len(hypotheses)\n",
    "    # exp_src_encodings = src_encodings.expand(hyp_num,\n",
    "    #                                             src_encodings.size(1),\n",
    "    #                                             src_encodings.size(2))\n",
    "    \n",
    "\n",
    "while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n",
    "    t += 1\n",
    "    hyp_num = len(hypotheses)\n",
    "\n",
    "    exp_src_encodings = src_encodings.expand(hyp_num,\n",
    "                                                src_encodings.size(1),\n",
    "                                                src_encodings.size(2))\n",
    "\n",
    "    exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n",
    "                                                                    src_encodings_att_linear.size(1),\n",
    "                                                                    src_encodings_att_linear.size(2))\n",
    "\n",
    "    y_tm1 = torch.tensor([self.vocab.tgt[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)\n",
    "    y_t_embed = self.model_embeddings.target(y_tm1)\n",
    "\n",
    "    x = torch.cat([y_t_embed, att_tm1], dim=-1)\n",
    "\n",
    "    (h_t, cell_t), att_t, _  = self.step(x, h_tm1,\n",
    "                                                exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n",
    "\n",
    "    # log probabilities over target words\n",
    "    log_p_t = F.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n",
    "\n",
    "    live_hyp_num = beam_size - len(completed_hypotheses)\n",
    "    contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n",
    "    top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n",
    "\n",
    "    prev_hyp_ids = torch.div(top_cand_hyp_pos, len(self.vocab.tgt), rounding_mode='floor')\n",
    "    hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n",
    "\n",
    "    new_hypotheses = []\n",
    "    live_hyp_ids = []\n",
    "    new_hyp_scores = []\n",
    "\n",
    "    for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n",
    "        prev_hyp_id = prev_hyp_id.item()\n",
    "        hyp_word_id = hyp_word_id.item()\n",
    "        cand_new_hyp_score = cand_new_hyp_score.item()\n",
    "\n",
    "        hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n",
    "        new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n",
    "        if hyp_word == '</s>':\n",
    "            completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n",
    "                                                    score=cand_new_hyp_score))\n",
    "        else:\n",
    "            new_hypotheses.append(new_hyp_sent)\n",
    "            live_hyp_ids.append(prev_hyp_id)\n",
    "            new_hyp_scores.append(cand_new_hyp_score)\n",
    "\n",
    "    if len(completed_hypotheses) == beam_size:\n",
    "        break\n",
    "\n",
    "    live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n",
    "    h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n",
    "    att_tm1 = att_t[live_hyp_ids]\n",
    "\n",
    "    hypotheses = new_hypotheses\n",
    "    hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n",
    "\n",
    "if len(completed_hypotheses) == 0:\n",
    "    completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n",
    "                                            score=hyp_scores[0].item()))\n",
    "\n",
    "completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "if config['selection_criteria'] == \"weighted_sum\":\n",
    "    best_ix = np.argmin(np.array(candidate_total_losses))\n",
    "elif config['selection_criteria'] == \"allsat_primary\":\n",
    "    allsat_ix = np.where(np.array(candidate_allsats)==True)[0]\n",
    "    if len(allsat_ix) > 0:\n",
    "        best_ix = np.argmin(np.array(candidate_primary_losses)[allsat_ix]) # select min primary loss among allsats\n",
    "        best_ix = allsat_ix[best_ix]\n",
    "    else: # if no candidate satisfying constraints, default to weighted_sum\n",
    "        best_ix = np.argmin(np.array(candidate_total_losses))\n",
    "    \n",
    "if _iter == 0: \n",
    "    ## save the best prediction in a format compatible with mucola outputs\n",
    "    best_prediction = test_sequences[best_ix].squeeze().tolist()\n",
    "    predicted_batch = test_sequences[best_ix]\n",
    "    # logger.debug(best_prediction)\n",
    "    best_text = primary_tokenizer.decode(best_prediction)\n",
    "    # logger.debug(best_text)\n",
    "    best_allsat = candidate_allsats[best_ix]\n",
    "    best_losses = candidate_losses_for_loggings[best_ix]\n",
    "    best_weighted_loss = candidate_total_losses[best_ix]\n",
    "    \n",
    "    \n",
    "    logger.debug(f\"best_prediction: {best_prediction}\")\n",
    "    logger.debug(f\"best_text: {best_text}\")\n",
    "    logger.debug(f\"best_allsat: {best_allsat}\")\n",
    "    logger.debug(f\"best_losses: {best_losses}\")\n",
    "    logger.debug(f\"best_weighted_loss: {best_weighted_loss}\")\n",
    "else:\n",
    "    update = False\n",
    "    if config['selection_criteria'] == \"weighted_sum\":\n",
    "        if best_weighted_loss > candidate_total_losses[best_ix]:\n",
    "            update = True\n",
    "    elif config['selection_criteria'] == \"allsat_primary\":\n",
    "        if best_allsat == False and candidate_allsats[best_ix] == True:\n",
    "            update = True\n",
    "        elif best_allsat == False and candidate_allsats[best_ix] == False:\n",
    "            if best_weighted_loss > candidate_total_losses[best_ix]:\n",
    "                update = True\n",
    "        elif best_allsat == True and candidate_allsats[best_ix] == True:\n",
    "            if best_losses[0] > candidate_losses_for_loggings[best_ix][0]:\n",
    "                update = True\n",
    "    if update:\n",
    "        ## save the best prediction in a format compatible with mucola outputs\n",
    "        best_prediction = test_sequences[best_ix].squeeze().tolist()\n",
    "        predicted_batch = test_sequences[best_ix]\n",
    "        # logger.debug(best_prediction)\n",
    "        best_text = primary_tokenizer.decode(best_prediction)\n",
    "        # logger.debug(best_text)\n",
    "        best_allsat = candidate_allsats[best_ix]\n",
    "        best_losses = candidate_losses_for_loggings[best_ix]\n",
    "        best_weighted_loss = candidate_total_losses[best_ix]\n",
    "        \n",
    "        logger.debug(f\"iter {_iter}. Update best prediction\")\n",
    "        logger.debug(f\"best_prediction: {best_prediction}\")\n",
    "        logger.debug(f\"best_text: {best_text}\")\n",
    "        logger.debug(f\"best_allsat: {best_allsat}\")\n",
    "        logger.debug(f\"best_losses: {best_losses}\")\n",
    "        logger.debug(f\"best_weighted_loss: {best_weighted_loss}\")\n",
    "    \n",
    "    # if best_allsat:\n",
    "    #     es_patience_count += 1\n",
    "    #     if config['early_stopping_patience'] == -1:\n",
    "    #         continue\n",
    "    #     if es_patience_count > config['early_stopping_patience']:\n",
    "    #         break\n",
    "    \n",
    "if sample_idx == 0:\n",
    "    output = {\n",
    "        \"prompt\":{\n",
    "            \"text\":source_text,\n",
    "            \"tokens\":source_indices.tolist()\n",
    "            }, \n",
    "        \"generations\":[{\n",
    "            \"text\": best_text,\n",
    "            \"tokens\": best_prediction,\n",
    "            \"indices\": indices, \n",
    "            \"allsat\": best_allsat,\n",
    "            \"losses\": best_losses,\n",
    "            \"weighted_loss\": best_weighted_loss\n",
    "            }]\n",
    "    }\n",
    "else:\n",
    "    output['generations'].append(\n",
    "        {\n",
    "            \"text\": best_text,\n",
    "            \"tokens\": best_prediction,\n",
    "            \"indices\": indices, \n",
    "            \"allsat\": best_allsat,\n",
    "            \"losses\": best_losses,\n",
    "            \"weighted_loss\": best_weighted_loss\n",
    "        }\n",
    "    )\n",
    "\n",
    "if sample_idx + 1 == config['num_samples']:\n",
    "    json.dump(output, outf)\n",
    "    outf.write(\"\\n\")\n",
    "    outf.flush()\n",
    "            \n",
    "outf.close()\n",
    "\n",
    "from loc_edit.evalute_only_wandb import main\n",
    "\n",
    "run_path=run.path\n",
    "run.finish()\n",
    "\n",
    "main(run_path = run_path, \n",
    "     outfile = outfile, \n",
    "     metrics = 'toxicity,toxicity-energy,toxicity-mucola,ppl-big,dist-n')\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from evaluation.prompted_sampling.evaluate import conditional_perplexity, toxicity_score, toxicity_score_energy, toxicity_score_mucola, distinctness\n",
    "# from pathlib import Path\n",
    "\n",
    "# output_dir = Path(os.path.dirname(outfile))\n",
    "# output_file = f\"{outdir}/results_epsilon{config['min_epsilons'][0]}-test.txt\"\n",
    "# generations_df = pd.read_json(outfile, lines=True) \n",
    "\n",
    "# metrics=\"toxicity,toxicity-energy,toxicity-mucola,ppl-big,dist-n\"\n",
    "# metricset = set(metrics.strip().lower().split(\",\"))\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# if \"ppl-big\" in metricset: #GPT2-XL\n",
    "#     logger.debug(\"big\")\n",
    "    \n",
    "#     eval_model = AutoModelForCausalLM.from_pretrained('gpt2-xl').to(device)\n",
    "#     eval_tokenizer = AutoTokenizer.from_pretrained('gpt2-xl')\n",
    "#     torch.cuda.empty_cache()\n",
    "#     with torch.no_grad():\n",
    "#         ppl, total_ppl = conditional_perplexity(generations_df, eval_model, eval_tokenizer, device=device, write_file=output_dir / (output_file+\".ppl-big\"))\n",
    "#     wandb.log({'ppl': ppl, 'total_ppl': total_ppl})\n",
    "\n",
    "# if 'toxicity' in metricset:\n",
    "#     logger.debug(\"toxicity\")\n",
    "#     (avg_max_toxicity, toxic_probability_p, avg_toxicity, toxic_probability_s) = toxicity_score(generations_df, perspective_file=output_dir / (output_file+\".toxicity\"))\n",
    "#     wandb.log({'avg_max_toxicity': avg_max_toxicity, 'toxic_probability_p': toxic_probability_p,\n",
    "#                'avg_toxicity': avg_toxicity, 'toxic_probability_s': toxic_probability_s})\n",
    "        \n",
    "# if 'toxicity-energy' in metricset:\n",
    "#     logger.debug(\"toxicity-energy\")\n",
    "#     (avg_max_toxicity, toxic_probability_p, avg_toxicity, toxic_probability_s) = toxicity_score_energy(generations_df, toxicity_file=output_dir / (output_file+\".toxicity_energy\"))\n",
    "#     wandb.log({'avg_max_toxicity_energy': avg_max_toxicity, 'toxic_probability_p_energy': toxic_probability_p,\n",
    "#                'avg_toxicity_energy': avg_toxicity, 'toxic_probability_s_energy': toxic_probability_s})\n",
    "#     # with open(output_dir / output_file, 'a') as fo:\n",
    "#     #     fo.write(f'[energy model] avg_max_toxicity = {avg_max_toxicity}, toxicity prob prompt = {toxic_probability_p}, avg_toxicity = {avg_toxicity}, toxicity prob={toxic_probability_s}\\n')\n",
    "#     #     logger.debug(f'[energy model] avg_max_toxicity = {avg_max_toxicity}, toxicity prob prompt = {toxic_probability_p}, avg_toxicity = {avg_toxicity}, toxicity prob={toxic_probability_s}\\n')\n",
    "        \n",
    "# if 'toxicity-mucola' in metricset:\n",
    "#     logger.debug(\"toxicity-mucola\")\n",
    "#     (avg_max_toxicity, toxic_probability_p, avg_toxicity, toxic_probability_s) = toxicity_score_mucola(generations_df, toxicity_file=output_dir / (output_file+\".toxicity_mucola\"))\n",
    "#     wandb.log({'avg_max_toxicity_mucola': avg_max_toxicity, 'toxic_probability_p_mucola': toxic_probability_p,\n",
    "#                'avg_toxicity_mucola': avg_toxicity, 'toxic_probability_s_mucola': toxic_probability_s})\n",
    "#     # with open(output_dir / output_file, 'a') as fo:\n",
    "#     #     fo.write(f'[mucola model] avg_max_toxicity = {avg_max_toxicity}, toxicity prob prompt = {toxic_probability_p}, avg_toxicity = {avg_toxicity}, toxicity prob={toxic_probability_s}\\n')\n",
    "#     #     logger.debug(f'[mucola model] avg_max_toxicity = {avg_max_toxicity}, toxicity prob prompt = {toxic_probability_p}, avg_toxicity = {avg_toxicity}, toxicity prob={toxic_probability_s}\\n')\n",
    "\n",
    "# if \"dist-n\" in metricset:\n",
    "#     logger.debug(\"dist-n\")\n",
    "#     dist1, dist2, dist3 = distinctness(generations_df)\n",
    "#     wandb.log({'dist-1': dist1, 'dist-2': dist2, 'dist-3': dist3})\n",
    "#     # # write output results\n",
    "#     # with open(output_dir / output_file, 'a') as fo:\n",
    "#     #     for i, dist_n in enumerate([dist1, dist2, dist3]):\n",
    "#     #         fo.write(f'dist-{i+1} = {dist_n}\\n')\n",
    "#     #         print(f'dist-{i+1} = {dist_n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6829a000-5207-4e54-b1eb-08feea1b49b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

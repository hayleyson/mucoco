{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import argparse\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "os.chdir('/data/hyeryung/mucoco')\n",
    "from itertools import repeat\n",
    "import torch.multiprocessing as mp\n",
    "from nb_test_module import Processor, f\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from new_module.utils.robertacustom import RobertaCustomForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(ckpt_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='abc'\n",
    "prediction=['dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(prediction, add_special_tokens=False, padding=True, truncation=True, return_tensors=\"pt\").to(device) # prediction이 list여도 처리가능함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch, output_attentions=True, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = output.attentions\n",
    "num_layer = 10\n",
    "max_num_tokens = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실제 locate하는 로직이 나오는 부분\n",
    "punctuations = string.punctuation + '\\n '\n",
    "punctuations = list(punctuations)\n",
    "punctuations.remove('-')\n",
    "stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in tokenizer.special_tokens_map.values()]\n",
    "stopwords_ids = tokenizer.batch_encode_plus(stopwords, return_tensors=\"pt\",add_special_tokens=False)['input_ids'].squeeze().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## attentions : tuple of length num hidden layers\n",
    "## attentions[i] : attention value of ith hidden layer of shape (batch, num_heads, query, value)\n",
    "lengths = batch.attention_mask.sum(dim=-1)\n",
    "# 보고자 하는 attention layer 만 가져옴\n",
    "attentions = attentions[\n",
    "    num_layer # originally 10\n",
    "]\n",
    "cls_attns = attentions.max(1)[0][:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## avg_value만 구하면 된다고 한다면, 이렇게도 가능하다.\n",
    "## NOTE: softmax를 안했음\n",
    "cls_attns[torch.isin(batch.input_ids, stopwords_ids)]=0.0\n",
    "no_punc_len=(~torch.isin(batch.input_ids, stopwords_ids)).sum(dim=-1) # tensor([2, 4], device='cuda:0')\n",
    "avg_values=cls_attns.sum(dim=-1)/no_punc_len # tensor([0.5120, 0.3744], device='cuda:0', grad_fn=<DivBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## stopwords가 아닌 부분에서만 index를 찾아야 한다.\n",
    "cls_attns[torch.isin(batch.input_ids, stopwords_ids)]=-float(\"inf\") ## stopwords 인부분을 -inf로 세팅하면, avg랑 비교할 때랑 topk를 찾을때 빠질것으로 예상\n",
    "top_masks = (cls_attns >= avg_values.unsqueeze(1))# unsqueeze to allow implicit broadcasting : (N) -> (N, 1) -> (N, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_located_tokens = torch.minimum((lengths//3), torch.LongTensor([max_num_tokens]).to(device))\n",
    "max_num_located_tokens = torch.minimum(max_num_located_tokens, top_masks.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_masks_final = [x[:max_num_located_tokens[i]] for i,x in enumerate(cls_attns.argsort(dim=-1,descending=True).tolist())] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit=\"word\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unit == \"token\":\n",
    "    locate_ixes=top_masks_final\n",
    "\n",
    "elif unit == \"word\":\n",
    "    proc=Processor(tokenizer)\n",
    "\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "    with mp.Pool(3) as pool:\n",
    "        # https://stackoverflow.com/questions/5442910/how-to-use-multiprocessing-pool-map-with-multiple-arguments\n",
    "        locate_ixes = pool.starmap(f, zip(prediction,batch.input_ids.tolist(), lengths.tolist(), top_masks_final,repeat(proc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 µs ± 22.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "# [x[:max_num_located_tokens[i]] for i,x in enumerate(cls_attns.argsort(dim=-1,descending=True).tolist())] \n",
    "## 이 implementation이 더 빠르다\n",
    "# 375 µs ± 22.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
    "\n",
    "# from multiprocessing import Pool\n",
    "# def f(x, length):\n",
    "#     return x[:length]\n",
    "# os.environ['TOKENIZERS_PARALLELISM']='true'\n",
    "# %%timeit\n",
    "\n",
    "# with Pool(3) as pool:\n",
    "#     L = pool.starmap(f, zip(cls_attns.argsort(dim=-1,descending=True).tolist(), max_num_located_tokens.tolist()))\n",
    "# 346 ms ± 26.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint/'\n",
    "\n",
    "prompt='abc'\n",
    "prediction=['dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe']\n",
    "\n",
    "unit=\"word\"\n",
    "num_layer = 10\n",
    "max_num_tokens = 6\n",
    "device='cuda'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckpt_path)\n",
    "model = model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n",
    "\n",
    "punctuations = string.punctuation + '\\n '\n",
    "punctuations = list(punctuations)\n",
    "punctuations.remove('-')\n",
    "stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in tokenizer.special_tokens_map.values()]\n",
    "stopwords_ids = tokenizer.batch_encode_plus(stopwords, return_tensors=\"pt\",add_special_tokens=False)['input_ids'].squeeze().to(device)\n",
    "\n",
    "###\n",
    "batch = tokenizer(prediction, add_special_tokens=False, padding=True, truncation=True, return_tensors=\"pt\").to(device) # prediction이 list여도 처리가능함\n",
    "output = model(**batch, output_attentions=True, output_hidden_states=True)\n",
    "attentions = output.attentions\n",
    "\n",
    "## 실제 locate하는 로직이 나오는 부분\n",
    "## attentions : tuple of length num hidden layers\n",
    "## attentions[i] : attention value of ith hidden layer of shape (batch, num_heads, query, value)\n",
    "lengths = batch.attention_mask.sum(dim=-1)\n",
    "attentions = attentions[num_layer]\n",
    "cls_attns = attentions.max(1)[0][:, 0] # cls_attns's dimension: (N, L)\n",
    "\n",
    "## avg_value만 구하면 된다고 한다면, 이렇게도 가능하다.\n",
    "## NOTE: softmax를 안했음\n",
    "cls_attns[torch.isin(batch.input_ids, stopwords_ids)]=0.0\n",
    "no_punc_len=(~torch.isin(batch.input_ids, stopwords_ids)).sum(dim=-1) # tensor([2, 4], device='cuda:0')\n",
    "avg_values=cls_attns.sum(dim=-1)/no_punc_len # tensor([0.5120, 0.3744], device='cuda:0', grad_fn=<DivBackward0>)\n",
    "\n",
    "## stopwords가 아닌 부분에서만 index를 찾아야 한다.\n",
    "cls_attns[torch.isin(batch.input_ids, stopwords_ids)]=-float(\"inf\") ## stopwords 인부분을 -inf로 세팅하면, avg랑 비교할 때랑 topk를 찾을때 빠질것으로 예상\n",
    "top_masks = (cls_attns >= avg_values.unsqueeze(1))# unsqueeze to allow implicit broadcasting : (N) -> (N, 1) -> (N, L)\n",
    "max_num_located_tokens = torch.minimum((lengths//3), torch.LongTensor([max_num_tokens]).to(device))\n",
    "max_num_located_tokens = torch.minimum(max_num_located_tokens, top_masks.sum(dim=-1))\n",
    "top_masks_final = [x[:max_num_located_tokens[i]] for i,x in enumerate(cls_attns.argsort(dim=-1,descending=True).tolist())] \n",
    "\n",
    "if unit == \"token\":\n",
    "    locate_ixes=top_masks_final\n",
    "\n",
    "elif unit == \"word\":\n",
    "    proc=Processor(tokenizer)\n",
    "\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "    with mp.Pool(3) as pool:\n",
    "        # https://stackoverflow.com/questions/5442910/how-to-use-multiprocessing-pool-map-with-multiple-arguments\n",
    "        locate_ixes = pool.starmap(f, zip(prediction,batch.input_ids.tolist(), lengths.tolist(), top_masks_final,repeat(proc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = 0\n",
    "\n",
    "## output['hidden_states']: tuple of length num_hidden_layers\n",
    "## output['hidden_states'][0]: (batch_size, seq_len, hidden_size)\n",
    "layer = output['hidden_states'][0]\n",
    "layer.retain_grad()\n",
    "\n",
    "## output['logits'] : (batch_size, num_labels)\n",
    "softmax=torch.nn.Softmax(dim=-1)\n",
    "probs = softmax(output['logits'])[:, label_id]\n",
    "\n",
    "# print(f\"probs.shape:{probs.shape}\")\n",
    "\n",
    "probs.sum().backward(retain_graph=True) ## NOTE. https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments/47026836#47026836\n",
    "\n",
    "## layer.grad : (batch_size, seq_len, hidden_size)\n",
    "# print(f\"layer.grad.shape:{layer.grad.shape}\")\n",
    "norm = torch.norm(layer.grad, dim=-1)\n",
    "\n",
    "## norm : (batch_size, seq_len)\n",
    "# print(f\"norm.shape:{norm.shape}\")\n",
    "norm = torch.where(norm > 0, norm, torch.full_like(norm, 1e-10))\n",
    "# print(f\"norm:{norm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint/'\n",
    "\n",
    "prompt='abc'\n",
    "prediction=['dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe']\n",
    "\n",
    "unit=\"word\"\n",
    "num_layer = 10\n",
    "max_num_tokens = 6\n",
    "device='cuda'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckpt_path)\n",
    "model = model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n",
    "\n",
    "punctuations = string.punctuation + '\\n '\n",
    "punctuations = list(punctuations)\n",
    "punctuations.remove('-')\n",
    "stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in tokenizer.special_tokens_map.values()]\n",
    "stopwords_ids = tokenizer.batch_encode_plus(stopwords, return_tensors=\"pt\",add_special_tokens=False)['input_ids'].squeeze().to(device)\n",
    "\n",
    "###\n",
    "batch = tokenizer(prediction, add_special_tokens=False, padding=True, truncation=True, return_tensors=\"pt\").to(device) # prediction이 list여도 처리가능함\n",
    "output = model(**batch, output_attentions=True, output_hidden_states=True)\n",
    "attentions = output.attentions\n",
    "\n",
    "## 실제 locate하는 로직이 나오는 부분\n",
    "## attentions : tuple of length num hidden layers\n",
    "## attentions[i] : attention value of ith hidden layer of shape (batch, num_heads, query, value)\n",
    "lengths = batch.attention_mask.sum(dim=-1)\n",
    "attentions = attentions[num_layer]\n",
    "cls_attns = attentions.max(1)[0][:, 0] # cls_attns's dimension: (N, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_attns=norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocateMachine:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def locate_main(self, prediction: List[str], method, max_num_tokens = 6, unit=\"word\", device=\"cuda\", **kwargs):\n",
    "        \n",
    "        punctuations = string.punctuation + '\\n '\n",
    "        punctuations = list(punctuations)\n",
    "        punctuations.remove('-')\n",
    "        stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in self.tokenizer.special_tokens_map.values()]\n",
    "        stopwords_ids = self.tokenizer.batch_encode_plus(stopwords, return_tensors=\"pt\",add_special_tokens=False)['input_ids'].squeeze().to(device)\n",
    "        batch = self.tokenizer(prediction, add_special_tokens=False, padding=True, truncation=True, return_tensors=\"pt\").to(device) # prediction이 list여도 처리가능함\n",
    "        \n",
    "        if method == \"attention\":\n",
    "            output = self.model(**batch, output_attentions=True)\n",
    "            attentions = output.attentions\n",
    "            ## attentions : tuple of length num hidden layers\n",
    "            ## attentions[i] : attention value of ith hidden layer of shape (batch, num_heads, query, value)\n",
    "            lengths = batch.attention_mask.sum(dim=-1)\n",
    "            attentions = attentions[kwargs['num_layer']]\n",
    "            token_wise_scores = attentions.max(1)[0][:, 0] # cls_attns's dimension: (N, L)\n",
    "        elif method == \"grad_norm\":\n",
    "            output = self.model(**batch, output_hidden_states=True)\n",
    "            ## output['hidden_states']: tuple of length num_hidden_layers\n",
    "            ## output['hidden_states'][0]: (batch_size, seq_len, hidden_size)\n",
    "            layer = output['hidden_states'][0]\n",
    "            layer.retain_grad()\n",
    "\n",
    "            softmax=torch.nn.Softmax(dim=-1)\n",
    "            probs = softmax(output['logits'])[:, kwargs['label_id']]\n",
    "            probs.sum().backward(retain_graph=True) ## NOTE. https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments/47026836#47026836\n",
    "            ## layer.grad : (batch_size, seq_len, hidden_size)\n",
    "            norm = torch.norm(layer.grad, dim=-1)\n",
    "            ## norm : (batch_size, seq_len)\n",
    "            token_wise_scores = torch.where(norm > 0, norm, torch.full_like(norm, 1e-10))\n",
    "        \n",
    "        ## avg_value만 구하면 된다고 한다면, 이렇게도 가능하다.\n",
    "        ## NOTE: softmax를 안했음\n",
    "        token_wise_scores[torch.isin(batch.input_ids, stopwords_ids)]=0.0\n",
    "        no_punc_len=(~torch.isin(batch.input_ids, stopwords_ids)).sum(dim=-1) # tensor([2, 4], device='cuda:0')\n",
    "        avg_values=token_wise_scores.sum(dim=-1)/no_punc_len # tensor([0.5120, 0.3744], device='cuda:0', grad_fn=<DivBackward0>)\n",
    "\n",
    "        ## stopwords가 아닌 부분에서만 index를 찾아야 한다.\n",
    "        token_wise_scores[torch.isin(batch.input_ids, stopwords_ids)]=-float(\"inf\") ## stopwords 인부분을 -inf로 세팅하면, avg랑 비교할 때랑 topk를 찾을때 빠질것으로 예상\n",
    "        top_masks = (token_wise_scores >= avg_values.unsqueeze(1))# unsqueeze to allow implicit broadcasting : (N) -> (N, 1) -> (N, L)\n",
    "        max_num_located_tokens = torch.minimum((lengths//3), torch.LongTensor([max_num_tokens]).to(device))\n",
    "        max_num_located_tokens = torch.minimum(max_num_located_tokens, top_masks.sum(dim=-1))\n",
    "        top_masks_final = [x[:max_num_located_tokens[i]] for i,x in enumerate(token_wise_scores.argsort(dim=-1,descending=True).tolist())] \n",
    "\n",
    "        if unit == \"token\":\n",
    "            locate_ixes=top_masks_final\n",
    "\n",
    "        elif unit == \"word\":\n",
    "            proc=Processor(self.tokenizer)\n",
    "\n",
    "            try:\n",
    "                mp.set_start_method('spawn', force=True)\n",
    "            except RuntimeError:\n",
    "                pass\n",
    "\n",
    "            with mp.Pool(3) as pool:\n",
    "                # https://stackoverflow.com/questions/5442910/how-to-use-multiprocessing-pool-map-with-multiple-arguments\n",
    "                locate_ixes = pool.starmap(f, zip(prediction,batch.input_ids.tolist(), lengths.tolist(), top_masks_final,repeat(proc)))\n",
    "        \n",
    "        return locate_ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsxe<s> [11622, 42336, 0, 1] 3 [0] <nb_test_module.Processor object at 0x7f1724ef3d00>\n",
      "sdvbfe [28045, 705, 428, 7068] 4 [3] <nb_test_module.Processor object at 0x7f3662327d00>\n",
      "dsxe<s> [11622, 42336, 0, 1] 3 [0] <nb_test_module.Processor object at 0x7ff7ed375d00>\n",
      "sdvbfe [28045, 705, 428, 7068] 4 [3] <nb_test_module.Processor object at 0x7f170b936190>\n",
      "dsxe<s> [11622, 42336, 0, 1] 3 [0] <nb_test_module.Processor object at 0x7f170b9e1eb0>\n",
      "sdvbfe [28045, 705, 428, 7068] 4 [3] <nb_test_module.Processor object at 0x7f364cd7e190>\n",
      "dsxe<s> [11622, 42336, 0, 1] 3 [0] <nb_test_module.Processor object at 0x7ff7d3dc8190>\n",
      "sdvbfe [28045, 705, 428, 7068] 4 [3] <nb_test_module.Processor object at 0x7f170b936190>\n",
      "dsxe<s> [11622, 42336, 0, 1] 3 [0] <nb_test_module.Processor object at 0x7f364ce29eb0>\n",
      "sdvbfe [28045, 705, 428, 7068] 4 [3] <nb_test_module.Processor object at 0x7ff7d8067eb0>\n"
     ]
    }
   ],
   "source": [
    "## avg_value만 구하면 된다고 한다면, 이렇게도 가능하다.\n",
    "## NOTE: softmax를 안했음\n",
    "cls_attns[torch.isin(batch.input_ids, stopwords_ids)]=0.0\n",
    "no_punc_len=(~torch.isin(batch.input_ids, stopwords_ids)).sum(dim=-1) # tensor([2, 4], device='cuda:0')\n",
    "avg_values=cls_attns.sum(dim=-1)/no_punc_len # tensor([0.5120, 0.3744], device='cuda:0', grad_fn=<DivBackward0>)\n",
    "\n",
    "## stopwords가 아닌 부분에서만 index를 찾아야 한다.\n",
    "cls_attns[torch.isin(batch.input_ids, stopwords_ids)]=-float(\"inf\") ## stopwords 인부분을 -inf로 세팅하면, avg랑 비교할 때랑 topk를 찾을때 빠질것으로 예상\n",
    "top_masks = (cls_attns >= avg_values.unsqueeze(1))# unsqueeze to allow implicit broadcasting : (N) -> (N, 1) -> (N, L)\n",
    "max_num_located_tokens = torch.minimum((lengths//3), torch.LongTensor([max_num_tokens]).to(device))\n",
    "max_num_located_tokens = torch.minimum(max_num_located_tokens, top_masks.sum(dim=-1))\n",
    "top_masks_final = [x[:max_num_located_tokens[i]] for i,x in enumerate(cls_attns.argsort(dim=-1,descending=True).tolist())] \n",
    "\n",
    "if unit == \"token\":\n",
    "    locate_ixes=top_masks_final\n",
    "\n",
    "elif unit == \"word\":\n",
    "    proc=Processor(tokenizer)\n",
    "\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "    with mp.Pool(3) as pool:\n",
    "        # https://stackoverflow.com/questions/5442910/how-to-use-multiprocessing-pool-map-with-multiple-arguments\n",
    "        locate_ixes = pool.starmap(f, zip(prediction,batch.input_ids.tolist(), lengths.tolist(), top_masks_final,repeat(proc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lengths = [i.tolist().count(1) for i in batch[\"attention_mask\"]]\n",
    "# print(f\"lengths: {lengths}\")\n",
    "\n",
    "locate_ixes = []\n",
    "locate_scores = []\n",
    "for i in range(batch[\"input_ids\"].shape[0]):\n",
    "    \n",
    "    ## norm_ : (seq_len,)\n",
    "    current_norm = norm[i, :]\n",
    "    # print(f\"norm_ shape: {current_norm.shape}\")\n",
    "    \n",
    "    ## current_sent : (lengths[i], )\n",
    "    current_sent = batch[\"input_ids\"][i][: lengths[i]]\n",
    "    no_punc_indices = torch.where(~torch.isin(current_sent, torch.tensor(stopwords_ids).to(device)))[0]\n",
    "    \n",
    "    # print(f\"current_sent: {current_sent}\")\n",
    "    # print(f\"len(current_sent), lengths[i]: {len(current_sent), lengths[i]}\")\n",
    "    # print(f\"no_punc_indices: {no_punc_indices}\")\n",
    "    # print(f\"current_sent[no_punc_indices]: {current_sent[no_punc_indices]}\")\n",
    "    # print(f\"tokenizer.decode(current_sent[no_punc_indices]): {tokenizer.decode(current_sent[no_punc_indices])}\")\n",
    "    \n",
    "    ## normalize current_norm\n",
    "    current_norm = current_norm[: lengths[i]].softmax(-1) \n",
    "    # print(f\"current_norm after normalizing: {current_norm}\")\n",
    "    \n",
    "    current_locate_scores = torch.zeros_like(current_norm)\n",
    "    current_locate_scores[no_punc_indices] = current_norm[no_punc_indices].clone()\n",
    "    locate_scores.append(current_locate_scores.cpu().detach().tolist())\n",
    "    \n",
    "    current_norm = current_norm[no_punc_indices]\n",
    "    # print(f\"current_norm after selecting non stop words indices: {current_norm}\")\n",
    "    \n",
    "    ## calculate mean value within the sequence\n",
    "    avg_value = current_norm.view(-1).mean().item()\n",
    "    # print(\"avg_value\", avg_value)\n",
    "    \n",
    "    ## find indices of tokens whose norm value is greater than the mean value\n",
    "    top_masks = ((current_norm >= avg_value).nonzero().view(-1)) \n",
    "    top_masks = top_masks.cpu().tolist()\n",
    "    torch.cuda.empty_cache()\n",
    "    # print(\"indices of non stopwords tokens whose grad norm value is greater than the mean value\", top_masks)\n",
    "    \n",
    "    ## in case the number of above average gradient norm tokens is greater than the max_num_tokens or 1/3 of the lengths[i]\n",
    "    if len(top_masks) > min((lengths[i]) // 3, max_num_tokens):\n",
    "        # print(\"len(located_tokens) exceeds max_num_tokens or 1/3 of the lengths[i]. Taking top k.\")\n",
    "        top_masks = (\n",
    "            current_norm.topk(max(min((lengths[i]) // 3, max_num_tokens), 1))[1]\n",
    "        )\n",
    "        top_masks = top_masks.cpu().tolist()\n",
    "        # print(\"indices of non stopwords tokens located after taking top k\", top_masks)\n",
    "    \n",
    "    top_masks = no_punc_indices[top_masks].cpu().detach().tolist()\n",
    "    # print(\"indices of tokens located\", top_masks)\n",
    "    \n",
    "    if unit == \"token\":\n",
    "        locate_ixes.append(list(set(top_masks)))\n",
    "    \n",
    "    elif unit == \"word\":\n",
    "\n",
    "        ## group token indices that belong to the same word\n",
    "        words = tokenizer.decode(current_sent).strip().split()\n",
    "        word2tok_mapper=Processor(tokenizer)\n",
    "        # print(f\"input to word2tok: {pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})}\")\n",
    "        grouped_tokens = list(word2tok_mapper.get_word2tok(pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})).values())            # j, k = 0, 0\n",
    "        # grouped_tokens = []\n",
    "        # grouped_tokens_for_word = []\n",
    "        # while j < len(current_sent):\n",
    "        #     if (tokenizer.decode(current_sent[j]).strip() not in stopwords):\n",
    "        #         while k < len(words):\n",
    "        #             if tokenizer.decode(current_sent[j]).strip() in words[k]:\n",
    "        #                 grouped_tokens_for_word.append(j)\n",
    "        #                 break\n",
    "        #             else:\n",
    "        #                 grouped_tokens.append(grouped_tokens_for_word)\n",
    "        #                 grouped_tokens_for_word = []\n",
    "        #                 k += 1\n",
    "        #     j += 1\n",
    "        # grouped_tokens.append(grouped_tokens_for_word)\n",
    "        \n",
    "        ## expand located token indices to include adjacent token indices that belong to the same word as already located tokens\n",
    "        top_masks.sort()\n",
    "        top_masks_final = set()\n",
    "        for index in top_masks:\n",
    "            if index not in top_masks_final:\n",
    "                word = [grouped_ixes for grouped_ixes in grouped_tokens if index in grouped_ixes]\n",
    "                # print(\"word\", word)\n",
    "                if len(word) > 0:\n",
    "                    word = set(word[0])\n",
    "                else:\n",
    "                    print(f\"warning. {index} not in the word groups. decoded value: {tokenizer.decode(index)}\")\n",
    "                    word = set([index])\n",
    "                top_masks_final |= word\n",
    "        locate_ixes.append(sorted(list(top_masks_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint/'\n",
    "\n",
    "prompt='abc'\n",
    "prediction=['dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe']\n",
    "\n",
    "unit=\"word\"\n",
    "num_layer = 10\n",
    "max_num_tokens = 6\n",
    "device='cuda'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckpt_path)\n",
    "model = model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n",
    "\n",
    "punctuations = string.punctuation + '\\n '\n",
    "punctuations = list(punctuations)\n",
    "punctuations.remove('-')\n",
    "stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in tokenizer.special_tokens_map.values()]\n",
    "stopwords_ids = tokenizer.batch_encode_plus(stopwords, return_tensors=\"pt\",add_special_tokens=False)['input_ids'].squeeze().to(device)\n",
    "\n",
    "###\n",
    "batch = tokenizer(prediction, add_special_tokens=False, padding=True, truncation=True, return_tensors=\"pt\").to(device) # prediction이 list여도 처리가능함\n",
    "output = model(**batch, output_attentions=True, output_hidden_states=True)\n",
    "attentions = output.attentions\n",
    "\n",
    "## 실제 locate하는 로직이 나오는 부분\n",
    "## attentions : tuple of length num hidden layers\n",
    "## attentions[i] : attention value of ith hidden layer of shape (batch, num_heads, query, value)\n",
    "lengths = batch.attention_mask.sum(dim=-1)\n",
    "attentions = attentions[num_layer]\n",
    "cls_attns = attentions.max(1)[0][:, 0] # cls_attns's dimension: (N, L)\n",
    "\n",
    "## avg_value만 구하면 된다고 한다면, 이렇게도 가능하다.\n",
    "## NOTE: softmax를 안했음\n",
    "cls_attns[torch.isin(batch.input_ids, stopwords_ids)]=0.0\n",
    "no_punc_len=(~torch.isin(batch.input_ids, stopwords_ids)).sum(dim=-1) # tensor([2, 4], device='cuda:0')\n",
    "avg_values=cls_attns.sum(dim=-1)/no_punc_len # tensor([0.5120, 0.3744], device='cuda:0', grad_fn=<DivBackward0>)\n",
    "\n",
    "## stopwords가 아닌 부분에서만 index를 찾아야 한다.\n",
    "cls_attns[torch.isin(batch.input_ids, stopwords_ids)]=-float(\"inf\") ## stopwords 인부분을 -inf로 세팅하면, avg랑 비교할 때랑 topk를 찾을때 빠질것으로 예상\n",
    "top_masks = (cls_attns >= avg_values.unsqueeze(1))# unsqueeze to allow implicit broadcasting : (N) -> (N, 1) -> (N, L)\n",
    "max_num_located_tokens = torch.minimum((lengths//3), torch.LongTensor([max_num_tokens]).to(device))\n",
    "max_num_located_tokens = torch.minimum(max_num_located_tokens, top_masks.sum(dim=-1))\n",
    "top_masks_final = [x[:max_num_located_tokens[i]] for i,x in enumerate(cls_attns.argsort(dim=-1,descending=True).tolist())] \n",
    "\n",
    "if unit == \"token\":\n",
    "    locate_ixes=top_masks_final\n",
    "\n",
    "elif unit == \"word\":\n",
    "    proc=Processor(tokenizer)\n",
    "\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "    with mp.Pool(3) as pool:\n",
    "        # https://stackoverflow.com/questions/5442910/how-to-use-multiprocessing-pool-map-with-multiple-arguments\n",
    "        locate_ixes = pool.starmap(f, zip(prediction,batch.input_ids.tolist(), lengths.tolist(), top_masks_final,repeat(proc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_grad_norm(prediction, output, tokenizer, batch, max_num_tokens = 6, unit=\"word\", device=\"cuda\", label_id = 1):\n",
    "\n",
    "    punctuations = string.punctuation + '\\n '\n",
    "    punctuations = list(punctuations)\n",
    "    punctuations.remove('-')\n",
    "    stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in tokenizer.special_tokens_map.values()]\n",
    "    stopwords_ids = [tokenizer.encode(word,add_special_tokens=False)[-1] for word in stopwords]\n",
    "\n",
    "    ## output['hidden_states']: tuple of length num_hidden_layers\n",
    "    ## output['hidden_states'][0]: (batch_size, seq_len, hidden_size)\n",
    "    layer = output['hidden_states'][0]\n",
    "    layer.retain_grad()\n",
    "    \n",
    "    ## output['logits'] : (batch_size, num_labels)\n",
    "    softmax=torch.nn.Softmax(dim=-1)\n",
    "    probs = softmax(output['logits'])[:, label_id]\n",
    "    # print(f\"probs.shape:{probs.shape}\")\n",
    "    \n",
    "    probs.sum().backward(retain_graph=True) ## NOTE. https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments/47026836#47026836\n",
    "\n",
    "    ## layer.grad : (batch_size, seq_len, hidden_size)\n",
    "    # print(f\"layer.grad.shape:{layer.grad.shape}\")\n",
    "    norm = torch.norm(layer.grad, dim=-1)\n",
    "    ## norm : (batch_size, seq_len)\n",
    "    # print(f\"norm.shape:{norm.shape}\")\n",
    "    norm = torch.where(norm > 0, norm, torch.full_like(norm, 1e-10))\n",
    "    # print(f\"norm:{norm}\")\n",
    "    \n",
    "    lengths = [i.tolist().count(1) for i in batch[\"attention_mask\"]]\n",
    "    # print(f\"lengths: {lengths}\")\n",
    "    \n",
    "    locate_ixes = []\n",
    "    locate_scores = []\n",
    "    for i in range(batch[\"input_ids\"].shape[0]):\n",
    "        \n",
    "        ## norm_ : (seq_len,)\n",
    "        current_norm = norm[i, :]\n",
    "        # print(f\"norm_ shape: {current_norm.shape}\")\n",
    "        \n",
    "        ## current_sent : (lengths[i], )\n",
    "        current_sent = batch[\"input_ids\"][i][: lengths[i]]\n",
    "        no_punc_indices = torch.where(~torch.isin(current_sent, torch.tensor(stopwords_ids).to(device)))[0]\n",
    "        \n",
    "        # print(f\"current_sent: {current_sent}\")\n",
    "        # print(f\"len(current_sent), lengths[i]: {len(current_sent), lengths[i]}\")\n",
    "        # print(f\"no_punc_indices: {no_punc_indices}\")\n",
    "        # print(f\"current_sent[no_punc_indices]: {current_sent[no_punc_indices]}\")\n",
    "        # print(f\"tokenizer.decode(current_sent[no_punc_indices]): {tokenizer.decode(current_sent[no_punc_indices])}\")\n",
    "        \n",
    "        ## normalize current_norm\n",
    "        current_norm = current_norm[: lengths[i]].softmax(-1) \n",
    "        # print(f\"current_norm after normalizing: {current_norm}\")\n",
    "        \n",
    "        current_locate_scores = torch.zeros_like(current_norm)\n",
    "        current_locate_scores[no_punc_indices] = current_norm[no_punc_indices].clone()\n",
    "        locate_scores.append(current_locate_scores.cpu().detach().tolist())\n",
    "        \n",
    "        current_norm = current_norm[no_punc_indices]\n",
    "        # print(f\"current_norm after selecting non stop words indices: {current_norm}\")\n",
    "        \n",
    "        ## calculate mean value within the sequence\n",
    "        avg_value = current_norm.view(-1).mean().item()\n",
    "        # print(\"avg_value\", avg_value)\n",
    "        \n",
    "        ## find indices of tokens whose norm value is greater than the mean value\n",
    "        top_masks = ((current_norm >= avg_value).nonzero().view(-1)) \n",
    "        top_masks = top_masks.cpu().tolist()\n",
    "        torch.cuda.empty_cache()\n",
    "        # print(\"indices of non stopwords tokens whose grad norm value is greater than the mean value\", top_masks)\n",
    "        \n",
    "        ## in case the number of above average gradient norm tokens is greater than the max_num_tokens or 1/3 of the lengths[i]\n",
    "        if len(top_masks) > min((lengths[i]) // 3, max_num_tokens):\n",
    "            # print(\"len(located_tokens) exceeds max_num_tokens or 1/3 of the lengths[i]. Taking top k.\")\n",
    "            top_masks = (\n",
    "                current_norm.topk(max(min((lengths[i]) // 3, max_num_tokens), 1))[1]\n",
    "            )\n",
    "            top_masks = top_masks.cpu().tolist()\n",
    "            # print(\"indices of non stopwords tokens located after taking top k\", top_masks)\n",
    "        \n",
    "        top_masks = no_punc_indices[top_masks].cpu().detach().tolist()\n",
    "        # print(\"indices of tokens located\", top_masks)\n",
    "        \n",
    "        if unit == \"token\":\n",
    "            locate_ixes.append(list(set(top_masks)))\n",
    "        \n",
    "        elif unit == \"word\":\n",
    "\n",
    "            ## group token indices that belong to the same word\n",
    "            words = tokenizer.decode(current_sent).strip().split()\n",
    "            word2tok_mapper=Processor(tokenizer)\n",
    "            # print(f\"input to word2tok: {pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})}\")\n",
    "            grouped_tokens = list(word2tok_mapper.get_word2tok(pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})).values())            # j, k = 0, 0\n",
    "            # grouped_tokens = []\n",
    "            # grouped_tokens_for_word = []\n",
    "            # while j < len(current_sent):\n",
    "            #     if (tokenizer.decode(current_sent[j]).strip() not in stopwords):\n",
    "            #         while k < len(words):\n",
    "            #             if tokenizer.decode(current_sent[j]).strip() in words[k]:\n",
    "            #                 grouped_tokens_for_word.append(j)\n",
    "            #                 break\n",
    "            #             else:\n",
    "            #                 grouped_tokens.append(grouped_tokens_for_word)\n",
    "            #                 grouped_tokens_for_word = []\n",
    "            #                 k += 1\n",
    "            #     j += 1\n",
    "            # grouped_tokens.append(grouped_tokens_for_word)\n",
    "            \n",
    "            ## expand located token indices to include adjacent token indices that belong to the same word as already located tokens\n",
    "            top_masks.sort()\n",
    "            top_masks_final = set()\n",
    "            for index in top_masks:\n",
    "                if index not in top_masks_final:\n",
    "                    word = [grouped_ixes for grouped_ixes in grouped_tokens if index in grouped_ixes]\n",
    "                    # print(\"word\", word)\n",
    "                    if len(word) > 0:\n",
    "                        word = set(word[0])\n",
    "                    else:\n",
    "                        print(f\"warning. {index} not in the word groups. decoded value: {tokenizer.decode(index)}\")\n",
    "                        word = set([index])\n",
    "                    top_masks_final |= word\n",
    "            locate_ixes.append(sorted(list(top_masks_final)))\n",
    "\n",
    "            \n",
    "    return locate_ixes, locate_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "if method == \"attention\":\n",
    "    output = model(**batch, output_attentions=True) # batch여도 처리가능함\n",
    "    locate_ixes, locate_scores = locate_attn(output.attentions, tokenizer, batch, max_num_tokens, unit, device, kwargs['num_layer'])\n",
    "elif method == \"grad_norm\":\n",
    "    output = model(**batch, output_hidden_states=True) # batch여도 처리가능함\n",
    "    locate_ixes, locate_scores = locate_grad_norm(output, tokenizer, batch, max_num_tokens, unit, device, kwargs['label_id'])\n",
    "\n",
    "## assuming batch_size = 1\n",
    "masked_sequence = batch['input_ids'].clone().detach()\n",
    "masked_sequence[:, locate_ixes[0]] = tokenizer.mask_token_id ## 이부분에 대해서 처리방식 고민 필요\n",
    "masked_sequence_text = tokenizer.batch_decode(\n",
    "    masked_sequence.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_attn(prediction, attentions, tokenizer, batch, max_num_tokens = 6, unit=\"word\", device=\"cuda\", num_layer=10):\n",
    "\n",
    "    punctuations = string.punctuation + '\\n '\n",
    "    punctuations = list(punctuations)\n",
    "    punctuations.remove('-')\n",
    "\n",
    "    ## attentions : tuple of length num hidden layers\n",
    "    ## attentions[i] : attention value of ith hidden layer of shape (batch, num_heads, query, value)\n",
    "    lengths = [i.tolist().count(1) for i in batch[\"attention_mask\"]]\n",
    "    # 보고자 하는 attention layer 만 가져옴\n",
    "    attentions = attentions[\n",
    "        num_layer # originally 10\n",
    "    ]\n",
    "    # print(attentions.shape)\n",
    "    # print(attentions.max(1)[0].shape)\n",
    "    # print( batch[\"input_ids\"].shape)\n",
    "    # print( batch[\"attention_mask\"][0,:])\n",
    "    cls_attns = attentions.max(1)[0][:, 0]\n",
    "    \n",
    "    stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in tokenizer.special_tokens_map.values()]\n",
    "    stopwords_ids = [tokenizer.encode(word,add_special_tokens=False)[-1] for word in stopwords]\n",
    "    # print(\"stopwords_ids\", torch.tensor(stopwords_ids))\n",
    "\n",
    "    locate_ixes=[]\n",
    "    locate_scores = []\n",
    "    for i, attn in enumerate(cls_attns):\n",
    "        \n",
    "        # print(\"attn.shape\", attn.shape)\n",
    "        current_sent = batch[\"input_ids\"][i][: lengths[i]]\n",
    "        # print(\"current_sent\", current_sent)\n",
    "        if device == \"cuda\":\n",
    "            no_punc_indices = torch.where(~torch.isin(current_sent, torch.tensor(stopwords_ids).to(torch.device('cuda'))))[0]\n",
    "        else:\n",
    "            no_punc_indices = torch.where(~torch.isin(current_sent, torch.tensor(stopwords_ids)))[0]\n",
    "        # print(\"no_punc_indices\", no_punc_indices)\n",
    "        # print(f\"current_sent[no_punc_indices]: {current_sent[no_punc_indices]}\")\n",
    "        # print(f\"tokenizer.decode(current_sent[no_punc_indices]): {tokenizer.decode(current_sent[no_punc_indices])}\")\n",
    "        \n",
    "        # current tokenizer does not add <s> and </s> to the sentence.\n",
    "        current_attn = attn[: lengths[i]].softmax(-1) \n",
    "        \n",
    "        current_locate_scores = torch.zeros_like(current_attn)\n",
    "        current_locate_scores[no_punc_indices] = current_attn[no_punc_indices].clone()\n",
    "        locate_scores.append(current_locate_scores.cpu().detach().tolist())\n",
    "        \n",
    "        # print(\"current_attn\", current_attn)\n",
    "        current_attn = current_attn[no_punc_indices]\n",
    "        # print(\"current_attn\", current_attn)\n",
    "        \n",
    "        # 이 값의 평균을 구한다.\n",
    "        avg_value = current_attn.view(-1).mean().item()\n",
    "        # print(\"avg_value\", avg_value)\n",
    "        # 이 값 중에 평균보다 큰 값을 지니는 위치를 찾는다.\n",
    "        # fixed to reflect that sometimes the sequence length is 1.\n",
    "        top_masks = ((current_attn >= avg_value).nonzero().view(-1)) \n",
    "        torch.cuda.empty_cache()\n",
    "        top_masks = top_masks.cpu().tolist()\n",
    "        # print(\"top_masks\", top_masks)\n",
    "        \n",
    "        \n",
    "        # attention 값이 평균보다 큰 토큰의 수가 k개 또는 문장 전체 토큰 수의 1/3 보다 크면  \n",
    "        if len(top_masks) > min((lengths[i]) // 3, max_num_tokens):\n",
    "            # 그냥 attention 값 기준 k 개 또는 토큰 수/3 중 작은 수를 뽑는다.\n",
    "            top_masks = (\n",
    "                current_attn.topk(max(min((lengths[i]) // 3, max_num_tokens), 1))[1]\n",
    "            )\n",
    "            top_masks = top_masks.cpu().tolist()\n",
    "            # print(\"top k top_masks\", top_masks)\n",
    "        top_masks_final = no_punc_indices[top_masks]\n",
    "        # print(\"top_masks_final\", top_masks_final)\n",
    "        if unit == \"token\":\n",
    "            locate_ixes.append(list(set(top_masks_final.cpu().detach().tolist())))\n",
    "        \n",
    "        elif unit == \"word\":\n",
    "            # word의 일부만 locate 한 경우, word 전체를 locate 한다.\n",
    "            # 같은 word 안에 있는 token 끼리 묶음.\n",
    "            words = tokenizer.decode(current_sent).strip().split()\n",
    "            # print(\"words\", words)\n",
    "            word2tok_mapper=Processor(tokenizer)\n",
    "            # print(f\"input to word2tok: {pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})}\")\n",
    "            grouped_tokens = list(word2tok_mapper.get_word2tok(pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})).values())\n",
    "            # j, k = 0, 0\n",
    "            # grouped_tokens = []\n",
    "            # grouped_tokens_for_word = []\n",
    "            # while j < len(current_sent):\n",
    "            #     if (tokenizer.decode(current_sent[j]).strip() not in stopwords):\n",
    "            #         # print(\"tokenizer.decode(current_sent[j])\", tokenizer.decode(current_sent[j]))\n",
    "            #         while k < len(words):\n",
    "            #             if tokenizer.decode(current_sent[j]).strip() in words[k]:\n",
    "            #                 grouped_tokens_for_word.append(j)\n",
    "            #                 break\n",
    "            #             else:\n",
    "            #                 grouped_tokens.append(grouped_tokens_for_word)\n",
    "            #                 grouped_tokens_for_word = []\n",
    "            #                 k += 1\n",
    "            #     j += 1\n",
    "            # grouped_tokens.append(grouped_tokens_for_word)\n",
    "            # print(\"grouped_tokens\", grouped_tokens)\n",
    "            \n",
    "            top_masks_final.sort()\n",
    "            top_masks_final_final = []\n",
    "            for index in top_masks_final:\n",
    "                # print(\"index\", index)\n",
    "                if index not in top_masks_final_final:\n",
    "                    word = [grouped_ixes for grouped_ixes in grouped_tokens if index in grouped_ixes]\n",
    "                    # print(\"word\", word)\n",
    "                    if len(word) > 0:\n",
    "                        word = word[0]\n",
    "                    else:\n",
    "                        print(f\"!!! {index} not in the grouped_ixes {grouped_tokens}\")\n",
    "                        print(f\"!!! tokenizer.decode(index): {tokenizer.decode(index)}\")\n",
    "                    top_masks_final_final.extend(word)\n",
    "            locate_ixes.append(list(set(top_masks_final_final)))\n",
    "\n",
    "            \n",
    "    return locate_ixes, locate_scores\n",
    "\n",
    "def locate_grad_norm(prediction, output, tokenizer, batch, max_num_tokens = 6, unit=\"word\", device=\"cuda\", label_id = 1):\n",
    "\n",
    "    punctuations = string.punctuation + '\\n '\n",
    "    punctuations = list(punctuations)\n",
    "    punctuations.remove('-')\n",
    "    stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in tokenizer.special_tokens_map.values()]\n",
    "    stopwords_ids = [tokenizer.encode(word,add_special_tokens=False)[-1] for word in stopwords]\n",
    "\n",
    "    ## output['hidden_states']: tuple of length num_hidden_layers\n",
    "    ## output['hidden_states'][0]: (batch_size, seq_len, hidden_size)\n",
    "    layer = output['hidden_states'][0]\n",
    "    layer.retain_grad()\n",
    "    \n",
    "    ## output['logits'] : (batch_size, num_labels)\n",
    "    softmax=torch.nn.Softmax(dim=-1)\n",
    "    probs = softmax(output['logits'])[:, label_id]\n",
    "    # print(f\"probs.shape:{probs.shape}\")\n",
    "    \n",
    "    probs.sum().backward(retain_graph=True) ## NOTE. https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments/47026836#47026836\n",
    "\n",
    "    ## layer.grad : (batch_size, seq_len, hidden_size)\n",
    "    # print(f\"layer.grad.shape:{layer.grad.shape}\")\n",
    "    norm = torch.norm(layer.grad, dim=-1)\n",
    "    ## norm : (batch_size, seq_len)\n",
    "    # print(f\"norm.shape:{norm.shape}\")\n",
    "    norm = torch.where(norm > 0, norm, torch.full_like(norm, 1e-10))\n",
    "    # print(f\"norm:{norm}\")\n",
    "    \n",
    "    lengths = [i.tolist().count(1) for i in batch[\"attention_mask\"]]\n",
    "    # print(f\"lengths: {lengths}\")\n",
    "    \n",
    "    locate_ixes = []\n",
    "    locate_scores = []\n",
    "    for i in range(batch[\"input_ids\"].shape[0]):\n",
    "        \n",
    "        ## norm_ : (seq_len,)\n",
    "        current_norm = norm[i, :]\n",
    "        # print(f\"norm_ shape: {current_norm.shape}\")\n",
    "        \n",
    "        ## current_sent : (lengths[i], )\n",
    "        current_sent = batch[\"input_ids\"][i][: lengths[i]]\n",
    "        no_punc_indices = torch.where(~torch.isin(current_sent, torch.tensor(stopwords_ids).to(device)))[0]\n",
    "        \n",
    "        # print(f\"current_sent: {current_sent}\")\n",
    "        # print(f\"len(current_sent), lengths[i]: {len(current_sent), lengths[i]}\")\n",
    "        # print(f\"no_punc_indices: {no_punc_indices}\")\n",
    "        # print(f\"current_sent[no_punc_indices]: {current_sent[no_punc_indices]}\")\n",
    "        # print(f\"tokenizer.decode(current_sent[no_punc_indices]): {tokenizer.decode(current_sent[no_punc_indices])}\")\n",
    "        \n",
    "        ## normalize current_norm\n",
    "        current_norm = current_norm[: lengths[i]].softmax(-1) \n",
    "        # print(f\"current_norm after normalizing: {current_norm}\")\n",
    "        \n",
    "        current_locate_scores = torch.zeros_like(current_norm)\n",
    "        current_locate_scores[no_punc_indices] = current_norm[no_punc_indices].clone()\n",
    "        locate_scores.append(current_locate_scores.cpu().detach().tolist())\n",
    "        \n",
    "        current_norm = current_norm[no_punc_indices]\n",
    "        # print(f\"current_norm after selecting non stop words indices: {current_norm}\")\n",
    "        \n",
    "        ## calculate mean value within the sequence\n",
    "        avg_value = current_norm.view(-1).mean().item()\n",
    "        # print(\"avg_value\", avg_value)\n",
    "        \n",
    "        ## find indices of tokens whose norm value is greater than the mean value\n",
    "        top_masks = ((current_norm >= avg_value).nonzero().view(-1)) \n",
    "        top_masks = top_masks.cpu().tolist()\n",
    "        torch.cuda.empty_cache()\n",
    "        # print(\"indices of non stopwords tokens whose grad norm value is greater than the mean value\", top_masks)\n",
    "        \n",
    "        ## in case the number of above average gradient norm tokens is greater than the max_num_tokens or 1/3 of the lengths[i]\n",
    "        if len(top_masks) > min((lengths[i]) // 3, max_num_tokens):\n",
    "            # print(\"len(located_tokens) exceeds max_num_tokens or 1/3 of the lengths[i]. Taking top k.\")\n",
    "            top_masks = (\n",
    "                current_norm.topk(max(min((lengths[i]) // 3, max_num_tokens), 1))[1]\n",
    "            )\n",
    "            top_masks = top_masks.cpu().tolist()\n",
    "            # print(\"indices of non stopwords tokens located after taking top k\", top_masks)\n",
    "        \n",
    "        top_masks = no_punc_indices[top_masks].cpu().detach().tolist()\n",
    "        # print(\"indices of tokens located\", top_masks)\n",
    "        \n",
    "        if unit == \"token\":\n",
    "            locate_ixes.append(list(set(top_masks)))\n",
    "        \n",
    "        elif unit == \"word\":\n",
    "\n",
    "            ## group token indices that belong to the same word\n",
    "            words = tokenizer.decode(current_sent).strip().split()\n",
    "            word2tok_mapper=Processor(tokenizer)\n",
    "            # print(f\"input to word2tok: {pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})}\")\n",
    "            grouped_tokens = list(word2tok_mapper.get_word2tok(pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})).values())            # j, k = 0, 0\n",
    "            # grouped_tokens = []\n",
    "            # grouped_tokens_for_word = []\n",
    "            # while j < len(current_sent):\n",
    "            #     if (tokenizer.decode(current_sent[j]).strip() not in stopwords):\n",
    "            #         while k < len(words):\n",
    "            #             if tokenizer.decode(current_sent[j]).strip() in words[k]:\n",
    "            #                 grouped_tokens_for_word.append(j)\n",
    "            #                 break\n",
    "            #             else:\n",
    "            #                 grouped_tokens.append(grouped_tokens_for_word)\n",
    "            #                 grouped_tokens_for_word = []\n",
    "            #                 k += 1\n",
    "            #     j += 1\n",
    "            # grouped_tokens.append(grouped_tokens_for_word)\n",
    "            \n",
    "            ## expand located token indices to include adjacent token indices that belong to the same word as already located tokens\n",
    "            top_masks.sort()\n",
    "            top_masks_final = set()\n",
    "            for index in top_masks:\n",
    "                if index not in top_masks_final:\n",
    "                    word = [grouped_ixes for grouped_ixes in grouped_tokens if index in grouped_ixes]\n",
    "                    # print(\"word\", word)\n",
    "                    if len(word) > 0:\n",
    "                        word = set(word[0])\n",
    "                    else:\n",
    "                        print(f\"warning. {index} not in the word groups. decoded value: {tokenizer.decode(index)}\")\n",
    "                        word = set([index])\n",
    "                    top_masks_final |= word\n",
    "            locate_ixes.append(sorted(list(top_masks_final)))\n",
    "\n",
    "            \n",
    "    return locate_ixes, locate_scores\n",
    "\n",
    "def locate_main(prediction: List[str], method, model, tokenizer, max_num_tokens = 6, unit=\"word\", device=\"cuda\", **kwargs): #label_id = 1, num_layer=10):\n",
    "# def locate_main(prompt, prediction, method, model, tokenizer, max_num_tokens = 6, unit=\"word\", device=\"cuda\", **kwargs): #label_id = 1, num_layer=10):\n",
    "    \n",
    "#     ## prompt에 대한 처리를 어떻게 할지 고민이 필요\n",
    "#     if isinstance(prompt, list) and isinstance(prediction, list):\n",
    "#         prompt_prediction = [f\"{p}{g}\" for p, g in zip(prompt, prediction)]\n",
    "#     else:\n",
    "#         prompt_prediction = [f\"{prompt}{prediction}\"]\n",
    "#     batch = tokenizer(prompt_prediction, add_special_tokens=False, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    batch = tokenizer(prediction, add_special_tokens=False, padding=True, truncation=True, return_tensors=\"pt\").to(device) # prediction이 list여도 처리가능함\n",
    "    \n",
    "    if method == \"attention\":\n",
    "        output = model(**batch, output_attentions=True) # batch여도 처리가능함\n",
    "        locate_ixes, locate_scores = locate_attn(prediction, output.attentions, tokenizer, batch, max_num_tokens, unit, device, kwargs['num_layer'])\n",
    "    elif method == \"grad_norm\":\n",
    "        output = model(**batch, output_hidden_states=True) # batch여도 처리가능함\n",
    "        locate_ixes, locate_scores = locate_grad_norm(prediction, output, tokenizer, batch, max_num_tokens, unit, device, kwargs['label_id'])\n",
    "    \n",
    "    ## assuming batch_size = 1\n",
    "    masked_sequence = batch['input_ids'].clone().detach()\n",
    "    masked_sequence[:, locate_ixes[0]] = tokenizer.mask_token_id ## 이부분에 대해서 처리방식 고민 필요\n",
    "    masked_sequence_text = tokenizer.batch_decode(\n",
    "        masked_sequence.tolist()\n",
    "    )\n",
    "    return masked_sequence_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"\"\"Program to locate spans that contributed the most to the prediction of a model\\n\\\n",
    "Input format: jsonl or csv with a column named \"text\" containing the text to be analyzed\\n\\\n",
    "Output format: input dataframe with a new column named \"located_indices\" each of which is a list of indices of tokens. e.g. [2,3,4,8,10]\\n\\\n",
    "\"\"\")\n",
    "    parser.add_argument(\"--method\", type=str, choices=[\"attention\",\"grad_norm\"], help=\"method to use for locating tokens to edit\")\n",
    "    parser.add_argument(\"--input_path\", type=str, help=\"path to input file\")\n",
    "    parser.add_argument(\"--output_path\", type=str, help=\"path to output file\")\n",
    "    parser.add_argument(\"--model_name_or_path\", type=str, help=\"name of model to use or path to the checkpoint to use\")\n",
    "    parser.add_argument(\"--model_type\", type=str, choices=[\"AutoModelForSequenceClassification\", \"RobertaCustomForSequenceClassification\"], help=\"name of model to use\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"batch size to use\")\n",
    "    parser.add_argument(\"--text_column_name\", type=str, default=\"text\", help=\"name of the column containing text for analysis\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    if args.model_type == \"AutoModelForSequenceClassification\":\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(args.model_name_or_path)\n",
    "    elif args.model_type == \"RobertaCustomForSequenceClassification\":\n",
    "        model = RobertaCustomForSequenceClassification.from_pretrained(args.model_name_or_path)\n",
    "    model.to(device)\n",
    "        \n",
    "    if args.input_path.endswith(\".jsonl\"):\n",
    "        data = pd.read_json(args.input_path, lines=True)\n",
    "    elif args.input_path.endswith(\".csv\"):\n",
    "        data = pd.read_csv(args.input_path)\n",
    "        \n",
    "    if \"gpt2\" in args.input_path: ## unravel data\n",
    "        print(\"GPT2 in args.input_path\")\n",
    "        ## unravel the file \n",
    "        data['prompt']=data['prompt'].apply(lambda x: x['text'])\n",
    "        data = data.explode('generations')\n",
    "\n",
    "        data['text']=data['generations'].apply(lambda x: x['text'])\n",
    "        data['tokens']=data['generations'].apply(lambda x: x['tokens'])\n",
    "        data['locate_labels']=data['generations'].apply(lambda x: x.get('locate_labels', np.nan))\n",
    "        data = data.dropna(subset=['locate_labels'])\n",
    "        \n",
    "        del data['generations']\n",
    "        del data['locate_labels']\n",
    "        print(data.head())\n",
    "    \n",
    "    dataset = Dataset.from_pandas(data)\n",
    "    if \"gpt2\" in args.input_path:\n",
    "        print(\"GPT2 in args.input_path\")\n",
    "        def collate_fn(batch):\n",
    "            input_ids = pad_sequence([torch.LongTensor(example[\"tokens\"]) for example in batch], padding_value=tokenizer.pad_token_id, batch_first=True) \n",
    "            # print(f\"input_ids: {input_ids}\")\n",
    "            batch = {\"input_ids\": input_ids,\n",
    "                    \"attention_mask\": (input_ids != tokenizer.pad_token_id).long()}\n",
    "            return transformers.tokenization_utils_base.BatchEncoding(batch)\n",
    "    else:\n",
    "        def collate_fn(batch):\n",
    "            batch = tokenizer([example[args.text_column_name] for example in batch], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            return batch\n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "    \n",
    "    pred_indices = []\n",
    "    pred_scores = []\n",
    "    for batch in dataloader:\n",
    "        batch.to(device)\n",
    "        results, scores = locate_main(args.method, model, tokenizer, batch, max_num_tokens = 6, num_layer=10, unit=\"word\", use_cuda=True)\n",
    "        pred_indices.extend(results)\n",
    "        pred_scores.extend(scores)\n",
    "    \n",
    "    data[f'pred_indices_{args.method}'] = pred_indices\n",
    "    data[f'pred_scores_{args.method}'] = pred_scores\n",
    "    os.makedirs(os.path.dirname(args.output_path), exist_ok=True)\n",
    "    data.to_json(args.output_path, lines=True, orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc-edit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

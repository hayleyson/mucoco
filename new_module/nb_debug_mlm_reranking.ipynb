{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38b40a4-9262-4d88-afb9-3ee8d79fa3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.1.2 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/s3/hyeryung/mucoco')\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "import mucoco.utils as utils\n",
    "import new_module.losses as lossbuilder\n",
    "import wandb\n",
    "from new_module.decode_utils import beam_rerank_v0, beam_rerank_v1, beam_rerank_v2, score_hypotheses\n",
    "from new_module.evaluate_wandb import evaluate\n",
    "from new_module.locate.locate_utils import locate_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e80d7c-08b7-4ade-a95b-3c90ad98d3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import new_module.locate.locate_utils\n",
    "# importlib.reload(new_module.locate.locate_utils)\n",
    "# from new_module.locate.locate_utils import locate_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4a0813-e9f3-4596-96c0-ac71d0964386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import new_module.losses.gpt2\n",
    "# import new_module.losses as lossbuilder\n",
    "# importlib.reload(new_module.losses)\n",
    "# importlib.reload(new_module.losses.gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "060c3a04-8e47-4288-8d04-29c06a617d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(os.environ.get(\"LOGGING_LEVEL\", logging.DEBUG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3ba687b-3a13-4314-92d4-8cc0650ccf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={'model_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint'],\n",
    "        'tokenizer_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint'],\n",
    "        'model_types': [\"AutoModelForCausalLM\", \"AutoModelForSequenceClassification\"],\n",
    "        'cache_dir': \"hf_cache\",\n",
    "        'target_type': \"embeds\",\n",
    "        'method': \"mlm-beamsearch-v0\",\n",
    "       'losses': [\"gpt2\", \"classification_no_prefix_logprobloss\"],\n",
    "       'target_label_ids': [0,0] ,\n",
    "       'build_loss_dict': {\"coeff_steps\": 200, \"coeff_pattern\": \"constant\", \"loss_type\": \"xentropy\", \"length_normalize\": False, \"AR_temperature\": 1.0, \"AR_top_k\": 0, \"AR_top_p\": 0.96, \"max_output_length\": 20},\n",
    "       'min_epsilons': [0.75],\n",
    "       'source_data': 'new_module/data/toxicity-avoidance/testset_gpt2_2500.jsonl',\n",
    "       'locate_unit': 'word',\n",
    "       'locate_method': 'grad_norm',\n",
    "       'device': 'cuda',\n",
    "       'k_per_location': 3,\n",
    "       'closs_weight': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162fbf50-f9be-46c7-8d2f-1e7a0c912159",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dummyArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "build_loss_args = dummyArgs(**config[\"build_loss_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bcf7dff-3442-41e4-baf1-a0ea59929f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "name2tokenizer = {}\n",
    "name2model = {}\n",
    "name2config = {}\n",
    "loss2tokenizer = {}\n",
    "embed_luts = []\n",
    "primary_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcbb2800-f93f-44b7-a282-6b5d6576be37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/generation_config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "for i, model_path in enumerate(config[\"model_paths\"]):\n",
    "    if (\n",
    "        model_path not in name2model\n",
    "    ):  # making sure we are not loading the model twice in case some constraints use the same model.\n",
    "        try:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=True,\n",
    "            )\n",
    "        except:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=False,\n",
    "            )\n",
    "\n",
    "        name2config[model_path] = AutoConfig.from_pretrained(\n",
    "            model_path, cache_dir=config[\"cache_dir\"]\n",
    "        )\n",
    "\n",
    "        if \"Custom\" in config[\"model_types\"][i]:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(utils, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(transformers, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        name2model[model_path].eval()\n",
    "        name2model[model_path].cuda()\n",
    "\n",
    "    input_embeds = name2model[model_path].get_input_embeddings()\n",
    "    if isinstance(input_embeds, torch.nn.Sequential):\n",
    "        input_embeds = input_embeds[0]\n",
    "    embed_luts.append(input_embeds)\n",
    "\n",
    "    if config[\"target_type\"] == \"embeds\":\n",
    "        embed_luts[-1].requires_grad = False\n",
    "\n",
    "    if i == 0:\n",
    "        primary_model = name2model[model_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9535f8bd-6ae9-4ecf-b518-744ed8a56221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "mlm_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "mlm = None if config[\"method\"] == \"mlm-beamsearch-v2\" else AutoModelForMaskedLM.from_pretrained(\"roberta-base\")  \n",
    "\n",
    "lossfns = []\n",
    "for i, loss in enumerate(config[\"losses\"]):\n",
    "    lossfns.append(\n",
    "        lossbuilder.build_loss(\n",
    "            loss,\n",
    "            name2model[config[\"model_paths\"][i]],\n",
    "            name2tokenizer[config[\"model_paths\"][i]],\n",
    "            build_loss_args,\n",
    "        )\n",
    "    )\n",
    "    lossfns[i].tokenizer.add_special_tokens({\"mask_token\": mlm_tokenizer.mask_token})\n",
    "    loss2tokenizer[loss] = lossfns[i].tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55858af3-ce1f-4ea7-8b0f-f3f89916443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = config[\"target_label_ids\"]  # target label's ids for each loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "021c9739-ce48-4dfa-a6b3-623c680b66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"jsonl_primary_key\"]=\"prompt\"\n",
    "config[\"jsonl_secondary_key\"]=\"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70b47582-da01-4851-b8d1-caaa745882c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(config[\"source_data\"], \"r\") as f:\n",
    "#     generation_dataset = [line.rstrip('\\n') for line in f.readlines()]\n",
    "# source_dataset = [\"\" for l in generation_dataset]\n",
    "source_dataset = [\n",
    "    json.loads(l)[config[\"jsonl_primary_key\"]][config[\"jsonl_secondary_key\"]]\n",
    "    for l in open(config[\"source_data\"])\n",
    "]\n",
    "generation_dataset = [\n",
    "    json.loads(l)[\"generations\"] for l in open(config[\"source_data\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19de82ce-16c5-480a-8244-412d62a52d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id=0\n",
    "source_text = source_dataset[text_id]\n",
    "if source_text == \"\":\n",
    "    source_text = lossfns[0].tokenizer.bos_token\n",
    "AR_prediction_all = [x[\"text\"] for x in generation_dataset[text_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecd10d21-17e6-48b4-a336-a13134d0e5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_text: Let me just start by saying I hate horse\n",
      "AR_prediction_all: ['dirt. Unfortunately the majority of horses end up in shit that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?', 'wearing games and holy ****ing shit do I hate horse wearing games.', \"fetishes: it just makes me want to puke every time I see it on the internet, even though it's not worth a thing because I am a furry.\", 'head gelatin and see it as a sort of travelogues of American life, so it was good for me to be in Louisville as long', '-trading (talk).', 'riding or vegas\\' version of live riding, we have fine horse riding movies like \"Tancer\", \"Stoner Horse\", \"Ride', '-drawn cars.', \"racing. But we are a young country and if we can't beat those guys then that means we're not fit to do anything good in this world.\", 'racing as well as people who are into it.', \"play so you may just have to live with me when I say that this moment was glorious.'\\n\\nFinishing with a pious smile, he leapt upon the back\"]\n"
     ]
    }
   ],
   "source": [
    "print(f\"source_text: {source_text}\")\n",
    "print(f\"AR_prediction_all: {AR_prediction_all}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66611ff3-41ab-4819-be2d-584a53225796",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 0\n",
    "        \n",
    "# predicted_batch = predicted_batches[sample_idx].cuda()\n",
    "AR_prediction = AR_prediction_all[sample_idx]\n",
    "\n",
    "# --------------------------------------------------------------------------------------------- #\n",
    "## check whether initial text satisfies constraint\n",
    "allsat = True\n",
    "gold_losses = []\n",
    "for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "    with torch.no_grad():\n",
    "        lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "            source_text, AR_prediction,\n",
    "            label_id=label_ids[lossid],\n",
    "        )\n",
    "        \n",
    "    gold_losses.append(lossvalue.squeeze().item())\n",
    "    if (lossid >= 1) and (gold_losses[lossid] > -np.log(\n",
    "        config[\"min_epsilons\"][lossid - 1]\n",
    "    )):\n",
    "        allsat = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b612d77c-3a0b-41d0-90dc-4e0de751427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"locate_unit\"]='token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21c974ab-1d2c-44db-ba19-3fec8d9f55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_text  = locate_main(AR_prediction, \n",
    "            config[\"locate_method\"], \n",
    "            name2model[config[\"model_paths\"][1]], \n",
    "            name2tokenizer[config[\"tokenizer_paths\"][1]], \n",
    "            max_num_tokens = 6, \n",
    "            unit=config[\"locate_unit\"], \n",
    "            device=\"cuda\", \n",
    "            label_id=config[\"target_label_ids\"][1],\n",
    "            num_layer=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "546863ef-bcc5-4f7d-a762-f20bfc02865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = mlm_tokenizer(\n",
    "    source_text + ' ' + masked_text[0], return_tensors=\"pt\", add_special_tokens=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "788f604e-34b8-430b-8c48-d171ce32177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = mlm(**inputs).logits\n",
    "indices_in_mlm_tokens = (\n",
    "    inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    ")[0].nonzero(as_tuple=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08991e18-3f27-4454-a2ee-21503769cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get top k tokens for each index\n",
    "predicted_token_ids = torch.topk(\n",
    "    logits[0, indices_in_mlm_tokens],\n",
    "    k=config['k_per_location'],\n",
    "    dim=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c19dd9fe-1bc3-4554-be39-2f260a9cd452",
   "metadata": {},
   "outputs": [],
   "source": [
    "### \"mlm-reranking\"\n",
    "hypotheses = []\n",
    "num_located_tokens = len(indices_in_mlm_tokens)\n",
    "num_all_cases = config[\"k_per_location\"] ** num_located_tokens\n",
    "tok_cand_combo = [0 for i in range(num_located_tokens)]\n",
    "\n",
    "for case_id in range(num_all_cases):\n",
    "    for i in range(num_located_tokens):\n",
    "        tok_cand_combo[i] = (\n",
    "            case_id // (config[\"k_per_location\"] ** i)\n",
    "        ) % config[\"k_per_location\"]\n",
    "\n",
    "    tmp_seq = inputs[\"input_ids\"].clone()\n",
    "    for pos_id, tok_cand_id in enumerate(tok_cand_combo):\n",
    "        tmp_seq[\n",
    "            0, indices_in_mlm_tokens[pos_id]\n",
    "        ] = predicted_token_ids.indices[pos_id, tok_cand_id]\n",
    "\n",
    "    # need to do decode with RobertaTokenizer and encode with GPT2Tokenizer\n",
    "    # logger.debug(mlm_tokenizer.batch_decode(tmp_seq[:, indices_in_mlm_tokens], skip_special_tokens=True))\n",
    "    tmp_dec_seq = mlm_tokenizer.batch_decode(\n",
    "            tmp_seq, skip_special_tokens=True\n",
    "        )\n",
    "    hypotheses.append(tmp_dec_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3741426e-79ad-44a3-af10-900c4560bf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.57 s ± 10.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "hypotheses = constrained_beam_search(source_text,\n",
    "                               inputs.input_ids,\n",
    "                               indices_in_mlm_tokens,\n",
    "                               predicted_token_ids,\n",
    "                               mlm_tokenizer, \n",
    "                               lossfns,\n",
    "                               config, \n",
    "                               beam_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b73165a1-38c2-4eb5-b33e-f5825802314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fn():\n",
    "    beam_size= 5\n",
    "    hypotheses = [torch.LongTensor([]).to(config['device'])]\n",
    "    \n",
    "    masked_sequence = inputs[\"input_ids\"].clone()\n",
    "    L = masked_sequence.size(-1)\n",
    "    \n",
    "    for i in range(L):\n",
    "\n",
    "        if masked_sequence[0, i] != mlm_tokenizer.mask_token_id:\n",
    "            for j in range(len(hypotheses)):\n",
    "                hypotheses[j] = torch.cat([hypotheses[j], masked_sequence[0, i].unsqueeze(0).to(config['device'])], dim = -1)\n",
    "\n",
    "        else:\n",
    "            hypotheses_exp = []\n",
    "            losses = []\n",
    "            for hyp in hypotheses:\n",
    "                # logger.debug(f\"hyp: {hyp}\")\n",
    "                for j in range(config['k_per_location']):\n",
    "                    candidate = predicted_token_ids.indices[torch.where(indices_in_mlm_tokens == i)[0], j].to(config['device'])\n",
    "                    hypotheses_exp.append(torch.cat([hyp, candidate], dim=-1))\n",
    "    \n",
    "                    # logger.debug(f\"hypotheses_exp at {i}: {hypotheses_exp}\")\n",
    "                    with torch.no_grad():\n",
    "                        lossvalue = lossfns[0].compute_gold_loss(\n",
    "                            source_text, mlm_tokenizer.decode(hypotheses_exp[-1])\n",
    "                        )\n",
    "                    losses.append(lossvalue)\n",
    "    \n",
    "            hypotheses = sorted(zip(hypotheses_exp, losses), key=lambda x: x[1])[:beam_size]\n",
    "            hypotheses = [x[0] for x in hypotheses]\n",
    "            \n",
    "    return [mlm_tokenizer.decode(x) for x in hypotheses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "396a75e3-8b78-461a-9469-9fd956c792cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.61 s ± 61.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dummy_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2b5f2a9-6e0f-4e1b-ad84-db765b01682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_rerank_v0(source_text,\n",
    "                    masked_sequence,\n",
    "                    indices_in_mlm_tokens,\n",
    "                    predicted_token_ids,\n",
    "                    mlm_tokenizer, \n",
    "                    lossfns,\n",
    "                    config, \n",
    "                    beam_size = 5):\n",
    "    \n",
    "    hypotheses = [torch.LongTensor([]).to(config['device'])]\n",
    "    L = masked_sequence.size(-1)\n",
    "\n",
    "    for i in range(L):\n",
    "        if masked_sequence[0, i] != mlm_tokenizer.mask_token_id:\n",
    "            hypotheses = list(torch.cat([torch.stack(hypotheses,dim=0), \n",
    "                                        masked_sequence[:, i].unsqueeze(0).repeat((len(hypotheses),1)).to(config['device'])],dim=-1))\n",
    "        else:\n",
    "            num_hypotheses = len(hypotheses)\n",
    "            hypotheses = torch.stack(hypotheses,dim=0).unsqueeze(0)\n",
    "            hypotheses = hypotheses.repeat(config['k_per_location'], 1, 1)\n",
    "            candidates = predicted_token_ids.indices[torch.where(indices_in_mlm_tokens == i)[0], :].to(config['device']).T.unsqueeze(1)\n",
    "            candidates = candidates.repeat(1, num_hypotheses, 1)\n",
    "            hypotheses_exp = torch.cat([hypotheses, candidates], dim=-1)\n",
    "            hypotheses_exp = hypotheses_exp.view(-1, hypotheses_exp.shape[-1])\n",
    "            hypotheses_exp = list(hypotheses_exp)\n",
    "\n",
    "            losses = []\n",
    "            loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "            for hyp in hypotheses_exp:\n",
    "                curr_loss = 0.0\n",
    "                for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "                    with torch.no_grad():\n",
    "                        lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                            source_text, mlm_tokenizer.decode(hyp),\n",
    "                            label_id=config['target_label_ids'][lossid],\n",
    "                        )\n",
    "                    curr_loss += loss_weights[lossid] * lossvalue.item()\n",
    "                losses.append(curr_loss)\n",
    "\n",
    "            hypotheses = sorted(zip(hypotheses_exp, losses), key=lambda x: x[1])[:beam_size]\n",
    "            hypotheses = [x[0] for x in hypotheses]\n",
    "            \n",
    "    return [mlm_tokenizer.decode(x) for x in hypotheses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ac4f8b5-b88e-495f-b734-1b156f02b9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let me just start by saying I hate horse dong. But the majority of us grew up the horse that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'Let me just start by saying I hate horse dong. But the majority of people grow up the horse that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'Let me just start by saying I hate horse dong. But the majority of people grew up the horse that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'Let me just start by saying I hate horse dong. But the majority of us grow up the horse that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'Let me just start by saying I hate horse dong. But the majority of us grew up the fact that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_rerank_v0(source_text,\n",
    "                    inputs.input_ids,\n",
    "                    indices_in_mlm_tokens,\n",
    "                    predicted_token_ids,\n",
    "                    mlm_tokenizer, \n",
    "                    lossfns,\n",
    "                    config, \n",
    "                    beam_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8e8a3-13dd-4cb0-b524-e686a12dbfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hypotheses = len(hypotheses)\n",
    "hypotheses = torch.stack(hypotheses,dim=0).unsqueeze(0)\n",
    "hypotheses = hypotheses.repeat(config['k_per_location'], 1, 1)\n",
    "candidates = predicted_token_ids.indices[torch.where(indices_in_mlm_tokens == i)[0], :].to(config['device']).T.unsqueeze(1)\n",
    "candidates = candidates.repeat(1, num_hypotheses, 1)\n",
    "hypotheses_exp = torch.cat([hypotheses, candidates], dim=-1)\n",
    "hypotheses_exp = hypotheses_exp.view(-1, hypotheses_exp.shape[-1])\n",
    "hypotheses_exp = list(hypotheses_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66f5802c-4e68-423f-870d-14aee45b91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "hypotheses = [torch.LongTensor([]).to(config['device'])]\n",
    "L = masked_sequence.size(-1)\n",
    "\n",
    "for i in range(L):\n",
    "    if masked_sequence[0, i] != primary_tokenizer.mask_token_id:\n",
    "        # print('!')\n",
    "        # print(masked_sequence[:, i])\n",
    "        hypotheses = list(torch.cat([torch.stack(hypotheses,dim=0), \n",
    "                                    masked_sequence[:, i].unsqueeze(0).repeat((len(hypotheses),1)).to(config['device'])],dim=-1))\n",
    "        # print(hypotheses)\n",
    "    else:\n",
    "        prefix_added_hypotheses = torch.cat([source_batch.expand(len(hypotheses), -1), torch.stack(hypotheses,dim=0)], dim=-1)\n",
    "        with torch.no_grad():\n",
    "            model_output = primary_model(input_ids = prefix_added_hypotheses)\n",
    "\n",
    "        logits_t = model_output.logits[:, -1, :] # get logits for the last timestep\n",
    "        logp_t = F.log_softmax(logits_t, dim=-1) # (num_hypotheses, |V|)\n",
    "        # print(logp_t.shape)\n",
    "        top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(-logp_t, k=beam_size, largest=True, dim=-1)\n",
    "\n",
    "        candidates = top_cand_hyp_pos.T.unsqueeze(1).repeat(1, beam_size, 1)\n",
    "        hypotheses_ = torch.stack(hypotheses).unsqueeze(1).repeat(1, beam_size, 1).view(len(hypotheses)*beam_size,-1)\n",
    "        hypotheses_exp = list(torch.cat([hypotheses_, top_cand_hyp_pos.view(-1,1)], dim=-1))\n",
    "        # print(len(hypotheses_exp))\n",
    "\n",
    "        losses = []\n",
    "        for hyp in hypotheses_exp:\n",
    "            with torch.no_grad():\n",
    "                lossvalue = lossfns[0].compute_gold_loss(\n",
    "                    source_text, mlm_tokenizer.decode(hyp),\n",
    "                )\n",
    "            losses.append(lossvalue.item())\n",
    "        hypotheses = sorted(zip(hypotheses_exp, losses), key=lambda x: x[1])[:beam_size]\n",
    "        hypotheses = [x[0] for x in hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "daacccca-c78e-42ae-a4f1-be1e2b8d8930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   67,   185,    13,   212,   262,  3741,   286,   211,   208,   510,\n",
       "           208,   208,   326,   345,   550,   284,  3708,  3511,    13,  2011,\n",
       "           691, 38424,   318,   284,  3745,   340,  3589,    13,  1867,   561,\n",
       "           307,   262,  3772, 12838,   286,   616,  1204,   788,    30],\n",
       "        device='cuda:0'),\n",
       " tensor([   67,   185,    13,   212,   262,  3741,   286,   211,   208,   510,\n",
       "           208,   181,   326,   345,   550,   284,  3708,  3511,    13,  2011,\n",
       "           691, 38424,   318,   284,  3745,   340,  3589,    13,  1867,   561,\n",
       "           307,   262,  3772, 12838,   286,   616,  1204,   788,    30],\n",
       "        device='cuda:0'),\n",
       " tensor([   67,   185,    13,   212,   262,  3741,   286,   211,   208,   510,\n",
       "           208,   212,   326,   345,   550,   284,  3708,  3511,    13,  2011,\n",
       "           691, 38424,   318,   284,  3745,   340,  3589,    13,  1867,   561,\n",
       "           307,   262,  3772, 12838,   286,   616,  1204,   788,    30],\n",
       "        device='cuda:0'),\n",
       " tensor([   67,   185,    13,   212,   262,  3741,   286,   211,   208,   510,\n",
       "           216,   208,   326,   345,   550,   284,  3708,  3511,    13,  2011,\n",
       "           691, 38424,   318,   284,  3745,   340,  3589,    13,  1867,   561,\n",
       "           307,   262,  3772, 12838,   286,   616,  1204,   788,    30],\n",
       "        device='cuda:0'),\n",
       " tensor([   67,   185,    13,   212,   262,  3741,   286,   211,   208,   510,\n",
       "           203,   208,   326,   345,   550,   284,  3708,  3511,    13,  2011,\n",
       "           691, 38424,   318,   284,  3745,   340,  3589,    13,  1867,   561,\n",
       "           307,   262,  3772, 12838,   286,   616,  1204,   788,    30],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3640c1a-2035-4325-9009-59b61b23410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = [primary_tokenizer.decode(x) for x in list(hypotheses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b3cb0b1-15ac-40ef-847b-952eda34b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "closs_weight = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06f9cb59-be4d-4005-a61a-064fd38fd581",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_total_losses = []\n",
    "candidate_primary_losses = []\n",
    "candidate_losses_for_loggings = []\n",
    "candidate_allsats = []\n",
    "loss_weights = [1 - closs_weight, closs_weight]\n",
    "for hyp in hypotheses:\n",
    "    curr_loss = 0.0\n",
    "    logging_loss = []\n",
    "    allsat = True\n",
    "    for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "        with torch.no_grad():\n",
    "            lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                source_text, hyp,\n",
    "                label_id=config['target_label_ids'][lossid],\n",
    "            )\n",
    "        curr_loss += loss_weights[lossid] * lossvalue.item()\n",
    "        logging_loss.append(lossvalue.item())\n",
    "        if lossid==0:\n",
    "            candidate_primary_losses.append(lossvalue.item())\n",
    "        elif (lossid >= 1) and (\n",
    "            lossvalue.item()\n",
    "            > -np.log(config[\"min_epsilons\"][lossid - 1])\n",
    "        ):\n",
    "            allsat = False\n",
    "    candidate_total_losses.append(curr_loss)\n",
    "    candidate_losses_for_loggings.append(logging_loss)\n",
    "    candidate_allsats.append(allsat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "070ad447-be07-4c0f-980c-60917671fc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[309.912670763582,\n",
       " 280.2622925773263,\n",
       " 309.9250766009092,\n",
       " 309.9390413619578,\n",
       " 309.9804595440626]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_total_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a85228c9-ab83-4f6f-b042-949e93d3a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = torch.LongTensor([[]]).to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52e2dd6c-9b09-459a-8854-fbe10b50e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_batch = lossfns[0].tokenizer(source_text, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cebf7b3e-ccd9-44a7-bd20-f3df9dcd1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sequence = lossfns[0].tokenizer(masked_text, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62614bc3-3560-454d-943a-2cfce7dd0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_tokenizer = name2tokenizer['gpt2-large']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05aa67d0-196a-49d6-a040-657672e31fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4df267d4-267f-441e-af86-4822c5d4f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_batch = masked_sequence\n",
    "hypotheses = torch.LongTensor([[]]).to(config['device'])\n",
    "hyp_scores = torch.zeros(len(hypotheses), dtype = torch.float, device = config['device'])\n",
    "L = masked_sequence.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a104c9c9-deda-426b-aaed-ddd1c98116f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_added_hypotheses = torch.cat([source_batch.expand(hypotheses.size(0), -1), hypotheses], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18dfe978-9185-4c56-839d-a049d3dda424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5756,  502,  655,  923,  416, 2282,  314, 5465, 8223]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_added_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c2857c8-1e04-4aa5-ac84-fe27e5cb3187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primary_tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d59b7c54-4687-473e-af0b-882a91924c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = primary_model(input_ids = prefix_added_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f26acdb-3cc1-44b1-9d88-1dce7d87fbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 50257])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28c6a73b-dd2f-41e7-88f5-fcc0964ce14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fe63f89-81b8-418e-aa5d-e8dda40d9199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.69 s ± 9.44 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "predicted_batch = masked_sequence\n",
    "hypotheses = torch.LongTensor([[]]).to(config['device'])\n",
    "hyp_scores = torch.zeros(len(hypotheses), dtype = torch.float, device = config['device'])\n",
    "L = masked_sequence.size(-1)\n",
    "\n",
    "for t in range(L):\n",
    "    prefix_added_hypotheses = torch.cat([source_batch.expand(hypotheses.size(0), -1), hypotheses], dim=-1)\n",
    "    # print(prefix_added_hypotheses)\n",
    "    with torch.no_grad():\n",
    "        model_output = primary_model(input_ids = prefix_added_hypotheses)\n",
    "\n",
    "    logits_t = model_output.logits[:, -1, :] # get logits for the last timestep\n",
    "    logp_t = F.log_softmax(logits_t, dim=-1) # (num_hypotheses, |V|)\n",
    "    \n",
    "    if predicted_batch[:,t] != primary_tokenizer.mask_token_id:\n",
    "        \n",
    "        curr_nll = F.nll_loss(logp_t, predicted_batch[:, t].expand(logp_t.size(0)), reduction=\"none\") # returns (num_hypotheses)\n",
    "        hyp_scores = hyp_scores.expand_as(curr_nll) + curr_nll # (num_hypotheses)\n",
    "        hypotheses = torch.cat([hypotheses, predicted_batch[:, t].expand(hypotheses.size(0), -1)], dim=-1)\n",
    "        \n",
    "    else:\n",
    "        contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(logp_t) + (-logp_t)).view(-1) # (num_hypotheses x |V|)\n",
    "        top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=beam_size, largest=True)\n",
    "        \n",
    "        prev_hyp_ids = torch.div(top_cand_hyp_pos, len(primary_tokenizer), rounding_mode='floor') # prev_hyp_id for each of top_cand_hyp. (beam_size)\n",
    "        hyp_word_ids = top_cand_hyp_pos % len(primary_tokenizer) # hyp_word_id for each of top_cand_hyp. (beam_size)\n",
    "        \n",
    "        hypotheses = torch.cat([hypotheses[prev_hyp_ids], hyp_word_ids.unsqueeze(1)], dim=-1)\n",
    "        hyp_scores = top_cand_hyp_scores\n",
    "\n",
    "    # torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb3a43e7-867c-4f59-9a85-28ffe8338193",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = [primary_tokenizer.decode(x) for x in list(hypotheses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50b10949-2734-4e9d-87b4-7eba2d91389c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d�.� the majority of\\x17� up\\x12\\x14 that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'd�.� the majority of\\x17� up\\x12� that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'd�.� the majority of\\x17� up\\x12� that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'd�.� the majority of\\x17� up\\x12\\x18 that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'd�.� the majority of\\x17� up\\x12� that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8910f62f-3ef5-4469-9fbd-186f17a47e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5756,  502,  655,  923,  416, 2282,  314, 5465, 8223]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b53386-b9c1-452a-aaf5-38aca0f482f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hypotheses = len(hypotheses)\n",
    "hypotheses = torch.stack(hypotheses,dim=0).unsqueeze(0)\n",
    "hypotheses = hypotheses.repeat(config['k_per_location'], 1, 1)\n",
    "candidates = predicted_token_ids.indices[torch.where(indices_in_mlm_tokens == i)[0], :].to(config['device']).T.unsqueeze(1)\n",
    "candidates = candidates.repeat(1, num_hypotheses, 1)\n",
    "hypotheses_exp = torch.cat([hypotheses, candidates], dim=-1)\n",
    "hypotheses_exp = hypotheses_exp.view(-1, hypotheses_exp.shape[-1])\n",
    "hypotheses_exp = list(hypotheses_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a418e28e-f85e-4f0e-95c6-c37c0ab9b2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  125],\n",
       "        [  212],\n",
       "        [  193],\n",
       "        [  181],\n",
       "        [36173],\n",
       "        [  125],\n",
       "        [  212],\n",
       "        [  193],\n",
       "        [  181],\n",
       "        [36173],\n",
       "        [  125],\n",
       "        [  212],\n",
       "        [  193],\n",
       "        [  181],\n",
       "        [36173],\n",
       "        [  125],\n",
       "        [  212],\n",
       "        [  193],\n",
       "        [  181],\n",
       "        [36173],\n",
       "        [  125],\n",
       "        [  212],\n",
       "        [  193],\n",
       "        [  181],\n",
       "        [36173]], device='cuda:0')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b671b-b5a5-45df-a17e-9c9eb5b05be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "974a0fab-9616-46a6-9e8c-6c0067c40d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   67,   183,    13,   125],\n",
       "        [   67,   183,    13,   212],\n",
       "        [   67,   183,    13,   193],\n",
       "        [   67,   183,    13,   181],\n",
       "        [   67,   183,    13, 36173],\n",
       "        [   67,   125,    13,   125],\n",
       "        [   67,   125,    13,   212],\n",
       "        [   67,   125,    13,   193],\n",
       "        [   67,   125,    13,   181],\n",
       "        [   67,   125,    13, 36173],\n",
       "        [   67,   185,    13,   125],\n",
       "        [   67,   185,    13,   212],\n",
       "        [   67,   185,    13,   193],\n",
       "        [   67,   185,    13,   181],\n",
       "        [   67,   185,    13, 36173],\n",
       "        [   67,   184,    13,   125],\n",
       "        [   67,   184,    13,   212],\n",
       "        [   67,   184,    13,   193],\n",
       "        [   67,   184,    13,   181],\n",
       "        [   67,   184,    13, 36173],\n",
       "        [   67,   186,    13,   125],\n",
       "        [   67,   186,    13,   212],\n",
       "        [   67,   186,    13,   193],\n",
       "        [   67,   186,    13,   181],\n",
       "        [   67,   186,    13, 36173]], device='cuda:0')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9baf71df-2dbd-4aaa-a78b-ff771708b8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  125,   212,   193,   181, 36173],\n",
       "        [  125,   212,   193,   181, 36173],\n",
       "        [  125,   212,   193,   181, 36173],\n",
       "        [  125,   212,   193,   181, 36173],\n",
       "        [  125,   212,   193,   181, 36173]], device='cuda:0')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_cand_hyp_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "64102383-7808-4212-82b2-a29af979fd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   67,   183,    13,   125],\n",
       "        [   67,   183,    13,   212],\n",
       "        [   67,   183,    13,   193],\n",
       "        [   67,   183,    13,   181],\n",
       "        [   67,   183,    13, 36173],\n",
       "        [   67,   125,    13,   125],\n",
       "        [   67,   125,    13,   212],\n",
       "        [   67,   125,    13,   193],\n",
       "        [   67,   125,    13,   181],\n",
       "        [   67,   125,    13, 36173],\n",
       "        [   67,   185,    13,   125],\n",
       "        [   67,   185,    13,   212],\n",
       "        [   67,   185,    13,   193],\n",
       "        [   67,   185,    13,   181],\n",
       "        [   67,   185,    13, 36173],\n",
       "        [   67,   184,    13,   125],\n",
       "        [   67,   184,    13,   212],\n",
       "        [   67,   184,    13,   193],\n",
       "        [   67,   184,    13,   181],\n",
       "        [   67,   184,    13, 36173],\n",
       "        [   67,   186,    13,   125],\n",
       "        [   67,   186,    13,   212],\n",
       "        [   67,   186,    13,   193],\n",
       "        [   67,   186,    13,   181],\n",
       "        [   67,   186,    13, 36173]], device='cuda:0')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d839f55-edea-4cf9-90cd-8656d13992c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfab21-dd97-420a-8e22-53a1157bdd02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a5c27-2d35-465c-9d4e-187e8872e66f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7662ba78-7413-4855-80b7-e6f44f5cd987",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 5\n",
    "top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(-logp_t, k=beam_size, largest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5a0496b5-9b45-43b5-9461-e31d82d2f113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 33])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(hypotheses).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3415125f-1529-4fa3-82a4-4006fb04327c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_cand_hyp_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69828e1c-c3ef-4a24-9e58-be81e88473a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_rerank_v2(source_batch, ## in primary tokens\n",
    "                    masked_sequence, ## in primary tokens\n",
    "                    primary_model, \n",
    "                    primary_tokenizer,\n",
    "                    config, \n",
    "                    beam_size = 5):\n",
    "    \n",
    "    hypotheses = [torch.LongTensor([]).to(config['device'])]\n",
    "    L = masked_sequence.size(-1)\n",
    "\n",
    "    for i in range(L):\n",
    "        if masked_sequence[0, i] != primary_tokenizer.mask_token_id:\n",
    "            hypotheses = list(torch.cat([torch.stack(hypotheses,dim=0), \n",
    "                                        masked_sequence[:, i].unsqueeze(0).repeat((len(hypotheses),1)).to(config['device'])],dim=-1))\n",
    "        else:\n",
    "            prefix_added_hypotheses = torch.cat([source_batch.expand(hypotheses.size(0), -1), hypotheses], dim=-1)\n",
    "            with torch.no_grad():\n",
    "                model_output = primary_model(input_ids = prefix_added_hypotheses)\n",
    "    \n",
    "            logits_t = model_output.logits[:, -1, :] # get logits for the last timestep\n",
    "            logp_t = F.log_softmax(logits_t, dim=-1) # (num_hypotheses, |V|)\n",
    "            \n",
    "            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(-logp_t, k=beam_size, largest=True)\n",
    "\n",
    "            prev_hyp_ids = torch.div(top_cand_hyp_pos, len(primary_tokenizer), rounding_mode='floor') # prev_hyp_id for each of top_cand_hyp. (beam_size)\n",
    "            hyp_word_ids = top_cand_hyp_pos % len(primary_tokenizer) # hyp_word_id for each of top_cand_hyp. (beam_size)\n",
    "            \n",
    "            hypotheses = torch.cat([hypotheses[prev_hyp_ids], hyp_word_ids.unsqueeze(1)], dim=-1)\n",
    "            hyp_scores = top_cand_hyp_scores\n",
    "\n",
    "            \n",
    "            \n",
    "            num_hypotheses = len(hypotheses)\n",
    "            hypotheses = torch.stack(hypotheses,dim=0).unsqueeze(0)\n",
    "            hypotheses = hypotheses.repeat(config['k_per_location'], 1, 1)\n",
    "            candidates = predicted_token_ids.indices[torch.where(indices_in_mlm_tokens == i)[0], :].to(config['device']).T.unsqueeze(1)\n",
    "            candidates = candidates.repeat(1, num_hypotheses, 1)\n",
    "            hypotheses_exp = torch.cat([hypotheses, candidates], dim=-1)\n",
    "            hypotheses_exp = hypotheses_exp.view(-1, hypotheses_exp.shape[-1])\n",
    "            hypotheses_exp = list(hypotheses_exp)\n",
    "\n",
    "            losses = []\n",
    "            loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "            for hyp in hypotheses_exp:\n",
    "                curr_loss = 0.0\n",
    "                for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "                    with torch.no_grad():\n",
    "                        lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                            source_text, mlm_tokenizer.decode(hyp),\n",
    "                            label_id=config['target_label_ids'][lossid],\n",
    "                        )\n",
    "                    curr_loss += loss_weights[lossid] * lossvalue.item()\n",
    "                losses.append(curr_loss)\n",
    "\n",
    "            hypotheses = sorted(zip(hypotheses_exp, losses), key=lambda x: x[1])[:beam_size]\n",
    "            hypotheses = [x[0] for x in hypotheses]\n",
    "            \n",
    "    return [mlm_tokenizer.decode(x) for x in hypotheses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cdbeb9f-2cfb-4630-a9ae-df2736ede7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = torch.LongTensor([[]]).to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d308ab65-26c8-4d0e-a1de-252969b5971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_batch = lossfns[0].tokenizer(source_text, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09906447-8132-4933-96ee-beabe4ae9c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sequence = lossfns[0].tokenizer(masked_text, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a30a3104-48d7-471a-bf36-0583e9f08c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_added_hypotheses = torch.cat([source_batch, hypotheses],dim=-1).to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "237d38b8-a2bb-491c-b969-cc3a7dff3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_added_hypotheses = torch.cat([source_batch.expand(hypotheses.size(0), -1), hypotheses], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e96ecb4e-bdf5-4469-b787-7e4d7e03039f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5756,  502,  655,  923,  416, 2282,  314, 5465, 8223]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_added_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec2eb111-f8db-4119-9022-f22affb5b469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5756,  502,  655,  923,  416, 2282,  314, 5465, 8223]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_added_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f85eaf5-b45b-47ef-b721-19190a5c7862",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = name2model['gpt2-large'](prefix_added_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a331b2e9-9c4a-40a6-90bf-4e2dc4abbd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_t = model_output.logits[:, -1, :] # get logits for the last timestep\n",
    "logp_t = F.log_softmax(logits_t, dim=-1) # (num_hypotheses, |V|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91f54bfb-8248-4f87-9cd3-3694dc85f1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50257])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logp_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed725ec7-0131-4d91-99b7-8c7a4d8e9ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "if masked_sequence[0, i] != mlm_tokenizer.mask_token_id:\n",
    "    print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6000e99e-cddc-4182-bd9f-73d3312e2bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = [torch.LongTensor([]).to(config['device'])]\n",
    "hypotheses = list(torch.cat([torch.stack(hypotheses,dim=0), \n",
    "                                        masked_sequence[:, i].unsqueeze(0).repeat((len(hypotheses),1)).to(config['device'])],dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f06f7314-4e1d-4abc-91b9-ebd850e5ffed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([50257], device='cuda:0')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba60dc3-476a-4f53-9593-56a14288820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_rerank_v2(\n",
    "                    source_batch: torch.Tensor,\n",
    "                    predicted_batch: torch.Tensor, \n",
    "                    edit_token_index_primary, \n",
    "                    primary_model: transformers.AutoModel, \n",
    "                    primary_tokenizer: transformers.AutoTokenizer,\n",
    "                    config: dict, \n",
    "                    beam_size: int\n",
    "                ) -> torch.Tensor:\n",
    "    \"\"\" Function that autoregressively edits a sequence(predicted_batch) by updating tokens at edit_token_index_primary indices and keeping the other tokens as were.\n",
    "    @param source_batch (Tensor): token ids of the prefix\n",
    "    @param predicted_batch (Tensor): token ids of the original continuation\n",
    "    @param edit_token_index_primary (Tensor): indices that indicate locations in the original continuation to edit\n",
    "    @param primary_model (AutoModel): model to calculate likelihood of candidate sequences\n",
    "    @param primary_tokenizer (AutoTokenizer): tokenizer for the primary_model\n",
    "    @param config (dict)\n",
    "    @param beam_size (int)\n",
    "\n",
    "    @returns hypotheses (Tensor): beam_size number of hypotheses to edit the original continuation. Tensor of shape (beam_size, sequence length).\n",
    "    \"\"\"\n",
    "\n",
    "    hypotheses = torch.LongTensor([[]]).to(config['device'])\n",
    "    hyp_scores = torch.zeros(len(hypotheses), dtype = torch.float, device = config['device'])\n",
    "    L = masked_sequence.size(-1)\n",
    "    \n",
    "    for t in range(seq_len):\n",
    "        \n",
    "        prefix_added_hypotheses = torch.cat([source_batch.expand(hypotheses.size(0), -1), hypotheses], dim=-1)\n",
    "        with torch.no_grad():\n",
    "            model_output = primary_model(input_ids = prefix_added_hypotheses)\n",
    "\n",
    "        logits_t = model_output.logits[:, -1, :] # get logits for the last timestep\n",
    "        logp_t = F.log_softmax(logits_t, dim=-1) # (num_hypotheses, |V|)\n",
    "        \n",
    "        if t not in edit_token_index_primary:\n",
    "            \n",
    "            curr_nll = F.nll_loss(logp_t, predicted_batch[:, t].expand(logp_t.size(0)), reduction=\"none\") # returns (num_hypotheses)\n",
    "            hyp_scores = hyp_scores.expand_as(curr_nll) + curr_nll # (num_hypotheses)\n",
    "            hypotheses = torch.cat([hypotheses, predicted_batch[:, t].expand(hypotheses.size(0), -1)], dim=-1)\n",
    "            \n",
    "        else:\n",
    "            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(logp_t) + (-logp_t)).view(-1) # (num_hypotheses x |V|)\n",
    "            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=beam_size, largest=True)\n",
    "            \n",
    "            prev_hyp_ids = torch.div(top_cand_hyp_pos, len(primary_tokenizer), rounding_mode='floor') # prev_hyp_id for each of top_cand_hyp. (beam_size)\n",
    "            hyp_word_ids = top_cand_hyp_pos % len(primary_tokenizer) # hyp_word_id for each of top_cand_hyp. (beam_size)\n",
    "            \n",
    "            hypotheses = torch.cat([hypotheses[prev_hyp_ids], hyp_word_ids.unsqueeze(1)], dim=-1)\n",
    "            hyp_scores = top_cand_hyp_scores\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    return hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851e5f6a-60d9-4e9b-9598-88ffc8c5612e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c6f54-e7e6-42e5-a7f4-81a440506980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9431a0a0-ff8c-48af-a365-79f810e46ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5863025f-9490-4f11-b22f-cb1f63f54868",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=pd.read_json('outputs/toxicity/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/gbi-word-netps3-nls1-os20-es4-allsat-toxic-to-nontoxic-attention-kf1j15xk/outputs_epsilon-3.txt',\n",
    "            lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d5aae-a025-48eb-b594-0c6bb2aabef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## unravel\n",
    "outputs=outputs.explode('generations')\n",
    "\n",
    "outputs.shape\n",
    "\n",
    "outputs['prompt']=outputs['prompt'].apply(lambda x: x['text'])\n",
    "\n",
    "outputs['text']=outputs['generations'].apply(lambda x: x['text'])\n",
    "\n",
    "gen_dict=outputs['generations'].values[0]\n",
    "\n",
    "for col in gen_dict.keys():\n",
    "    outputs[col] = outputs['generations'].apply(lambda x: x[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "245ff163-4865-4872-83ff-b1f0f85be984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>generations</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>allsat</th>\n",
       "      <th>losses</th>\n",
       "      <th>weighted_loss</th>\n",
       "      <th>repeat_count</th>\n",
       "      <th>mucoco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [prompt, generations, text, tokens, allsat, losses, weighted_loss, repeat_count, mucoco]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.loc[~outputs['mucoco'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f8b3362-dfa0-4491-a5a7-99919b92ed9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>generations</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>allsat</th>\n",
       "      <th>losses</th>\n",
       "      <th>weighted_loss</th>\n",
       "      <th>repeat_count</th>\n",
       "      <th>mucoco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>According to the criminal complaint, Edah kick...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>According to the criminal complaint, Edah kick...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>According to the criminal complaint, Edah kick...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>An advertisement for the product shows a girl ...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>An advertisement for the product shows a girl ...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Cruz began by mocking Trump’s reputation for i...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Behold, My Little Deathwing: Ponywing ...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>The lawsuits dispute Airbnb's assertion that i...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>The lawsuits dispute Airbnb's assertion that i...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Dotan says the bridge will serve as a runway f...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>And for those of you who are happily and comfo...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt  \\\n",
       "46   According to the criminal complaint, Edah kick...   \n",
       "46   According to the criminal complaint, Edah kick...   \n",
       "46   According to the criminal complaint, Edah kick...   \n",
       "51   An advertisement for the product shows a girl ...   \n",
       "51   An advertisement for the product shows a girl ...   \n",
       "70   Cruz began by mocking Trump’s reputation for i...   \n",
       "91           Behold, My Little Deathwing: Ponywing ...   \n",
       "134  The lawsuits dispute Airbnb's assertion that i...   \n",
       "134  The lawsuits dispute Airbnb's assertion that i...   \n",
       "167  Dotan says the bridge will serve as a runway f...   \n",
       "223  And for those of you who are happily and comfo...   \n",
       "\n",
       "                                           generations text tokens  allsat  \\\n",
       "46   {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "46   {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "46   {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "51   {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "51   {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "70   {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "91   {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "134  {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "134  {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "167  {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "223  {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "\n",
       "    losses  weighted_loss  repeat_count  mucoco  \n",
       "46      -1           -1.0            -1    True  \n",
       "46      -1           -1.0            -1    True  \n",
       "46      -1           -1.0            -1    True  \n",
       "51      -1           -1.0            -1    True  \n",
       "51      -1           -1.0            -1    True  \n",
       "70      -1           -1.0            -1    True  \n",
       "91      -1           -1.0            -1    True  \n",
       "134     -1           -1.0            -1    True  \n",
       "134     -1           -1.0            -1    True  \n",
       "167     -1           -1.0            -1    True  \n",
       "223     -1           -1.0            -1    True  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.loc[outputs['weighted_loss']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d560e-75af-4ff5-b938-943ac1293fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08fcf6a1-f1b3-442b-81b1-4593d61a5b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir('/home/s3/hyeryung/mucoco')\n",
    "os.chdir('/data/hyeryung/mucoco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c066e8e3-16d6-4ebe-b7ef-e62b6f9302d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb3352e-8a67-4733-9b23-7cae5182004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from new_module.losses import BaseLoss, register_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3313b997-e7ec-4879-b7ea-c2ea2cbbbe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda:1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f8b2e2",
   "metadata": {},
   "source": [
    "# GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f6deacf-1bac-4fe0-beaa-be4cd46c6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=AutoModel.from_pretrained('gpt2-large',cache_dir='/shared/s3/lab07/hyeryung/hf_cache')\n",
    "model=AutoModelForCausalLM.from_pretrained('gpt2-large',cache_dir='/data/hyeryung/hf_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f41454-ee02-4b54-93aa-93c490fc1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(device)\n",
    "model=model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5afacd68-e0c6-4a41-8df3-ff4a83bab0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('gpt2-large')\n",
    "tokenizer.pad_token_id =tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc3fe319-6377-45b8-b4b5-1316e80cbcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='abc'\n",
    "gens=['dsxe','sdvbfe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "327384c9-f3b4-4405-a94d-eb435ba8879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "num_samples=len(gens); print(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "286ad9f4-9d70-41ab-8fab-bfa4694974d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_enc=tokenizer.encode_plus(prompt,add_special_tokens=False, return_tensors=\"pt\", padding=True, truncation=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05e8dc95-ae4f-400f-b5e8-276aa96c5c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_enc['input_ids']=prompt_enc['input_ids'].expand(num_samples,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "914f2777-004d-41cf-91c0-7f3430151016",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_enc['attention_mask']=prompt_enc['attention_mask'].expand(num_samples,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a38b3046-6b44-4a3b-819b-7d5f66354a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[39305],\n",
       "        [39305]], device='cuda:1'), 'attention_mask': tensor([[1],\n",
       "        [1]], device='cuda:1')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae7eaf10-e8bc-4880-a40f-c10f8566e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gens_enc=tokenizer.batch_encode_plus(gens, add_special_tokens=False, return_tensors=\"pt\", padding=True, truncation=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a38eb9c-d6d5-4f4f-b308-96278e09679c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 9310, 27705, 50256, 50256],\n",
       "        [21282,    85,    65,  5036]], device='cuda:1'), 'attention_mask': tensor([[1, 1, 0, 0],\n",
       "        [1, 1, 1, 1]], device='cuda:1')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gens_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f24b9417-4a91-4f42-8b94-bd0b3e162ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = torch.cat([prompt_enc.input_ids, gens_enc.input_ids], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fa19425-70ff-4ebe-9606-dc8aca9fcb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = torch.cat([prompt_enc.attention_mask, gens_enc.attention_mask], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8410fc8-e09b-4b9a-9b91-4189267f21b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(input_ids=input_tokens,\n",
    "                    attention_mask=attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3634640a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d847343a-1831-4708-be11-f63e6e433aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_logits = model_output[0][:, prompt_enc.input_ids.size(1)-1:-1, :]\n",
    "lm_logprobs = F.log_softmax(lm_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd92f986-5396-447c-8f24-da80de76468b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 50257])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_logprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa4880ec-1cbd-4590-b45c-1654c6727a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gens_enc.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ca04c7d-ef6a-431e-93fe-bb9de0494826",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.nll_loss(lm_logprobs.permute(0,2,1), gens_enc.input_ids, reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "114536e3-2c40-491a-aa8f-65ede0225f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss * gens_enc.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2e94f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9185fd8-3474-47cf-a9dc-4c90b41f64c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cd544a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19.280, 26.730], device='cuda:1', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55bf0d89-efdd-4be1-99c4-012896d2fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss /= gens_enc.attention_mask.sum(dim=-1) ## 이렇게 하는게 맞을지 조금 고민이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02174cef-6c8c-4a1c-b0a9-1e7765cdf83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.640, 6.683], device='cuda:1', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4963b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class GPT2Loss(BaseLoss):\n",
    "\n",
    "    def __init__(self, model, tokenizer, args):\n",
    "        super().__init__() \n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer \n",
    "        self.args = args\n",
    "        self.device = model.device\n",
    "        \n",
    "        self.eos_token_id = self.tokenizer.eos_token_id    \n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model.config.pad_token_id = self.model.config.eos_token_id # to remove the warning\n",
    "    \n",
    "    def compute_gold_loss(self, prompt:str, predictions:List[str], **kwargs):\n",
    "        '''\n",
    "        given a discrete target output, this will compute the loss wrt to it. Useful in debugging\n",
    "        '''\n",
    "        # prompt = self.tokenizer.batch_encode_plus(prompt, add_special_tokens=False, return_tensors=\"pt\", padding=True, truncation=True).to(self.device).long()\n",
    "        # # assuming batch size of 1 (prediction is a string instance.)\n",
    "        # prediction = self.tokenizer.batch_encode_plus(prediction, add_special_tokens=False, return_tensors=\"pt\", padding=True, truncation=True).to(self.device).long()\n",
    "        # input_tokens = torch.cat([prompt.input_ids, prediction.input_ids], dim=1)\n",
    "        # model_output = self.model(input_tokens)\n",
    "\n",
    "        # lm_logits = model_output[0][:, prompt.size(1)-1:-1, :]\n",
    "        # lm_logprobs = F.log_softmax(lm_logits, dim=-1)\n",
    "\n",
    "        # loss = F.nll_loss(lm_logprobs.squeeze(0), prediction.squeeze(0), reduction=\"none\").sum(dim=-1)\n",
    "        \n",
    "        # if self.args.length_normalize:\n",
    "        #     loss /= lm_logprobs.size(1)\n",
    "\n",
    "        # return loss\n",
    "        num_samples = len(predictions)\n",
    "        prompt_enc=self.tokenizer.encode_plus(prompt,add_special_tokens=False, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "        prompt_enc['input_ids']=prompt_enc['input_ids'].expand(num_samples,-1)\n",
    "        prompt_enc['attention_mask']=prompt_enc['attention_mask'].expand(num_samples,-1)\n",
    "    \n",
    "        predictions_enc=self.tokenizer.batch_encode_plus(predictions, add_special_tokens=False, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "\n",
    "        input_tokens = torch.cat([prompt_enc.input_ids, predictions_enc.input_ids], dim=1)\n",
    "        attention_masks = torch.cat([prompt_enc.attention_mask, predictions_enc.attention_mask], dim=1)\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(input_ids=input_tokens,\n",
    "                                attention_mask=attention_masks)\n",
    "        lm_logits = model_output[0][:, prompt_enc.input_ids.size(1)-1:-1, :]\n",
    "        lm_logprobs = F.log_softmax(lm_logits, dim=-1)\n",
    "\n",
    "        # input dimensions : (N, C, d1), (N, d1)\n",
    "        loss = F.nll_loss(lm_logprobs.permute(0,2,1), predictions_enc.input_ids, reduction=\"none\")\n",
    "        loss = loss * predictions_enc.attention_mask # make losses for pad tokens 0.\n",
    "        \n",
    "        loss = loss.sum(dim=-1)\n",
    "        if self.args.length_normalize:\n",
    "            loss /= predictions_enc.attention_mask.sum(dim=-1) \n",
    "        return loss # dimensions: (N)\n",
    "    \n",
    "    def generate(self, input_ids, **kwargs):\n",
    "        prepared_input = self._prepare_input_for_generation(input_ids, **kwargs)\n",
    "        output = self.model.generate(**prepared_input)\n",
    "        \n",
    "        return self._postprocess_output(prepared_input, output)\n",
    "\n",
    "    def _prepare_input_for_generation(self, input_ids, **kwargs):\n",
    "        max_output_length = getattr(self.args, \"max_output_length\", 10)\n",
    "        batch_size = input_ids.size(0)\n",
    "        #batch size is 1, padding and stuff needs to be modified for this to work for larger batches\n",
    "\n",
    "        return_object = {'input_ids': input_ids,\n",
    "                'max_length': input_ids.size(1) + max_output_length,\n",
    "                'do_sample': True,\n",
    "                'temperature': self.args.AR_temperature,\n",
    "                'top_k': self.args.AR_top_k,\n",
    "                'top_p': self.args.AR_top_p,\n",
    "                'num_return_sequences': kwargs.get('num_return_sequences', 1)}\n",
    "   \n",
    "        return return_object\n",
    "    \n",
    "    def _postprocess_output(self, prepared_input, output_ids):\n",
    "        return output_ids[:, prepared_input['input_ids'].size(1):, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67d9fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    length_normalize = False\n",
    "\n",
    "gpt2_loss = GPT2Loss(model,tokenizer,Args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96e80952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19.280, 26.730], device='cuda:1')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_loss.compute_gold_loss(prompt, gens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd435b3b",
   "metadata": {},
   "source": [
    "# Classification no prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6fc4559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b34bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint/'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckpt_path)\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c00388e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1763bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='abc'\n",
    "prediction=['dsxe','sdvbfe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2103c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a04e5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "210206d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tokenizer.batch_encode_plus(prediction, add_special_tokens=True, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        \n",
    "# eos = torch.empty((prediction.size(0), 1)).long().to(device).fill_(eos_token_id)\n",
    "# prediction = torch.cat([prediction, eos, eos], dim=1)\n",
    "\n",
    "model_output = model(**prediction)\n",
    "lm_logits = model_output[0]\n",
    "lm_logprobs = F.log_softmax(lm_logits, dim=-1)\n",
    "loss = -lm_logprobs[:, label_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5d216acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.142, 0.013], device='cuda:1', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b2e3f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationLogProbLoss(BaseLoss):\n",
    "\n",
    "    def __init__(self, model, tokenizer, args):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.model = model \n",
    "        self.tokenizer = tokenizer \n",
    "        self.args = args\n",
    "        self.device = model.device\n",
    "\n",
    "        self.bos_token_id = self.tokenizer.bos_token_id\n",
    "        self.eos_token_id = self.tokenizer.eos_token_id    \n",
    "\n",
    "    def compute_gold_loss(self, prompt:str, prediction:List[str], label_id, **kwargs):\n",
    "        '''\n",
    "        given a discrete target output, this will compute the loss wrt to it. Useful in debugging\n",
    "        '''\n",
    "\n",
    "        # prediction = self.tokenizer.encode(prediction, add_special_tokens=True, return_tensors=\"pt\", padding=True, truncation=True).to(self.device).long()\n",
    "        \n",
    "        # eos = torch.empty((prediction.size(0), 1)).long().to(self.device).fill_(self.eos_token_id)\n",
    "        # prediction = torch.cat([prediction, eos, eos], dim=1)\n",
    "    \n",
    "        # model_output = self.model(prediction)\n",
    "        # lm_logits = model_output[0]\n",
    "        # lm_logprobs = F.log_softmax(lm_logits, dim=-1)\n",
    "        # loss = -lm_logprobs[:, label_id]\n",
    "        # return loss\n",
    "        \n",
    "        prediction = self.tokenizer.batch_encode_plus(prediction, add_special_tokens=True, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "        model_output = self.model(**prediction)\n",
    "        lm_logits = model_output[0]\n",
    "        lm_logprobs = F.log_softmax(lm_logits, dim=-1)\n",
    "        loss = -lm_logprobs[:, label_id]\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6968c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "clsf_loss = ClassificationLogProbLoss(model, tokenizer, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d51655f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.142, 0.013], device='cuda:1', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "clsf_loss.compute_gold_loss(prompt, gens, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833eb98f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/data/hyeryung/mucoco')\n",
    "\n",
    "from transformers import AutoModelForMaskedLM, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from new_module.locate.new_locate_utils import LocateMachine\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel(outputs_df):\n",
    "    outputs_df=outputs_df.explode('generations',ignore_index=True)\n",
    "    outputs_df['prompt']=outputs_df['prompt'].apply(lambda x: x['text'])\n",
    "    outputs_df['generations']=outputs_df['generations'].apply(lambda x: x['text'] if isinstance(x, dict) else x)\n",
    "    outputs_df = outputs_df.dropna().reset_index(drop=True)\n",
    "    return outputs_df\n",
    "toxic_gen=pd.read_json('new_module/data/toxicity-avoidance/dev_set.jsonl',lines=True)\n",
    "toxic_gen=unravel(toxic_gen)\n",
    "prompt = '“I went to use the bathroom, and I went in and Lance was bent'\n",
    "prediction = toxic_gen.loc[toxic_gen['prompt']==prompt,'generations'].tolist()\n",
    "ckpt_path = '/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint/'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckpt_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n",
    "device='cuda'\n",
    "model = model.to(device)\n",
    "loc_machine=LocateMachine(model,tokenizer)\n",
    "res = loc_machine.locate_main(prediction, \"attention\", max_num_tokens = 6, unit=\"word\", num_layer=10, label_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_tokenizer = AutoTokenizer.from_pretrained('roberta-base',cache_dir='/data/hyeryung/hf_cache')\n",
    "mlm = AutoModelForMaskedLM.from_pretrained('roberta-base',cache_dir='/data/hyeryung/hf_cache')\n",
    "mlm = mlm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_text = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace tokens at the indices with mask tokens\n",
    "inputs = mlm_tokenizer(\n",
    "    masked_text, return_tensors=\"pt\", padding=True, truncation=True\n",
    ")\n",
    "inputs = inputs.to(device) \n",
    "# inputs = mlm_tokenizer(\n",
    "#     source_text + ' ' + masked_text[0], return_tensors=\"pt\", add_special_tokens=False\n",
    "# )\n",
    "\n",
    "## make predictions for the masked indices\n",
    "with torch.no_grad():\n",
    "    logits = mlm(**inputs).logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token_ids = mlm_tokenizer.convert_tokens_to_ids(mlm_tokenizer.all_special_tokens)\n",
    "logits[:, :, special_token_ids] = -float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_in_mlm_tokens = (\n",
    "    inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    ").nonzero(as_tuple=True)\n",
    "# print(f\"indices_in_mlm_tokens: {indices_in_mlm_tokens}\")\n",
    "## get top k tokens for each index\n",
    "\n",
    "predicted_token_ids = torch.topk(\n",
    "    logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "    k=10,\n",
    "    dim=-1,\n",
    ")\n",
    "# print(f\"predicted_token_ids: {predicted_token_ids}\")\n",
    "# print(f\"mlm_tokenizer.batch_decode(predicted_token_ids.indices): {mlm_tokenizer.batch_decode(predicted_token_ids.indices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65, 10])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_ids.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65, 10])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_ids.indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_token_ids: torch.return_types.topk(\n",
      "values=tensor([[22.0476, 19.4729, 17.5394, 16.8642, 16.5963, 16.4827, 16.4021, 15.7711,\n",
      "         15.7436, 15.3570],\n",
      "        [17.5601, 16.3528, 15.0402, 14.9633, 14.3649, 13.8561, 13.2594, 13.2089,\n",
      "         12.6610, 12.5642],\n",
      "        [16.7789, 15.7252, 15.5652, 15.4932, 15.3546, 15.3041, 15.2817, 15.1390,\n",
      "         14.8545, 14.6381],\n",
      "        [16.9867, 16.6328, 16.1767, 14.9714, 14.3802, 13.4213, 13.3534, 13.3342,\n",
      "         12.8598, 12.8383],\n",
      "        [15.5927, 15.5903, 15.5631, 14.8100, 14.4180, 14.1002, 13.6288, 13.5641,\n",
      "         13.5315, 13.0986],\n",
      "        [16.8475, 16.7198, 16.0989, 15.9949, 15.4332, 15.2327, 15.2147, 14.9328,\n",
      "         14.8248, 14.5735],\n",
      "        [19.4643, 14.3413, 13.1266, 12.6918, 12.6008, 12.3762, 11.8818, 11.5772,\n",
      "         11.5281, 11.4883],\n",
      "        [17.3266, 15.7880, 15.7409, 15.6494, 15.2200, 14.8165, 14.7254, 14.6882,\n",
      "         14.5433, 14.4847],\n",
      "        [16.9962, 16.8003, 16.3852, 16.3049, 15.7828, 14.8172, 14.3738, 14.3228,\n",
      "         14.3083, 14.1886],\n",
      "        [13.7294, 11.5412, 10.8240, 10.7314, 10.7028, 10.6764, 10.5329, 10.4109,\n",
      "         10.3960, 10.3690],\n",
      "        [12.3987, 11.9212, 10.4466, 10.1989,  9.6715,  9.6135,  9.2713,  8.3205,\n",
      "          8.2772,  8.1494],\n",
      "        [20.3395, 17.1361, 16.0284, 14.1271, 13.9386, 13.5283, 13.4905, 13.4451,\n",
      "         13.3406, 13.3271],\n",
      "        [15.3060, 14.7931, 13.6338, 13.4726, 13.4477, 13.0987, 13.0713, 12.9350,\n",
      "         12.8565, 12.7045],\n",
      "        [21.6066, 17.7277, 17.2978, 15.2125, 14.3531, 14.2456, 13.9140, 13.6551,\n",
      "         13.4436, 13.1048],\n",
      "        [17.5683, 15.8340, 15.3837, 14.9424, 14.8776, 14.6397, 14.6173, 14.5269,\n",
      "         14.5191, 14.4529],\n",
      "        [17.2929, 16.9789, 16.3205, 16.1346, 16.1313, 15.8705, 15.8305, 15.7945,\n",
      "         15.7890, 15.7714],\n",
      "        [16.6591, 16.4555, 15.5120, 15.3955, 15.2928, 15.0606, 14.9854, 14.8208,\n",
      "         14.7260, 14.2846],\n",
      "        [24.6838, 22.2196, 20.0829, 19.6098, 18.6050, 17.4756, 17.2337, 16.9556,\n",
      "         16.9251, 16.8624],\n",
      "        [17.3613, 17.2660, 17.2039, 16.6825, 16.3908, 16.3624, 16.1745, 16.1668,\n",
      "         16.0909, 15.8996],\n",
      "        [16.5434, 15.6583, 14.6893, 13.6877, 13.3963, 12.7079, 12.4097, 12.1087,\n",
      "         12.0791, 11.9534],\n",
      "        [11.6818, 11.3892, 11.2418, 10.7842, 10.6130, 10.5801, 10.5325, 10.4319,\n",
      "         10.3238, 10.2934],\n",
      "        [14.6520, 10.6053, 10.2869, 10.2206, 10.1680,  9.7241,  9.6235,  9.4395,\n",
      "          9.4208,  9.3407],\n",
      "        [16.2670, 16.1860, 12.2441, 12.0107, 11.9568, 11.8927, 11.7062, 11.6845,\n",
      "         11.5221, 11.4659],\n",
      "        [16.2611, 15.6528, 15.5850, 15.3666, 15.0552, 14.9438, 14.8739, 14.7708,\n",
      "         14.6664, 14.6562],\n",
      "        [16.7940, 16.0715, 15.6949, 14.3513, 14.2239, 12.6533, 12.2404, 11.7762,\n",
      "         11.7043, 11.6140],\n",
      "        [12.5030, 12.1752, 11.3853, 11.0454, 10.9880, 10.8681, 10.7428, 10.7190,\n",
      "         10.7164, 10.6158],\n",
      "        [12.4775, 10.6382,  9.4757,  9.4126,  9.2984,  9.1551,  9.1359,  8.7835,\n",
      "          8.7765,  8.6265],\n",
      "        [14.8725, 10.6246,  9.6712,  9.6377,  9.5931,  9.5598,  9.5452,  9.4959,\n",
      "          9.4917,  9.4481],\n",
      "        [18.0430, 16.4545, 16.2739, 14.1019, 14.0798, 13.7798, 13.4499, 13.2706,\n",
      "         12.9771, 12.3293],\n",
      "        [16.2178, 14.4060, 14.0525, 13.2931, 13.2857, 13.1219, 12.9970, 12.7741,\n",
      "         12.7433, 12.6224],\n",
      "        [11.0056,  8.4309,  8.1386,  7.9530,  7.8284,  7.7673,  7.2747,  7.0072,\n",
      "          6.9334,  6.8611],\n",
      "        [ 8.4579,  7.3155,  6.8890,  6.8741,  6.7990,  6.7346,  6.7270,  6.7043,\n",
      "          6.6640,  6.5345],\n",
      "        [13.3470, 11.2756, 11.1037, 10.9579,  9.2129,  8.9679,  8.7225,  8.6266,\n",
      "          8.3710,  8.1272],\n",
      "        [12.5272, 12.3023, 12.1785, 11.9845, 11.7178, 11.6290, 11.4701, 11.3804,\n",
      "         11.2989, 11.0957],\n",
      "        [17.4838, 14.7471, 12.7699, 11.9957, 10.8344, 10.7814, 10.6511, 10.4972,\n",
      "         10.3356, 10.1236],\n",
      "        [12.5862, 12.2585, 12.1265, 12.0491, 12.0333, 11.9309, 11.8144, 11.6493,\n",
      "         11.2648, 11.2431],\n",
      "        [15.4437, 15.3970, 15.3796, 14.3433, 14.0274, 13.8893, 13.5711, 13.4968,\n",
      "         13.4756, 13.3448],\n",
      "        [14.8397, 14.2946, 14.2432, 13.6779, 13.4320, 13.2952, 13.0454, 13.0384,\n",
      "         12.9852, 12.9375],\n",
      "        [17.3366, 17.2938, 17.2633, 17.0872, 16.7387, 16.7199, 16.6693, 16.4655,\n",
      "         16.2365, 16.1801],\n",
      "        [13.8143, 12.5842, 12.5532, 12.4577, 12.3116, 12.1870, 11.8876, 11.8222,\n",
      "         11.8101, 11.7884],\n",
      "        [11.9764, 11.8356, 11.0066, 11.0048, 10.9544, 10.5593, 10.3371, 10.1163,\n",
      "         10.1064, 10.0791],\n",
      "        [10.1244,  9.8185,  9.8136,  9.7552,  9.6990,  9.4315,  9.2682,  9.2578,\n",
      "          9.0533,  9.0467],\n",
      "        [10.3246, 10.3117, 10.2452,  9.3077,  9.2941,  9.2631,  9.2600,  9.0213,\n",
      "          8.9470,  8.7906],\n",
      "        [13.1737, 12.1326, 11.5879, 11.5153, 11.2998, 10.9739, 10.7371, 10.4959,\n",
      "         10.3613, 10.0577],\n",
      "        [18.1189, 17.6685, 17.5668, 16.5649, 16.5332, 16.4984, 16.4581, 16.2756,\n",
      "         16.2651, 16.2184],\n",
      "        [12.1741, 11.5570, 11.4887,  9.9604,  9.9159,  9.6920,  9.6651,  9.2638,\n",
      "          9.2059,  9.1644],\n",
      "        [14.8374, 14.2960, 13.2226, 12.0285, 10.6259, 10.5887, 10.4392, 10.4063,\n",
      "         10.2194, 10.2073],\n",
      "        [17.5327, 13.8236, 13.5184, 13.4858, 13.3646, 13.3524, 13.2270, 13.1555,\n",
      "         13.1448, 12.7813],\n",
      "        [17.0002, 14.9120, 14.3164, 13.8453, 12.5836, 12.4139, 12.3873, 12.1105,\n",
      "         11.9802, 11.9759],\n",
      "        [16.0269, 12.7591, 12.2586, 11.7645, 10.4903, 10.1939, 10.0674, 10.0284,\n",
      "          9.8827,  9.8295],\n",
      "        [13.6261, 12.2448,  9.6000,  9.5228,  9.0972,  9.0584,  8.9690,  8.8170,\n",
      "          8.7639,  8.4920],\n",
      "        [11.6912, 10.9697, 10.7246, 10.2914, 10.2655, 10.0594,  9.6215,  9.5316,\n",
      "          9.5197,  9.4886],\n",
      "        [14.6190, 13.6906, 13.4823, 13.1197, 13.0796, 12.9315, 12.9179, 12.8872,\n",
      "         12.8727, 12.5557],\n",
      "        [15.6871, 15.1593, 13.7373, 13.3152, 12.2546, 11.7941, 11.4437, 11.0900,\n",
      "         10.9021, 10.7618],\n",
      "        [13.0073, 12.2181, 12.1756, 11.3281, 11.2876, 11.1551, 11.1233, 11.1039,\n",
      "         10.8946, 10.6629],\n",
      "        [11.6256, 11.4828, 11.2422, 11.1773, 11.0506, 11.0234, 10.9395, 10.8891,\n",
      "         10.8563, 10.6002],\n",
      "        [14.4815, 13.5182,  9.9793,  9.8677,  9.3634,  9.2744,  8.8969,  8.8018,\n",
      "          8.7192,  8.6465],\n",
      "        [19.2828, 13.4236, 13.2601, 13.1129, 12.9068, 12.8670, 12.8370, 12.7302,\n",
      "         12.7177, 12.5860],\n",
      "        [12.0505, 11.7634, 11.5489, 11.3869, 11.3336, 11.2840, 11.2032, 11.2020,\n",
      "         10.9349, 10.8301],\n",
      "        [12.4332, 12.0853, 11.8002, 11.5481, 11.3784, 11.1415, 10.9458, 10.7536,\n",
      "         10.7411, 10.6720],\n",
      "        [10.8997,  9.7332,  8.9318,  8.7648,  8.5686,  8.5231,  8.2146,  8.1864,\n",
      "          8.0756,  8.0754],\n",
      "        [ 9.2460,  8.9909,  8.8651,  8.3054,  8.3053,  8.0316,  7.8765,  7.4020,\n",
      "          7.3568,  7.3073],\n",
      "        [ 9.4998,  9.4989,  9.3606,  8.8060,  8.3204,  8.1605,  8.0850,  7.6194,\n",
      "          7.5446,  7.4015],\n",
      "        [ 9.8854,  9.1884,  9.0537,  8.7679,  8.3014,  8.0413,  7.7634,  7.6151,\n",
      "          7.5579,  7.5433],\n",
      "        [11.0511, 10.8366, 10.3990, 10.1026,  9.5487,  9.5048,  9.4536,  9.2226,\n",
      "          9.1748,  9.1358]], device='cuda:0'),\n",
      "indices=tensor([[  100,   894,  1213,   170, 11243,  4763,   243,  1620,  2515, 15216],\n",
      "        [   21,  1682,  2145,   437,  2294,   554,   489,    58,  1381,   524],\n",
      "        [14575,   206,   912,  3581,  8016,   517,   422, 22093,  1067,  2145],\n",
      "        [    4,   142,     8,     6,    25,    77,    53,    14,    99,   454],\n",
      "        [  106,   123, 18424,  6820,    82,  1925,    24,  6941, 21715, 14006],\n",
      "        [  123,  9589,   106,  1375,    24,   667,  2185,  9701, 17587,   878],\n",
      "        [    4,   328,   734,  1555,  1666,  1174,     6,   479,    72,    35],\n",
      "        [29374,   383,  5418, 15684,  1420,  5582, 23332, 37047, 10317, 14341],\n",
      "        [27827,   100,   894,  6785, 18891, 22616,  9690, 15173,  8987,   170],\n",
      "        [  101,  1462,  7758,  8180,  1613,    98, 11339,   235,   269, 34449],\n",
      "        [    4,    35,   116,  1174,   328,   734,  1555,  1666,   162,    72],\n",
      "        [  894,   700,   100,  8773, 10567, 12083, 25244, 33667, 15827, 14009],\n",
      "        [  376,   439, 20185,  1348, 13503,  3148,  2294,  1224,  1410, 21654],\n",
      "        [   39,    10,   127,    65,   123,   277,     5,    42,    14,   110],\n",
      "        [ 2549,   652,  6399,   865,  3124,  6085,  9304,  9377,  7524,  8040],\n",
      "        [ 3124,   124,  2985,  6181,   865,   652, 14599,  9377,  5397, 18781],\n",
      "        [  865,  2549,   652, 32461,  3124,   124,  6399,  6181,  9377, 14262],\n",
      "        [  113,   100,   894,  2515,  1213, 46353,   170, 46150, 47096, 11243],\n",
      "        [  100,  6785,   894, 15216, 11243, 33724, 13721,  2515, 29375, 22174],\n",
      "        [  127,     5,    10,   162,    39,    69,    24,    14,    42,   402],\n",
      "        [  865,  1883, 21342,  9304, 16179,  1420,  3298,  1028, 10654,   464],\n",
      "        [    6, 21342,  3298,   865,   631,  1883,  6148,  1555, 10654, 30241],\n",
      "        [   79,    37,  1436,  1259,  1547,  1604,  2505,  1454,  3452,  2250],\n",
      "        [32424,   560, 40900, 35349, 43414, 13837,  9591, 19256,  5632, 27300],\n",
      "        [   38,    37,    24,    89,    79,   960,    14,    99,  3370,   961],\n",
      "        [   11,  2190,    10,  2429,    95,   878,     5,    15,  1826,   127],\n",
      "        [  162,   123,  8265,    24,    69,   409,    89,   159,  2400,   124],\n",
      "        [    4,   328,   116,  1666,  1174,     6,   734,  1555,   479,   162],\n",
      "        [   37,    24,    79,    38,  5907,  3370,   961,  4909,   951,    51],\n",
      "        [  243,   133,   100,  1711,  2264,   713,  2409,   170,   970,   894],\n",
      "        [   21,    24,     5,    58,     6,    18,     4,    10,    56,   243],\n",
      "        [   24,    21,    11,    89,  2721,  2131,  8080,   929,    14,    66],\n",
      "        [    4,   328,   116,    35,     6,   742,    72,  1666,   734,  1174],\n",
      "        [ 2721,  2770,  1969,  1307,  2131,  5802,  3997, 14011, 12058,  2933],\n",
      "        [    4,   328,    35,   116,  1174,   734,   479,  1666,    72,  1555],\n",
      "        [ 3820, 14633,  2913,     6,   455,  5802, 10905,  1104,  1969,  6474],\n",
      "        [ 4682,     6,    19,    30,    13,    35,    93,   227,     9,    31],\n",
      "        [ 9310, 12045, 17465, 16433, 11471, 34034,  3535, 11824,  3716,  1929],\n",
      "        [  179,   415, 12473,   560,  1121, 40900,  9591, 10777,  3750, 27227],\n",
      "        [ 2468,  3148,   547,   551,  1654, 15158,  8473,  1432,  2037,  7249],\n",
      "        [   30,     7,    11,    66,    19,    88,   159,    62,    10,   124],\n",
      "        [    5,    10,    66,    30,    39,     7,   514,    11,    19,     9],\n",
      "        [   11,    39,     9,    30,     5,  7727,    19,     7,   514,  2498],\n",
      "        [   39,    18,  7727,   127,     9,     8,     5, 11025,    69,  6966],\n",
      "        [ 4148,  2409,  1121, 21674,  2765,  3684,  1708,  3750,  7605,  2264],\n",
      "        [  253,   169,   177,    86,   569,  1151,   477, 10299,  1079,   822],\n",
      "        [   35,     4,   116,     6,  1666,    93,   328,  1174,   734,  1555],\n",
      "        [   37,    38,    24,    10,    14,    52,  3370,    42,    47,   961],\n",
      "        [   21,    18,     6,    58,    95,  1682,  2173,   214,     8,   437],\n",
      "        [  828,    55,  2125,     9,  1823,  1280, 15836,   422,  9049, 10084],\n",
      "        [    9,    55,    10,  1823,     8,     5,    12,    50,   103,     7],\n",
      "        [   86,    24,   173,   123,  1007,  1351,   402,  1524,  2682, 12989],\n",
      "        [ 1174,   894,   734,  2515,     6,  1711, 10567,  7516,   578,  8773],\n",
      "        [    5,   127,    10,    84,    24,    39,    14,    41,    65,     6],\n",
      "        [ 1011,  3242,  6325, 29432, 20604, 19015,  5405,  1028,  3403,  3188],\n",
      "        [10802,  8411, 13670, 33138,  2473,  1028, 19015, 15686,   865, 15459],\n",
      "        [    4,    35,   734,  1174,  1555,   328,     6,  1666,    72,   131],\n",
      "        [48584,    17, 49093, 47639, 48555,   176, 43251,   246,  2744,  4056],\n",
      "        [ 1275,  5718,  1104,  5009,   249,  3579,  2440,   200,    78,  6907],\n",
      "        [   11,    15,    25, 11291,    23,    19,   137,    71,     6,     7],\n",
      "        [    5,    10,     6,     8,    11,     7,    25,    15,   639,    23],\n",
      "        [    5,     6,     8,    25,   514,     9,    10,   150,     7,   512],\n",
      "        [    8,     5,     6,    25,     9,    19,   150,    24,    10,   514],\n",
      "        [    5,     8,    24,     6,    16,   514,    63,     9,    19,    10],\n",
      "        [   16,     8,   137,    71,   514,     6,    24,   150,   450,     9]],\n",
      "       device='cuda:0'))\n",
      "mlm_tokenizer.batch_decode(predicted_token_ids.indices): ['IHeTheyWeEveryonePeopleItAsSheEverybody', \" was kept remember'm stopped started keep were tried am\", ' breathe think stop sleep breath move run scream talk remember', '. because and, as when but that what until', ' them him footsteps voices people blood it tears gunshots bullets', ' him breathing them moving it trying myself crying praying running', '.!... …...…,..\":', ' handcuffs things clothes headphones hands shoes restraints goggles boots gloves', 'SomethingIHeJustsomethingThingsMoreEverythingjustWe', ' like dead mad dying gone so laughing right really pissed', '.:?…!... …... me.\"', 'HeheIDavidJohnPaulRobertMatthewMikeMichael', ' came went smiled reached laughed pushed stopped turned moved squeezed', ' his a my one him another the this that your', ' hair face shirt hand arm mouth pants stomach pocket lap', ' arm back leg breast hand face throat stomach neck thigh', ' hand hair face vagina arm back shirt breast stomach palm', '\"IHeSheThey\"…We\"\\'\"...Everyone', 'IJustHeEverybodyEveryoneStartLikeSheSomeoneStop', ' my the a me his her it that this something', ' hand door towel pants stall hands bag phone keys change', ', towel bag hand thing door button … keysbrush', ' she he Johnson Smith Brown Williams Davis Jones Anderson Jackson', 'behindtoHitsawBehindaroundhitfoundwithgrab', ' I he it there she everything that what everybody everyone', ' in fighting a killing just running the on holding my', ' me him scared it her away there down pain back', '.!?...…,... …. me', ' he it she I nobody everybody everyone somebody someone they', 'ItTheIThatWhatThisAndWeThereHe', \" was it the were,'s. a hadIt\", ' it was in there beautiful hot bathroom room that out', '.!?:,].\"......…', ' beautiful amazing perfect huge hot empty incredible unbelievable gorgeous dark', '.!:?…........\" …', ' filled decorated covered, full empty bare white perfect lit', ' except, with by for: — between of from', ' shower sink tub couch toilet sinks showers bath pool floor', 'inatintotoInHithitOverAtonto', ' pulled pushed held taken forced washed swept followed caught grabbed', ' by to in out with into down up a back', ' the a out by his to water in with of', ' in his of by the wet with to water wearing', \" his's wet my of and the yoga her swim\", 'OnAndInAboutByAllButAtFromWhat', ' end way game time video moment point meantime rest film', ':.?,... —!…... …', ' he I it a that we everybody this you everyone', \" was's, were just kept guy're and'm\", ' bit more piece of extra amount chunk run spark burst', ' of more a extra and the- or some to', ' time it work him energy effort something practice stuff rhythm', '…He...She,ThatJohnOh—David', ' the my a our it his that an one,', ' ball pitch bat slider fastball glove switch phone baseball radio', ' fingers finger imagination fingertips eyes phone glove tongue hand thumb', '.:...… …!,....\";', '��●********************************�2�3+�', ' red yellow white damaged police stolen blue second first pink', ' in on as floating at with before after, to', ' the a, and in to as on behind at', ' the, and as water of a while to car', ' and the, as of with while it a water', ' the and it, is water its of with a', ' is and before after water, it while seen of']\n"
     ]
    }
   ],
   "source": [
    "## replace tokens at the indices with mask tokens\n",
    "inputs = mlm_tokenizer(\n",
    "    masked_text, return_tensors=\"pt\", padding=True, truncation=True\n",
    ")\n",
    "inputs = inputs.to(device) \n",
    "# inputs = mlm_tokenizer(\n",
    "#     source_text + ' ' + masked_text[0], return_tensors=\"pt\", add_special_tokens=False\n",
    "# )\n",
    "\n",
    "## make predictions for the masked indices\n",
    "with torch.no_grad():\n",
    "    logits = mlm(**inputs).logits\n",
    "\n",
    "special_token_ids = mlm_tokenizer.convert_tokens_to_ids(mlm_tokenizer.all_special_tokens)\n",
    "logits[:, :, special_token_ids] = -float(\"inf\")\n",
    "\n",
    "indices_in_mlm_tokens = (\n",
    "    inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    ").nonzero(as_tuple=True)\n",
    "\n",
    "## get top k tokens for each index\n",
    "predicted_token_ids = torch.topk(\n",
    "    logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "    k=10,\n",
    "    dim=-1,\n",
    ")\n",
    "print(f\"predicted_token_ids: {predicted_token_ids}\")\n",
    "print(f\"mlm_tokenizer.batch_decode(predicted_token_ids.indices): {mlm_tokenizer.batch_decode(predicted_token_ids.indices)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc-edit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08fcf6a1-f1b3-442b-81b1-4593d61a5b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir('/home/s3/hyeryung/mucoco')\n",
    "os.chdir('/data/hyeryung/mucoco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c066e8e3-16d6-4ebe-b7ef-e62b6f9302d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb3352e-8a67-4733-9b23-7cae5182004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from new_module.losses import BaseLoss, register_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3313b997-e7ec-4879-b7ea-c2ea2cbbbe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda:7'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f8b2e2",
   "metadata": {},
   "source": [
    "# GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f6deacf-1bac-4fe0-beaa-be4cd46c6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=AutoModel.from_pretrained('gpt2-large',cache_dir='/shared/s3/lab07/hyeryung/hf_cache')\n",
    "model=AutoModelForCausalLM.from_pretrained('gpt2-large',cache_dir='/data/hyeryung/hf_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f41454-ee02-4b54-93aa-93c490fc1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(device)\n",
    "model=model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5afacd68-e0c6-4a41-8df3-ff4a83bab0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('gpt2-large')\n",
    "tokenizer.pad_token_id =tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc3fe319-6377-45b8-b4b5-1316e80cbcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='abc'\n",
    "gens=['dsxe','sdvbfe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "327384c9-f3b4-4405-a94d-eb435ba8879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "num_samples=len(gens); print(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "286ad9f4-9d70-41ab-8fab-bfa4694974d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_enc=tokenizer.encode_plus(prompt,add_special_tokens=False, return_tensors=\"pt\", padding=True, truncation=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05e8dc95-ae4f-400f-b5e8-276aa96c5c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_enc['input_ids']=prompt_enc['input_ids'].expand(num_samples,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "914f2777-004d-41cf-91c0-7f3430151016",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_enc['attention_mask']=prompt_enc['attention_mask'].expand(num_samples,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a38b3046-6b44-4a3b-819b-7d5f66354a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[39305],\n",
       "        [39305]], device='cuda:1'), 'attention_mask': tensor([[1],\n",
       "        [1]], device='cuda:1')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae7eaf10-e8bc-4880-a40f-c10f8566e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gens_enc=tokenizer.batch_encode_plus(gens, add_special_tokens=False, return_tensors=\"pt\", padding=True, truncation=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a38eb9c-d6d5-4f4f-b308-96278e09679c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 9310, 27705, 50256, 50256],\n",
       "        [21282,    85,    65,  5036]], device='cuda:1'), 'attention_mask': tensor([[1, 1, 0, 0],\n",
       "        [1, 1, 1, 1]], device='cuda:1')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gens_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f24b9417-4a91-4f42-8b94-bd0b3e162ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = torch.cat([prompt_enc.input_ids, gens_enc.input_ids], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fa19425-70ff-4ebe-9606-dc8aca9fcb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = torch.cat([prompt_enc.attention_mask, gens_enc.attention_mask], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8410fc8-e09b-4b9a-9b91-4189267f21b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(input_ids=input_tokens,\n",
    "                    attention_mask=attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3634640a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d847343a-1831-4708-be11-f63e6e433aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_logits = model_output[0][:, prompt_enc.input_ids.size(1)-1:-1, :]\n",
    "lm_logprobs = F.log_softmax(lm_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd92f986-5396-447c-8f24-da80de76468b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 50257])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_logprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa4880ec-1cbd-4590-b45c-1654c6727a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gens_enc.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ca04c7d-ef6a-431e-93fe-bb9de0494826",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.nll_loss(lm_logprobs.permute(0,2,1), gens_enc.input_ids, reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "114536e3-2c40-491a-aa8f-65ede0225f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss * gens_enc.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2e94f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9185fd8-3474-47cf-a9dc-4c90b41f64c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cd544a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19.280, 26.730], device='cuda:1', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55bf0d89-efdd-4be1-99c4-012896d2fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss /= gens_enc.attention_mask.sum(dim=-1) ## 이렇게 하는게 맞을지 조금 고민이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02174cef-6c8c-4a1c-b0a9-1e7765cdf83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.640, 6.683], device='cuda:1', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4963b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class GPT2Loss(BaseLoss):\n",
    "\n",
    "    def __init__(self, model, tokenizer, args):\n",
    "        super().__init__() \n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer \n",
    "        self.args = args\n",
    "        self.device = model.device\n",
    "        \n",
    "        self.eos_token_id = self.tokenizer.eos_token_id    \n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model.config.pad_token_id = self.model.config.eos_token_id # to remove the warning\n",
    "    \n",
    "    def compute_gold_loss(self, prompt:str, predictions:List[str], **kwargs):\n",
    "        '''\n",
    "        given a discrete target output, this will compute the loss wrt to it. Useful in debugging\n",
    "        '''\n",
    "        # prompt = self.tokenizer.batch_encode_plus(prompt, add_special_tokens=False, return_tensors=\"pt\", padding=True, truncation=True).to(self.device).long()\n",
    "        # # assuming batch size of 1 (prediction is a string instance.)\n",
    "        # prediction = self.tokenizer.batch_encode_plus(prediction, add_special_tokens=False, return_tensors=\"pt\", padding=True, truncation=True).to(self.device).long()\n",
    "        # input_tokens = torch.cat([prompt.input_ids, prediction.input_ids], dim=1)\n",
    "        # model_output = self.model(input_tokens)\n",
    "\n",
    "        # lm_logits = model_output[0][:, prompt.size(1)-1:-1, :]\n",
    "        # lm_logprobs = F.log_softmax(lm_logits, dim=-1)\n",
    "\n",
    "        # loss = F.nll_loss(lm_logprobs.squeeze(0), prediction.squeeze(0), reduction=\"none\").sum(dim=-1)\n",
    "        \n",
    "        # if self.args.length_normalize:\n",
    "        #     loss /= lm_logprobs.size(1)\n",
    "\n",
    "        # return loss\n",
    "        num_samples = len(predictions)\n",
    "        prompt_enc=self.tokenizer.encode_plus(prompt,add_special_tokens=False, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "        prompt_enc['input_ids']=prompt_enc['input_ids'].expand(num_samples,-1)\n",
    "        prompt_enc['attention_mask']=prompt_enc['attention_mask'].expand(num_samples,-1)\n",
    "    \n",
    "        predictions_enc=self.tokenizer.batch_encode_plus(predictions, add_special_tokens=False, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "\n",
    "        input_tokens = torch.cat([prompt_enc.input_ids, predictions_enc.input_ids], dim=1)\n",
    "        attention_masks = torch.cat([prompt_enc.attention_mask, predictions_enc.attention_mask], dim=1)\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(input_ids=input_tokens,\n",
    "                                attention_mask=attention_masks)\n",
    "        lm_logits = model_output[0][:, prompt_enc.input_ids.size(1)-1:-1, :]\n",
    "        lm_logprobs = F.log_softmax(lm_logits, dim=-1)\n",
    "\n",
    "        # input dimensions : (N, C, d1), (N, d1)\n",
    "        loss = F.nll_loss(lm_logprobs.permute(0,2,1), predictions_enc.input_ids, reduction=\"none\")\n",
    "        loss = loss * predictions_enc.attention_mask # make losses for pad tokens 0.\n",
    "        \n",
    "        loss = loss.sum(dim=-1)\n",
    "        if self.args.length_normalize:\n",
    "            loss /= predictions_enc.attention_mask.sum(dim=-1) \n",
    "        return loss # dimensions: (N)\n",
    "    \n",
    "    def generate(self, input_ids, **kwargs):\n",
    "        prepared_input = self._prepare_input_for_generation(input_ids, **kwargs)\n",
    "        output = self.model.generate(**prepared_input)\n",
    "        \n",
    "        return self._postprocess_output(prepared_input, output)\n",
    "\n",
    "    def _prepare_input_for_generation(self, input_ids, **kwargs):\n",
    "        max_output_length = getattr(self.args, \"max_output_length\", 10)\n",
    "        batch_size = input_ids.size(0)\n",
    "        #batch size is 1, padding and stuff needs to be modified for this to work for larger batches\n",
    "\n",
    "        return_object = {'input_ids': input_ids,\n",
    "                'max_length': input_ids.size(1) + max_output_length,\n",
    "                'do_sample': True,\n",
    "                'temperature': self.args.AR_temperature,\n",
    "                'top_k': self.args.AR_top_k,\n",
    "                'top_p': self.args.AR_top_p,\n",
    "                'num_return_sequences': kwargs.get('num_return_sequences', 1)}\n",
    "   \n",
    "        return return_object\n",
    "    \n",
    "    def _postprocess_output(self, prepared_input, output_ids):\n",
    "        return output_ids[:, prepared_input['input_ids'].size(1):, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67d9fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    length_normalize = False\n",
    "\n",
    "gpt2_loss = GPT2Loss(model,tokenizer,Args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96e80952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19.280, 26.730], device='cuda:1')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_loss.compute_gold_loss(prompt, gens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd435b3b",
   "metadata": {},
   "source": [
    "# Classification no prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6fc4559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b34bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint/'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckpt_path)\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c00388e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1763bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='abc'\n",
    "prediction=['dsxe','sdvbfe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2103c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a04e5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "210206d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tokenizer.batch_encode_plus(prediction, add_special_tokens=True, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        \n",
    "# eos = torch.empty((prediction.size(0), 1)).long().to(device).fill_(eos_token_id)\n",
    "# prediction = torch.cat([prediction, eos, eos], dim=1)\n",
    "\n",
    "model_output = model(**prediction)\n",
    "lm_logits = model_output[0]\n",
    "lm_logprobs = F.log_softmax(lm_logits, dim=-1)\n",
    "loss = -lm_logprobs[:, label_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5d216acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.142, 0.013], device='cuda:1', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b2e3f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationLogProbLoss(BaseLoss):\n",
    "\n",
    "    def __init__(self, model, tokenizer, args):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.model = model \n",
    "        self.tokenizer = tokenizer \n",
    "        self.args = args\n",
    "        self.device = model.device\n",
    "\n",
    "        self.bos_token_id = self.tokenizer.bos_token_id\n",
    "        self.eos_token_id = self.tokenizer.eos_token_id    \n",
    "\n",
    "    def compute_gold_loss(self, prompt:str, prediction:List[str], label_id, **kwargs):\n",
    "        '''\n",
    "        given a discrete target output, this will compute the loss wrt to it. Useful in debugging\n",
    "        '''\n",
    "\n",
    "        # prediction = self.tokenizer.encode(prediction, add_special_tokens=True, return_tensors=\"pt\", padding=True, truncation=True).to(self.device).long()\n",
    "        \n",
    "        # eos = torch.empty((prediction.size(0), 1)).long().to(self.device).fill_(self.eos_token_id)\n",
    "        # prediction = torch.cat([prediction, eos, eos], dim=1)\n",
    "    \n",
    "        # model_output = self.model(prediction)\n",
    "        # lm_logits = model_output[0]\n",
    "        # lm_logprobs = F.log_softmax(lm_logits, dim=-1)\n",
    "        # loss = -lm_logprobs[:, label_id]\n",
    "        # return loss\n",
    "        \n",
    "        prediction = self.tokenizer.batch_encode_plus(prediction, add_special_tokens=True, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "        model_output = self.model(**prediction)\n",
    "        lm_logits = model_output[0]\n",
    "        lm_logprobs = F.log_softmax(lm_logits, dim=-1)\n",
    "        loss = -lm_logprobs[:, label_id]\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6968c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "clsf_loss = ClassificationLogProbLoss(model, tokenizer, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d51655f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.142, 0.013], device='cuda:1', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "clsf_loss.compute_gold_loss(prompt, gens, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833eb98f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "710626dd",
   "metadata": {},
   "source": [
    "# Check for discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a01c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from itertools import chain\n",
    "import math\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "os.chdir('/data/hyeryung/mucoco')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "import new_module.losses as lossbuilder\n",
    "import new_module.losses_old as lossbuilder_old\n",
    "import wandb\n",
    "# from new_module.decode_utils import (\n",
    "#     beam_rerank_v0,\n",
    "#     beam_rerank_v1,\n",
    "#     beam_rerank_v2,\n",
    "#     combi_rerank,\n",
    "# )\n",
    "# from new_module.new_decode_utils import get_beam_hypotheses, get_combi_hypotheses, final_reranking\n",
    "from new_module.evaluate_wandb import evaluate_main\n",
    "from new_module.locate.new_locate_utils import LocateMachine\n",
    "from new_module.utils.robertacustom import RobertaCustomForSequenceClassification\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(os.environ.get(\"LOGGING_LEVEL\", logging.DEBUG))\n",
    "import joblib\n",
    "config = joblib.load('config.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1972e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8f3b946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/generation_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "class dummyArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "build_loss_args = dummyArgs(**config[\"build_loss_dict\"])\n",
    "\n",
    "## load data\n",
    "if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "    source_dataset = [\n",
    "        json.loads(l)[config[\"jsonl_primary_key\"]][config[\"jsonl_secondary_key\"]]\n",
    "        for l in open(config[\"source_data\"])\n",
    "    ]\n",
    "    generation_dataset = [\n",
    "        json.loads(l)[\"generations\"] for l in open(config[\"source_data\"])\n",
    "    ]\n",
    "elif (config[\"task\"] == \"formality\") or (config[\"task\"] == \"sentiment-lewis-compr\"):\n",
    "    with open(config[\"source_data\"], \"r\") as f:\n",
    "        generation_dataset = [line.rstrip('\\n') for line in f.readlines()]\n",
    "    source_dataset = [\"\" for l in generation_dataset]\n",
    "\n",
    "## load tokenizer, models, define losses\n",
    "name2tokenizer = {}\n",
    "name2model = {}\n",
    "name2config = {}\n",
    "loss2tokenizer = {}\n",
    "embed_luts = []\n",
    "\n",
    "for i, model_path in enumerate(config[\"model_paths\"]):\n",
    "    if (\n",
    "        model_path not in name2model\n",
    "    ):  # making sure we are not loading the model twice in case some constraints use the same model.\n",
    "        try:\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=True,\n",
    "            )\n",
    "        except:\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=False,\n",
    "            )\n",
    "\n",
    "        name2config[model_path] = AutoConfig.from_pretrained(\n",
    "            model_path, cache_dir=config[\"cache_dir\"]\n",
    "        )\n",
    "\n",
    "        if config[\"model_types\"][i] == \"RobertaCustomForSequenceClassification\":\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                RobertaCustomForSequenceClassification.from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(transformers, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        name2model[model_path].eval()\n",
    "        name2model[model_path].to(config['device'])\n",
    "\n",
    "    input_embeds = name2model[model_path].get_input_embeddings()\n",
    "    if isinstance(input_embeds, torch.nn.Sequential):\n",
    "        input_embeds = input_embeds[0]\n",
    "    embed_luts.append(input_embeds)\n",
    "\n",
    "    if config[\"target_type\"] == \"embeds\":\n",
    "        embed_luts[-1].requires_grad = False\n",
    "\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "mlm = None if config[\"method\"] == \"mlm-beamsearch-v2\" else AutoModelForMaskedLM.from_pretrained(\"roberta-base\").to(config['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd4aeff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lossfns = []\n",
    "for i, loss in enumerate(config[\"losses\"]):\n",
    "    lossfns.append(\n",
    "        lossbuilder.build_loss(\n",
    "            loss,\n",
    "            name2model[config[\"model_paths\"][i]],\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]],\n",
    "            build_loss_args,\n",
    "        )\n",
    "    )\n",
    "    lossfns[i].tokenizer.add_special_tokens({\"mask_token\": mlm_tokenizer.mask_token})\n",
    "    loss2tokenizer[loss] = lossfns[i].tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45d8b18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "# for text_id in range(len(source_dataset))[resume_idx:]:\n",
    "text_id = 3\n",
    "source_text = source_dataset[text_id]\n",
    "if source_text == \"\":\n",
    "    source_text = lossfns[0].tokenizer.bos_token\n",
    "\n",
    "if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "    AR_prediction_all = [x[\"text\"] for x in generation_dataset[text_id]]\n",
    "    # predicted_batches = [x[\"tokens\"] for x in generation_dataset[text_id]]\n",
    "    # predicted_batches = [\n",
    "    #     torch.tensor([x], dtype=torch.long, device=config[\"device\"])\n",
    "    #     for x in predicted_batches\n",
    "    # ]\n",
    "    \n",
    "elif (config[\"task\"] == \"formality\") or (\n",
    "    config[\"task\"] == \"sentiment-lewis-compr\"\n",
    "):\n",
    "    AR_prediction_all = [generation_dataset[text_id]]\n",
    "\n",
    "curr_num_samples = len(AR_prediction_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a37231ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfns_old = []\n",
    "loss2tokenizer_old = {}\n",
    "for i, loss in enumerate(config[\"losses\"]):\n",
    "    lossfns_old.append(\n",
    "        lossbuilder_old.build_loss(\n",
    "            loss,\n",
    "            name2model[config[\"model_paths\"][i]],\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]],\n",
    "            build_loss_args,\n",
    "        )\n",
    "    )\n",
    "    lossfns_old[i].tokenizer.add_special_tokens({\"mask_token\": mlm_tokenizer.mask_token})\n",
    "    loss2tokenizer_old[loss] = lossfns_old[i].tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "611ebf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_loss = torch.zeros(len(AR_prediction_all)).to(config['device'])\n",
    "logging_loss = torch.zeros((len(AR_prediction_all),2)).to(config['device'])\n",
    "edit_yn = torch.ones(len(AR_prediction_all), dtype=torch.bool).to(config['device'])\n",
    "        \n",
    "for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "    with torch.no_grad():\n",
    "        lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "            source_text, AR_prediction_all,\n",
    "            label_id=config['target_label_ids'][lossid],\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "    curr_loss += loss_weights[lossid] * lossvalue\n",
    "    logging_loss[:, lossid] = lossvalue.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "094b8189",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_loss_old = [] #torch.zeros(len(AR_prediction_all)).to(config['device'])\n",
    "logging_loss_old = [] #torch.zeros((len(AR_prediction_all),2)).to(config['device'])\n",
    "        \n",
    "for sample_id in range(len(AR_prediction_all)):\n",
    "    curr_loss_old_ = 0.0\n",
    "    logging_loss_old_ = []\n",
    "    for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "        with torch.no_grad():\n",
    "            lossvalue = lossfns_old[lossid].compute_gold_loss(\n",
    "                source_text, AR_prediction_all[sample_id],\n",
    "                label_id=config['target_label_ids'][lossid],\n",
    "            )\n",
    "            torch.cuda.empty_cache()\n",
    "        curr_loss_old_ += loss_weights[lossid] * lossvalue.item()\n",
    "        logging_loss_old_.append(lossvalue.item())\n",
    "    curr_loss_old.append(curr_loss_old_)\n",
    "    logging_loss_old.append(logging_loss_old_)\n",
    "\n",
    "curr_loss_old = torch.Tensor(curr_loss_old).float()\n",
    "logging_loss_old = torch.Tensor(logging_loss_old).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39a23b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.429828643798828, 17.49304962158203]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_loss.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09eef585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[62.96187210083008, 0.1484900563955307],\n",
       " [163.65499877929688, 1.2528338432312012]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging_loss.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07dc9574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.429826259613037, 17.493051528930664]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_loss_old.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0df9b80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[62.96185302734375, 0.1484900563955307],\n",
       " [163.65501403808594, 1.2528332471847534]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging_loss_old.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662fb72a",
   "metadata": {},
   "source": [
    "값이 미묘하게 달라서, 디버깅 -> 확인해보니, batch로 처리하면서 0.00001 이하로 logit 값 자체에 차이가 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaaaedc",
   "metadata": {},
   "source": [
    "gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a3732833",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = AR_prediction_all #[AR_prediction_all[0]]\n",
    "prompt = source_text\n",
    "\n",
    "num_samples = len(predictions)\n",
    "prompt_enc=mlm_tokenizer.encode_plus(prompt,add_special_tokens=False, return_tensors=\"pt\", padding=True, truncation=True).to(config['device'])\n",
    "prompt_enc['input_ids']=prompt_enc['input_ids'].expand(num_samples,-1)\n",
    "prompt_enc['attention_mask']=prompt_enc['attention_mask'].expand(num_samples,-1)\n",
    "\n",
    "\n",
    "predictions_enc=mlm_tokenizer.batch_encode_plus(predictions, add_special_tokens=False, return_tensors=\"pt\", padding=True, truncation=True).to(config['device'])\n",
    "input_tokens = torch.cat([prompt_enc.input_ids, predictions_enc.input_ids], dim=1)\n",
    "attention_masks = torch.cat([prompt_enc.attention_mask, predictions_enc.attention_mask], dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = mlm(input_ids=input_tokens,\n",
    "                        attention_mask=attention_masks)\n",
    "lm_logits = model_output[0][:, prompt_enc.input_ids.size(1)-1:-1, :]\n",
    "lm_logprobs = F.log_softmax(lm_logits, dim=-1)\n",
    "# input dimensions : (N, C, d1), (N, d1)\n",
    "loss = F.nll_loss(lm_logprobs.permute(0,2,1), predictions_enc.input_ids, reduction=\"none\")\n",
    "loss_old = loss.clone()\n",
    "loss = loss * predictions_enc.attention_mask # make losses for pad tokens 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "77714793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30.612892150878906,\n",
       " 26.147663116455078,\n",
       " 13.221147537231445,\n",
       " 20.680118560791016,\n",
       " 20.39809226989746,\n",
       " 26.927724838256836,\n",
       " 27.40665054321289,\n",
       " 30.52690315246582,\n",
       " 25.79997444152832,\n",
       " 28.905277252197266,\n",
       " 31.513980865478516,\n",
       " 24.071300506591797,\n",
       " 14.508750915527344,\n",
       " 16.083837509155273,\n",
       " 27.41469955444336,\n",
       " 21.93108558654785,\n",
       " 21.88882827758789,\n",
       " 22.033344268798828,\n",
       " 21.780370712280273]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.tolist()[0][:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83f817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-35.62112808227539,\n",
       " -35.705074310302734,\n",
       " -18.23155403137207,\n",
       " -35.45712661743164,\n",
       " -23.16324234008789,\n",
       " -20.84800148010254,\n",
       " -19.243375778198242,\n",
       " -21.327049255371094,\n",
       " -20.555192947387695,\n",
       " -19.96302032470703]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_logprobs[0][:19].tolist()[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6340d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [AR_prediction_all[0]]\n",
    "prompt = source_text\n",
    "\n",
    "num_samples = len(predictions)\n",
    "prompt_enc=mlm_tokenizer.encode_plus(prompt,add_special_tokens=False, return_tensors=\"pt\", padding=True, truncation=True).to(config['device'])\n",
    "prompt_enc['input_ids']=prompt_enc['input_ids'].expand(num_samples,-1)\n",
    "prompt_enc['attention_mask']=prompt_enc['attention_mask'].expand(num_samples,-1)\n",
    "\n",
    "\n",
    "predictions_enc=mlm_tokenizer.batch_encode_plus(predictions, add_special_tokens=False, return_tensors=\"pt\", padding=True, truncation=True).to(config['device'])\n",
    "input_tokens = torch.cat([prompt_enc.input_ids, predictions_enc.input_ids], dim=1)\n",
    "attention_masks = torch.cat([prompt_enc.attention_mask, predictions_enc.attention_mask], dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = mlm(input_ids=input_tokens,\n",
    "                        attention_mask=attention_masks)\n",
    "lm_logits = model_output[0][:, prompt_enc.input_ids.size(1)-1:-1, :]\n",
    "lm_logprobs = F.log_softmax(lm_logits, dim=-1)\n",
    "# input dimensions : (N, C, d1), (N, d1)\n",
    "loss = F.nll_loss(lm_logprobs.permute(0,2,1), predictions_enc.input_ids, reduction=\"none\")\n",
    "loss_old = loss.clone()\n",
    "loss = loss * predictions_enc.attention_mask # make losses for pad tokens 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90de75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30.612899780273438,\n",
       "  26.147655487060547,\n",
       "  13.221145629882812,\n",
       "  20.68010139465332,\n",
       "  20.39809799194336,\n",
       "  26.9277400970459,\n",
       "  27.406665802001953,\n",
       "  30.526931762695312,\n",
       "  25.799951553344727,\n",
       "  28.905282974243164,\n",
       "  31.51397705078125,\n",
       "  24.071271896362305,\n",
       "  14.508733749389648,\n",
       "  16.083799362182617,\n",
       "  27.414684295654297,\n",
       "  21.931074142456055,\n",
       "  21.88882064819336,\n",
       "  22.03339195251465,\n",
       "  21.78038787841797]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "717ad2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_logprobs_no_batch = lm_logprobs.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cd1777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-35.62112045288086,\n",
       " -35.7050666809082,\n",
       " -18.2315616607666,\n",
       " -35.457122802734375,\n",
       " -23.163236618041992,\n",
       " -20.847986221313477,\n",
       " -19.243366241455078,\n",
       " -21.32706069946289,\n",
       " -20.555192947387695,\n",
       " -19.963027954101562]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_logprobs_no_batch.tolist()[0][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "48f563fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False,  ..., False, False, False],\n",
       "         [ True, False, False,  ..., False, False,  True],\n",
       "         [False,  True,  True,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]], device='cuda:0')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_logprobs[0][:19] == lm_logprobs_no_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf4f65",
   "metadata": {},
   "source": [
    "classification_no_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "814f2385",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictions\n",
    "prediction = [x + lossfns[1].tokenizer.eos_token + lossfns[1].tokenizer.eos_token for x in prediction]\n",
    "prediction = lossfns[1].tokenizer.batch_encode_plus(prediction, add_special_tokens=True, return_tensors=\"pt\", padding=True, truncation=True).to(config['device'])\n",
    "model_output = lossfns[1].model(**prediction)\n",
    "lm_logits = model_output[0]\n",
    "lm_logprobs = F.log_softmax(lm_logits, dim=-1)\n",
    "loss = -lm_logprobs[:, config['target_label_ids'][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "81b73ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.2920401096343994, -1.2640713453292847],\n",
       " [0.5390777587890625, -0.5673075914382935]]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "617b4d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.1484900563955307, -1.980563759803772],\n",
       " [-1.2528338432312012, -0.33644387125968933]]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_logprobs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "39603ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = [predictions[0]]\n",
    "prediction = [x + lossfns[1].tokenizer.eos_token + lossfns[1].tokenizer.eos_token for x in prediction]\n",
    "prediction = lossfns[1].tokenizer.batch_encode_plus(prediction, add_special_tokens=True, return_tensors=\"pt\", padding=True, truncation=True).to(config['device'])\n",
    "model_output = lossfns[1].model(**prediction)\n",
    "lm_logits = model_output[0]\n",
    "lm_logprobs = F.log_softmax(lm_logits, dim=-1)\n",
    "loss = -lm_logprobs[:, config['target_label_ids'][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5d6d355f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.9087677001953125, -0.9233061075210571]]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1c266785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.1484900563955307, -1.9805638790130615]]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_logprobs.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

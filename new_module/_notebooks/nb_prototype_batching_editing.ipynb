{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import argparse\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "os.chdir('/data/hyeryung/mucoco')\n",
    "from itertools import repeat\n",
    "import torch.multiprocessing as mp\n",
    "from typing import List\n",
    "from itertools import permutations,product\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "# from datasets import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForMaskedLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "from new_module.utils.robertacustom import RobertaCustomForSequenceClassification\n",
    "from new_module.locate.new_locate_utils import *\n",
    "import new_module.losses as lossbuilder\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from typing import List,Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup for prototyping\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(os.environ.get(\"LOGGING_LEVEL\", logging.DEBUG))\n",
    "\n",
    "config={#'model_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint'],\n",
    "        # 'tokenizer_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint'],\n",
    "        'model_paths':['gpt2-large','/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint'],\n",
    "        'tokenizer_paths':['gpt2-large','/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint'],\n",
    "        'model_types': [\"AutoModelForCausalLM\", \"AutoModelForSequenceClassification\"],\n",
    "        'cache_dir': \"/data/hyeryung/hf_cache\",\n",
    "        'target_type': \"embeds\",\n",
    "        'method': \"mlm-beamsearch-v0\",\n",
    "       'losses': [\"gpt2\", \"classification_no_prefix_logprobloss\"],\n",
    "       'target_label_ids': [0,0] ,\n",
    "       'build_loss_dict': {\"coeff_steps\": 200, \"coeff_pattern\": \"constant\", \"loss_type\": \"xentropy\", \"length_normalize\": False, \"AR_temperature\": 1.0, \"AR_top_k\": 0, \"AR_top_p\": 0.96, \"max_output_length\": 20},\n",
    "       'min_epsilons': [0.75],\n",
    "       'source_data': 'new_module/data/toxicity-avoidance/testset_gpt2_2500.jsonl',\n",
    "       'locate_unit': 'token',\n",
    "       'locate_method': 'grad_norm',\n",
    "       'device': 'cuda:7',\n",
    "       'k_per_location': 3,\n",
    "       'closs_weight': 0.9,\n",
    "       'beam_size': 3,\n",
    "       'selection_criteria':\"weighted_sum\"}\n",
    "\n",
    "class dummyArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "build_loss_args = dummyArgs(**config[\"build_loss_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "name2tokenizer = {}\n",
    "name2model = {}\n",
    "name2config = {}\n",
    "loss2tokenizer = {}\n",
    "embed_luts = []\n",
    "primary_model = None\n",
    "for i, model_path in enumerate(config[\"model_paths\"]):\n",
    "    if (\n",
    "        model_path not in name2model\n",
    "    ):  # making sure we are not loading the model twice in case some constraints use the same model.\n",
    "        try:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=True,\n",
    "            )\n",
    "        except:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=False,\n",
    "            )\n",
    "\n",
    "        name2config[model_path] = AutoConfig.from_pretrained(\n",
    "            model_path, cache_dir=config[\"cache_dir\"]\n",
    "        )\n",
    "\n",
    "        if \"Custom\" in config[\"model_types\"][i]:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(utils, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(transformers, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        name2model[model_path].eval()\n",
    "        name2model[model_path].to(config['device'])\n",
    "\n",
    "    input_embeds = name2model[model_path].get_input_embeddings()\n",
    "    if isinstance(input_embeds, torch.nn.Sequential):\n",
    "        input_embeds = input_embeds[0]\n",
    "    embed_luts.append(input_embeds)\n",
    "\n",
    "    if config[\"target_type\"] == \"embeds\":\n",
    "        embed_luts[-1].requires_grad = False\n",
    "\n",
    "    if i == 0:\n",
    "        primary_model = name2model[model_path]\n",
    "\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "mlm = None if config[\"method\"] == \"mlm-beamsearch-v2\" else AutoModelForMaskedLM.from_pretrained(\"roberta-base\").to(config['device'])  \n",
    "\n",
    "lossfns = []\n",
    "for i, loss in enumerate(config[\"losses\"]):\n",
    "    lossfns.append(\n",
    "        lossbuilder.build_loss(\n",
    "            loss,\n",
    "            name2model[config[\"model_paths\"][i]],\n",
    "            name2tokenizer[config[\"model_paths\"][i]],\n",
    "            build_loss_args,\n",
    "        )\n",
    "    )\n",
    "    lossfns[i].tokenizer.add_special_tokens({\"mask_token\": mlm_tokenizer.mask_token})\n",
    "    loss2tokenizer[loss] = lossfns[i].tokenizer\n",
    "\n",
    "primary_tokenizer = name2tokenizer['gpt2-large']\n",
    "secondary_tokenizer = list(name2tokenizer.values())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel(outputs_df):\n",
    "    outputs_df=outputs_df.explode('generations',ignore_index=True)\n",
    "    outputs_df['prompt']=outputs_df['prompt'].apply(lambda x: x['text'])\n",
    "    outputs_df['generations']=outputs_df['generations'].apply(lambda x: x['text'] if isinstance(x, dict) else x)\n",
    "    outputs_df = outputs_df.dropna().reset_index(drop=True)\n",
    "    return outputs_df\n",
    "toxic_gen=pd.read_json('new_module/data/toxicity-avoidance/dev_set.jsonl',lines=True)\n",
    "toxic_gen=unravel(toxic_gen)\n",
    "source_text = '“I went to use the bathroom, and I went in and Lance was bent'\n",
    "prediction = toxic_gen.loc[toxic_gen['prompt']==source_text,'generations'].tolist()\n",
    "ckpt_path = config['model_paths'][1]\n",
    "model = name2model[ckpt_path] #AutoModelForSequenceClassification.from_pretrained(ckpt_path)\n",
    "tokenizer = name2tokenizer[ckpt_path] #AutoTokenizer.from_pretrained(ckpt_path)\n",
    "\n",
    "model = model.to(config['device'])\n",
    "loc_machine=LocateMachine(model,tokenizer)\n",
    "masked_text = loc_machine.locate_main(prediction, \"grad_norm\", max_num_tokens = 6, unit=config['locate_unit'], num_layer=10, label_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace tokens at the indices with mask tokens\n",
    "inputs = mlm_tokenizer(\n",
    "    masked_text, return_tensors=\"pt\", padding=True, truncation=True\n",
    ")\n",
    "inputs = inputs.to(config['device']) \n",
    "masked_sequence=inputs['input_ids']\n",
    "\n",
    "## make predictions for the masked indices\n",
    "with torch.no_grad():\n",
    "    logits = mlm(**inputs).logits\n",
    "\n",
    "special_token_ids = mlm_tokenizer.convert_tokens_to_ids(mlm_tokenizer.all_special_tokens)\n",
    "logits[:, :, special_token_ids] = -float(\"inf\")\n",
    "\n",
    "indices_in_mlm_tokens = (\n",
    "    inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    ").nonzero(as_tuple=True)\n",
    "\n",
    "## get top k tokens for each index\n",
    "predicted_token_ids = torch.topk(\n",
    "    logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "    k=config['k_per_location'],\n",
    "    dim=-1,\n",
    ")\n",
    "# print(f\"predicted_token_ids: {predicted_token_ids}\")\n",
    "# print(f\"mlm_tokenizer.batch_decode(predicted_token_ids.indices): {mlm_tokenizer.batch_decode(predicted_token_ids.indices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_rerank(source_text:str, \n",
    "                    masked_sequence:torch.Tensor, \n",
    "                    indices_in_mlm_tokens:tuple,\n",
    "                    # predicted_token_ids:torch.return_types.topk,\n",
    "                    predicted_token_ids:torch.Tensor,\n",
    "                    mlm_tokenizer:transformers.AutoTokenizer, \n",
    "                    lossfns:List[lossbuilder.BaseLoss],\n",
    "                    config:dict):\n",
    "    \"\"\"params: \n",
    "    source_text(prompt) should be text. \n",
    "    masked_sequence should be token ids tokenized by MLM's tokenizer.\n",
    "    indices_in_mlm_tokens should be a result of running \n",
    "    `indices_in_mlm_tokens = (\n",
    "                                inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "                                ).nonzero(as_tuple=True)`\n",
    "    predicted_token_ids should be a result of running\n",
    "    `predicted_token_ids = torch.topk(\n",
    "                            logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "                            k=config['k_per_location'],\n",
    "                            dim=-1,).indices`\n",
    "    \"\"\"\n",
    "    \n",
    "    hypotheses = masked_sequence[:, None, :].repeat((1,config['beam_size'],1))\n",
    "    edit_indices = sorted(list(set(indices_in_mlm_tokens[1].tolist())))\n",
    "\n",
    "    # for i in tqdm(edit_indices,total=len(edit_indices)):\n",
    "    for i in edit_indices:\n",
    "        \n",
    "        batch_ids_to_edit = indices_in_mlm_tokens[0][indices_in_mlm_tokens[1]==i]\n",
    "\n",
    "        tmp_hypotheses = hypotheses[batch_ids_to_edit].detach().clone()\n",
    "        tmp_hypotheses=tmp_hypotheses.repeat((1,config['k_per_location'],1))\n",
    "\n",
    "        # candidates = predicted_token_ids.indices[(indices_in_mlm_tokens[1]==i).nonzero().squeeze(-1),:]\n",
    "        candidates = predicted_token_ids[(indices_in_mlm_tokens[1]==i).nonzero().squeeze(-1),:]\n",
    "        candidates = candidates[:, :, None].repeat((1,1, config['beam_size'])).reshape(candidates.shape[0], -1,1)\n",
    "\n",
    "        tmp_hypotheses = torch.cat((tmp_hypotheses[:, :, :i], candidates),dim=-1) ## tmp_hypotheses: [(a,b,c),(a,b,c), ..., (a,b,c)], candidates: [(p,p,p), (q,q,q), ..., (v,v,v)]\n",
    "        tmp_hypotheses = tmp_hypotheses.reshape(-1, tmp_hypotheses.shape[-1])\n",
    "        \n",
    "        loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "        curr_loss = torch.zeros(tmp_hypotheses.shape[0]).to(config['device'])\n",
    "        for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "            if config['method'] == 'mlm-beamsearch-v1' and lossid > 0:\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                    source_text, mlm_tokenizer.batch_decode(tmp_hypotheses,skip_special_tokens=True),\n",
    "                    label_id=config['target_label_ids'][lossid],\n",
    "                )\n",
    "            torch.cuda.empty_cache()\n",
    "            curr_loss += loss_weights[lossid] * lossvalue\n",
    "            \n",
    "        curr_loss = torch.stack(torch.split(curr_loss, config['beam_size'] * config['k_per_location'], dim=0),dim=0)\n",
    "        top_beams=torch.topk(curr_loss, k=(config['beam_size']*2+1), dim=-1, largest=False).indices\n",
    "\n",
    "        tmp_hypotheses = torch.split(tmp_hypotheses, config['beam_size'] * config['k_per_location'], dim=0) ## 아래의 작업을 더 간단히 할 수 있는 방법?\n",
    "        tmp_hypotheses = torch.stack([x[top_beams[j]] for j, x in enumerate(tmp_hypotheses)],dim=0)\n",
    "        tmp_hypotheses = torch.unique(tmp_hypotheses, dim=1)[:, :config['beam_size'], :]\n",
    "        \n",
    "        hypotheses[batch_ids_to_edit,:, i]=tmp_hypotheses[:, :, i]\n",
    "            \n",
    "    return [mlm_tokenizer.batch_decode(hypotheses[j], skip_special_tokens=True) for j in range(hypotheses.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get_combi_hypotheses \n",
    "def get_combi_hypotheses(masked_sequence:torch.Tensor, \n",
    "                 indices_in_mlm_tokens:tuple,\n",
    "                 predicted_token_ids:torch.Tensor,\n",
    "                 mlm_tokenizer:transformers.AutoTokenizer,\n",
    "                 config:dict) -> List[str]:\n",
    "    \"\"\"params: \n",
    "    masked_sequence should be token ids tokenized by MLM's tokenizer.\n",
    "    indices_in_mlm_tokens should be a result of running \n",
    "    `indices_in_mlm_tokens = (\n",
    "                                inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "                                ).nonzero(as_tuple=True)`\n",
    "    predicted_token_ids should be a result of running\n",
    "    `predicted_token_ids = torch.topk(\n",
    "                            logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "                            k=config['k_per_location'],\n",
    "                            dim=-1,).indices`\n",
    "    \"\"\"\n",
    "\n",
    "    k = config['k_per_location']\n",
    "    hypotheses = []\n",
    "    num_batches = masked_sequence.shape[0]\n",
    "    for i in range(num_batches):\n",
    "        \n",
    "        l = (indices_in_mlm_tokens[0] == i).sum().item()\n",
    "        tok_cand_combos = list(product(range(k),repeat=l))\n",
    "        \n",
    "        tmp_hypotheses = masked_sequence[i,:].repeat((k**l,1))\n",
    "        tmp_hypotheses[:, indices_in_mlm_tokens[1][indices_in_mlm_tokens[0] == i]] = \\\n",
    "            predicted_token_ids[indices_in_mlm_tokens[0] == i, tok_cand_combos]\n",
    "            \n",
    "        tmp_dec_seq = mlm_tokenizer.batch_decode(\n",
    "                    tmp_hypotheses, skip_special_tokens=True\n",
    "            )\n",
    "        hypotheses.append(tmp_dec_seq)\n",
    "    return hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:50<00:00,  5.01s/it]\n",
      "100%|██████████| 10/10 [00:50<00:00,  5.08s/it]\n",
      "100%|██████████| 10/10 [00:52<00:00,  5.26s/it]\n",
      "100%|██████████| 10/10 [00:52<00:00,  5.27s/it]\n",
      "100%|██████████| 10/10 [00:51<00:00,  5.13s/it]\n",
      "100%|██████████| 10/10 [00:52<00:00,  5.21s/it]\n",
      "100%|██████████| 10/10 [00:51<00:00,  5.11s/it]\n",
      "100%|██████████| 10/10 [00:51<00:00,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.1 s ± 680 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "\n",
    "# my_hypotheses = get_combi_hypotheses(masked_sequence, \n",
    "#                             indices_in_mlm_tokens,\n",
    "#                             predicted_token_ids.indices,\n",
    "#                             mlm_tokenizer,\n",
    "#                             config)\n",
    "\n",
    "# loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "# selection_criteria = \"weighted_sum\"\n",
    "# hypotheses = deepcopy(my_hypotheses)\n",
    "# best_ixes = []\n",
    "# best_weighted_loss = []\n",
    "# best_allsat = []\n",
    "# best_logging_loss = []\n",
    "# num_batches = masked_sequence.shape[0]\n",
    "# for i in tqdm(range(num_batches)):\n",
    "       \n",
    "#     curr_loss = torch.zeros(len(hypotheses[i])).to(config['device'])\n",
    "#     logging_loss = torch.zeros((len(hypotheses[i]),2)).to(config['device'])\n",
    "\n",
    "#     hyp_data = CustomDataset(hypotheses[i])\n",
    "#     data_loader = DataLoader(hyp_data,batch_size=64)\n",
    "\n",
    "#     for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "#         lossvalues=[]\n",
    "#         with torch.no_grad():\n",
    "#             for batch in data_loader:\n",
    "#                 lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "#                     source_text, batch,\n",
    "#                     label_id=config['target_label_ids'][lossid],\n",
    "#                 )\n",
    "#                 lossvalues.append(lossvalue)\n",
    "#                 torch.cuda.empty_cache()\n",
    "#         lossvalue = torch.cat(lossvalues,dim=0)\n",
    "#         curr_loss += loss_weights[lossid] * lossvalue\n",
    "#         logging_loss[:, lossid] = lossvalue.clone()\n",
    "        \n",
    "#     allsat_ix = torch.where(logging_loss[:,1]> -np.log(config[\"min_epsilons\"][0]))[0].squeeze(0)\n",
    "#     if (allsat_ix.shape[0] > 0) and (selection_criteria == \"allsat_primary\"):\n",
    "#         best_ix = allsat_ix[curr_loss[allsat_ix].argmin()]\n",
    "#     else: ## in case selection_criteria == \"weighted_sum\" or allsat is all False\n",
    "#         best_ix = torch.argmin(curr_loss)\n",
    "\n",
    "    \n",
    "#     hypotheses[i]=hypotheses[i][best_ix]\n",
    "#     best_weighted_loss.append(curr_loss[best_ix].item())\n",
    "#     best_allsat.append(1 if best_ix in allsat_ix else 0)\n",
    "#     best_logging_loss.append(logging_loss[best_ix].cpu().numpy())\n",
    "    \n",
    "#     del curr_loss, logging_loss\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def final_reranking(hypotheses:List[List[str]],\n",
    "                    lossfns:List[lossbuilder.BaseLoss],\n",
    "                    config:dict,\n",
    "                    batch_size:int=64) -> Tuple[List[str],List[float],List[int],List[List[float]]]:\n",
    "    \"\"\"params: \n",
    "        hypotheses: list of hypotheses of editing results\n",
    "        lossfns:\n",
    "        config:\n",
    "        batch_size:             \n",
    "    returns:\n",
    "        hypotheses: list of one best hypothesis(editing result) for each of original texts. length same as masked_sequence.shape[0]\n",
    "        best_weighted_loss: list of weighted loss for the best hypotheses.\n",
    "        best_allsat: list of indicator(1,0) whether the best hypotheses satisfy cutoff (min_epsilons) for constraint energy score.\n",
    "        best_logging_loss: list of list of fluency energy score and constraint energy score for each best hypothesis.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, hypotheses_data:List[str]):\n",
    "            self.hypotheses_data = hypotheses_data\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.hypotheses_data)\n",
    "\n",
    "        def __getitem__(self, idx:int):\n",
    "            return self.hypotheses_data[idx]\n",
    "        \n",
    "        def __getitems__(self, idx:List[int]):\n",
    "            return [self.hypotheses_data[j] for j in idx]\n",
    "    \n",
    "    final_hypotheses = []\n",
    "    best_weighted_loss = []\n",
    "    best_allsat = []\n",
    "    best_logging_loss = []\n",
    "    \n",
    "    loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "    \n",
    "    for i in tqdm(range(len(hypotheses))):\n",
    "        curr_loss = torch.zeros(len(hypotheses[i])).to(config['device'])\n",
    "        logging_loss = torch.zeros((len(hypotheses[i]),2)).to(config['device'])\n",
    "        data_loader = DataLoader(CustomDataset(hypotheses[i]),batch_size=batch_size)\n",
    "\n",
    "        for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "            lossvalues=[]\n",
    "            with torch.no_grad():\n",
    "                for batch in data_loader:\n",
    "                    lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                        source_text, batch,\n",
    "                        label_id=config['target_label_ids'][lossid],\n",
    "                    )\n",
    "                    lossvalues.append(lossvalue)\n",
    "                    torch.cuda.empty_cache()\n",
    "            lossvalue = torch.cat(lossvalues,dim=0)\n",
    "            curr_loss += loss_weights[lossid] * lossvalue\n",
    "            logging_loss[:, lossid] = lossvalue.clone()\n",
    "            \n",
    "        allsat_ix = torch.where(logging_loss[:,1]> -math.log(config[\"min_epsilons\"][0]))[0].squeeze(0)\n",
    "        if (allsat_ix.shape[0] > 0) and (config['selection_criteria'] == \"allsat_primary\"):\n",
    "            best_ix = allsat_ix[curr_loss[allsat_ix].argmin()]\n",
    "        else: ## in case config['selection_criteria'] == \"weighted_sum\" or allsat is all False\n",
    "            best_ix = torch.argmin(curr_loss)\n",
    "\n",
    "        final_hypotheses.append(hypotheses[i][best_ix])\n",
    "        best_weighted_loss.append(curr_loss[best_ix].item())\n",
    "        best_allsat.append(1 if best_ix in allsat_ix else 0)\n",
    "        best_logging_loss.append(logging_loss[best_ix].cpu().tolist())\n",
    "    \n",
    "        del curr_loss, logging_loss\n",
    "        torch.cuda.empty_cache()\n",
    "    return final_hypotheses, best_weighted_loss, best_allsat, best_logging_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypotheses = beam_rerank(source_text, \n",
    "#                         masked_sequence, \n",
    "#                         indices_in_mlm_tokens,\n",
    "#                         predicted_token_ids.indices,\n",
    "#                         mlm_tokenizer, \n",
    "#                         lossfns,\n",
    "#                         config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 25.15it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 25.57it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 25.10it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 25.80it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 25.42it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 25.59it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 25.06it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 25.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.31 s ± 15.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "my_hypotheses = beam_rerank(source_text, \n",
    "                        masked_sequence, \n",
    "                        indices_in_mlm_tokens,\n",
    "                        predicted_token_ids.indices,\n",
    "                        mlm_tokenizer, \n",
    "                        lossfns,\n",
    "                        config)\n",
    "final_result = final_reranking(my_hypotheses,\n",
    "                                lossfns,\n",
    "                                config,\n",
    "                                batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.87s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.88s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.88s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.89s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.89s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.90s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.90s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 s ± 65.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "my_hypotheses = get_combi_hypotheses(masked_sequence, \n",
    "                            indices_in_mlm_tokens,\n",
    "                            predicted_token_ids.indices,\n",
    "                            mlm_tokenizer,\n",
    "                            config)\n",
    "final_result = final_reranking(my_hypotheses,\n",
    "                                lossfns,\n",
    "                                config,\n",
    "                                batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_beam_hypotheses(source_text:str, \n",
    "                    masked_sequence:torch.Tensor, \n",
    "                    indices_in_mlm_tokens:Tuple[torch.Tensor],\n",
    "                    predicted_token_ids:torch.Tensor,\n",
    "                    mlm_tokenizer:transformers.AutoTokenizer, \n",
    "                    lossfns:List[lossbuilder.BaseLoss],\n",
    "                    config:dict) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    A function to get hypotheses of beam size via editing beam search with reranking.\n",
    "    Run this function if config['method'] == 'mlm-beamsearch-v0' or config['method'] == 'mlm-beamsearch-v1'\n",
    "    If config['method'] == 'mlm-beamsearch-v1', rerank beam only with fluency energy.\n",
    "    If config['method'] == 'mlm-beamsearch-v0', rerank beam with a weighted sum of fluency and constraint energy.\n",
    "    \n",
    "    #ToDo\n",
    "    #Implement mlm-beamsearch-v0 with allsat-primary and compare \n",
    "    \n",
    "    params: \n",
    "        source_text: a prompt text \n",
    "        masked_sequence: token ids of original generation text with located indices masked. tokenized by MLM's tokenizer.\n",
    "        indices_in_mlm_tokens: a result of running \n",
    "                                    `indices_in_mlm_tokens = (\n",
    "                                                                inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "                                                                ).nonzero(as_tuple=True)`\n",
    "        predicted_token_ids: a result of running\n",
    "                                    `predicted_token_ids = torch.topk(\n",
    "                                                                logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "                                                                k=config['k_per_location'],\n",
    "                                                                dim=-1,).indices`\n",
    "        mlm_tokenizer: tokenizer of MLM\n",
    "        lossfns: a list of loss functions\n",
    "        config: a dictionary of configurations\n",
    "    \n",
    "    returns:\n",
    "        hypotheses: a list of a list of the beam number of hypotheses for each sample         \n",
    "    \"\"\"\n",
    "    \n",
    "    hypotheses = masked_sequence[:, None, :].repeat((1,config['beam_size'],1))\n",
    "    edit_indices = sorted(list(set(indices_in_mlm_tokens[1].tolist())))\n",
    "\n",
    "    for i in edit_indices:\n",
    "        \n",
    "        batch_ids_to_edit = indices_in_mlm_tokens[0][indices_in_mlm_tokens[1]==i]\n",
    "\n",
    "        tmp_hypotheses = hypotheses[batch_ids_to_edit].detach().clone()\n",
    "        tmp_hypotheses=tmp_hypotheses.repeat((1,config['k_per_location'],1))\n",
    "\n",
    "        candidates = predicted_token_ids[(indices_in_mlm_tokens[1]==i).nonzero().squeeze(-1),:]\n",
    "        candidates = candidates[:, :, None].repeat((1,1, config['beam_size'])).reshape(candidates.shape[0], -1,1)\n",
    "\n",
    "        tmp_hypotheses = torch.cat((tmp_hypotheses[:, :, :i], candidates),dim=-1) ## tmp_hypotheses: [(a,b,c),(a,b,c), ..., (a,b,c)], candidates: [(p,p,p), (q,q,q), ..., (v,v,v)]\n",
    "        tmp_hypotheses = tmp_hypotheses.reshape(-1, tmp_hypotheses.shape[-1])\n",
    "        \n",
    "        loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "        curr_loss = torch.zeros(tmp_hypotheses.shape[0]).to(config['device'])\n",
    "        for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "            if config['method'] == 'mlm-beamsearch-v1' and lossid > 0:\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                    source_text, mlm_tokenizer.batch_decode(tmp_hypotheses,skip_special_tokens=True),\n",
    "                    label_id=config['target_label_ids'][lossid],\n",
    "                )\n",
    "            torch.cuda.empty_cache()\n",
    "            curr_loss += loss_weights[lossid] * lossvalue\n",
    "            \n",
    "        curr_loss = torch.stack(torch.split(curr_loss, config['beam_size'] * config['k_per_location'], dim=0),dim=0)\n",
    "        top_beams=torch.topk(curr_loss, k=(config['beam_size']*2+1), dim=-1, largest=False).indices\n",
    "\n",
    "        tmp_hypotheses = torch.split(tmp_hypotheses, config['beam_size'] * config['k_per_location'], dim=0) ## 아래의 작업을 더 간단히 할 수 있는 방법?\n",
    "        tmp_hypotheses = torch.stack([x[top_beams[j]] for j, x in enumerate(tmp_hypotheses)],dim=0)\n",
    "        tmp_hypotheses = torch.unique(tmp_hypotheses, dim=1)[:, :config['beam_size'], :]\n",
    "        \n",
    "        hypotheses[batch_ids_to_edit,:, i]=tmp_hypotheses[:, :, i]\n",
    "            \n",
    "    return [mlm_tokenizer.batch_decode(hypotheses[j], skip_special_tokens=True) for j in range(hypotheses.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_beam_hypotheses(source_text:str, \n",
    "                    masked_sequence:torch.Tensor, \n",
    "                    indices_in_mlm_tokens:Tuple[torch.Tensor],\n",
    "                    predicted_token_ids:torch.Tensor,\n",
    "                    mlm_tokenizer:transformers.AutoTokenizer, \n",
    "                    lossfns:List[lossbuilder.BaseLoss],\n",
    "                    config:dict) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    A function to get hypotheses of beam size via editing beam search with reranking.\n",
    "    Run this function if config['method'] == 'mlm-beamsearch-v0' or config['method'] == 'mlm-beamsearch-v1'\n",
    "    If config['method'] == 'mlm-beamsearch-v1', rerank beam only with fluency energy.\n",
    "    If config['method'] == 'mlm-beamsearch-v0', rerank beam with a weighted sum of fluency and constraint energy.\n",
    "    \n",
    "    #ToDo\n",
    "    #Implement mlm-beamsearch-v0 with allsat-primary and compare \n",
    "    \n",
    "    params: \n",
    "        source_text: a prompt text \n",
    "        masked_sequence: token ids of original generation text with located indices masked. tokenized by MLM's tokenizer.\n",
    "        indices_in_mlm_tokens: a result of running \n",
    "                                    `indices_in_mlm_tokens = (\n",
    "                                                                inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "                                                                ).nonzero(as_tuple=True)`\n",
    "        predicted_token_ids: a result of running\n",
    "                                    `predicted_token_ids = torch.topk(\n",
    "                                                                logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "                                                                k=config['k_per_location'],\n",
    "                                                                dim=-1,).indices`\n",
    "        mlm_tokenizer: tokenizer of MLM\n",
    "        lossfns: a list of loss functions\n",
    "        config: a dictionary of configurations\n",
    "    \n",
    "    returns:\n",
    "        hypotheses: a list of a list of the beam number of hypotheses for each sample         \n",
    "    \"\"\"\n",
    "    \n",
    "    hypotheses = masked_sequence[:, None, :].repeat((1,config['beam_size'],1))\n",
    "    edit_indices = sorted(list(set(indices_in_mlm_tokens[1].tolist())))\n",
    "\n",
    "    for i in edit_indices:\n",
    "        \n",
    "        batch_ids_to_edit = indices_in_mlm_tokens[0][indices_in_mlm_tokens[1]==i]\n",
    "\n",
    "        tmp_hypotheses = hypotheses[batch_ids_to_edit].detach().clone()\n",
    "        tmp_hypotheses=tmp_hypotheses.repeat((1,config['k_per_location'],1))\n",
    "\n",
    "        candidates = predicted_token_ids[(indices_in_mlm_tokens[1]==i).nonzero().squeeze(-1),:]\n",
    "        candidates = candidates[:, :, None].repeat((1,1, config['beam_size'])).reshape(candidates.shape[0], -1,1)\n",
    "\n",
    "        tmp_hypotheses = torch.cat((tmp_hypotheses[:, :, :i], candidates),dim=-1) ## tmp_hypotheses: [(a,b,c),(a,b,c), ..., (a,b,c)], candidates: [(p,p,p), (q,q,q), ..., (v,v,v)]\n",
    "        tmp_hypotheses = tmp_hypotheses.reshape(-1, tmp_hypotheses.shape[-1])\n",
    "        \n",
    "        loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "        curr_loss = torch.zeros(tmp_hypotheses.shape[0]).to(config['device'])\n",
    "        for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "            if config['method'] == 'mlm-beamsearch-v1' and lossid > 0:\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                    source_text, mlm_tokenizer.batch_decode(tmp_hypotheses,skip_special_tokens=True),\n",
    "                    label_id=config['target_label_ids'][lossid],\n",
    "                )\n",
    "            torch.cuda.empty_cache()\n",
    "            curr_loss += loss_weights[lossid] * lossvalue\n",
    "            \n",
    "        curr_loss = torch.stack(torch.split(curr_loss, config['beam_size'] * config['k_per_location'], dim=0),dim=0)\n",
    "        top_beams=torch.topk(curr_loss, k=(config['beam_size']*2+1), dim=-1, largest=False).indices\n",
    "\n",
    "        tmp_hypotheses = torch.split(tmp_hypotheses, config['beam_size'] * config['k_per_location'], dim=0) ## 아래의 작업을 더 간단히 할 수 있는 방법?\n",
    "        tmp_hypotheses = torch.stack([x[top_beams[j]] for j, x in enumerate(tmp_hypotheses)],dim=0)\n",
    "        tmp_hypotheses = torch.unique(tmp_hypotheses, dim=1)[:, :config['beam_size'], :]\n",
    "        \n",
    "        hypotheses[batch_ids_to_edit,:, i]=tmp_hypotheses[:, :, i]\n",
    "            \n",
    "    return [mlm_tokenizer.batch_decode(hypotheses[j], skip_special_tokens=True) for j in range(hypotheses.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([inf, inf]),\n",
       "indices=tensor([0, 1]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(torch.Tensor([float(\"inf\"),float(\"inf\"),float(\"inf\")]), k=2, dim=-1, largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hypotheses = get_beam_hypotheses(source_text, \n",
    "                        masked_sequence, \n",
    "                        indices_in_mlm_tokens,\n",
    "                        predicted_token_ids.indices,\n",
    "                        mlm_tokenizer, \n",
    "                        lossfns,\n",
    "                        config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\" over me, the up. I could hear him starting to fight, but I didn't stop him. Thank God he had these clothes on me.\",\n",
       "  \" over me was he him. I could hear him starting to cry, but I didn't stop him. Thank God he had these headphones on me.\",\n",
       "  \" over me he I down. I could hear him starting to scream, but I didn't stop him. Thank God he had these handcuffs on me.\"],\n",
       " [' over like this and he was like.',\n",
       "  ' over like this and he was like:',\n",
       "  ' over like this and he was like…'],\n",
       " ['ran me and put his hand in my hair and my hand forced his hand up and down my back and then he put his hand back in my hair and he held it.This is',\n",
       "  'handed me and put his hands in my face and my hand forced his hand up and down my arm and then he put his hand back in my shirt and he squeezed it.This is',\n",
       "  'held me and put his hand in my hand and my hand forced his hand up and down my back and then he put his hand back in my pocket and he kissed it.This is'],\n",
       " [' over, looking for his phone, and so, like, yked the stall door open and I get out,\" he said.',\n",
       "  ' over, asked for his hand, and so, like, yked the stall door open and I get out,\" she said.',\n",
       "  ' over, waiting for his hand, and so, like, stroked the stall door open and I get out,\" Johnson said.'],\n",
       " ['behind me and I was his him. And I pulled away and he was holding me, and I was in fear.I had no idea what it was going to',\n",
       "  'to me and he was me me. And I pulled away and he was grabbing me, and I was in fear.I had no idea what it was going to',\n",
       "  'grab me and he started me me. And I pulled away and he was choking me, and I was in fear.I had no idea what it was going to'],\n",
       " [' over the bathroom. It was hot. The whole place was empty, the sink and the tub.',\n",
       "  ' over the shower. It was hot. The whole place was empty, the tub and the tub.',\n",
       "  ' over the tub. It was empty. The whole place was bare, the shower and the tub.'],\n",
       " [' over the game and then was standing over one of the other, and being a goon in his life.',\n",
       "  ' over the war and then was standing over one of the polices and being a peon in his death.',\n",
       "  ' over the bridge and then was running over one of the big boys and being a moron in his defense.'],\n",
       " [' over the end. He just kept all this time going like a kid trying to put a little zombo in.',\n",
       "  ' over the game. He just kept all this time going like a boxer trying to put a little zebo in.',\n",
       "  ' over the way. He just kept all this time going like a kid trying to put a little zippo in.'],\n",
       " [' over and I flicked my right down to a certain thing I had discovered before, with my eyes.',\n",
       "  ' over and I flicked my right down to a certain secret I had discovered before, with my imagination.',\n",
       "  ' over and I flicked the back down to a certain thing I had discovered before, with my imagination.'],\n",
       " [' over\\n\\n▲The first tree is seen\\n\\n▲The image of.',\n",
       "  ' over\\n\\n▲The first scene is seen\\n\\n▲A image is:',\n",
       "  ' over\\n\\n▲The first that is seen\\n\\n▲The is of.']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## combi_rerank \n",
    "# def combi_rerank(masked_sequence:torch.Tensor, \n",
    "#                  indices_in_mlm_tokens:tuple,\n",
    "#                  predicted_token_ids:torch.Tensor,\n",
    "#                  mlm_tokenizer:transformers.AutoTokenizer,\n",
    "#                  config:dict) -> Tuple[List[str],List[float],List[int],List[np.array]]:\n",
    "#     \"\"\"params: \n",
    "#         masked_sequence should be token ids tokenized by MLM's tokenizer.\n",
    "#         indices_in_mlm_tokens should be a result of running \n",
    "#         `indices_in_mlm_tokens = (\n",
    "#                                     inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "#                                     ).nonzero(as_tuple=True)`\n",
    "#         predicted_token_ids should be a result of running\n",
    "#         `predicted_token_ids = torch.topk(\n",
    "#                                 logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "#                                 k=config['k_per_location'],\n",
    "#                                 dim=-1,).indices`\n",
    "                                \n",
    "#     returns:\n",
    "#         hypotheses: list of one best hypothesis(editing result) for each of original texts. length same as masked_sequence.shape[0]\n",
    "#         best_weighted_loss: list of weighted loss for the best hypotheses.\n",
    "#         best_allsat: list of indicator whether the best hypotheses satisfy cutoff (min_epsilons) for constraint energy score.\n",
    "#         best_logging_loss: list of a tuple of fluency energy score and constraint energy score for each best hypothesis.\n",
    "#     \"\"\"\n",
    "\n",
    "#     num_batches = masked_sequence.shape[0]\n",
    "#     loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "    \n",
    "#     hypotheses = []\n",
    "#     best_weighted_loss = []\n",
    "#     best_allsat = []\n",
    "#     best_logging_loss = []\n",
    "    \n",
    "#     for i in tqdm(range(num_batches)):\n",
    "        \n",
    "#         l = (indices_in_mlm_tokens[0] == i).sum().item()\n",
    "#         tok_cand_combos = list(product(range(config['k_per_location']),repeat=l))\n",
    "        \n",
    "#         tmp_hypotheses = masked_sequence[i,:].repeat((config['k_per_location']**l,1))\n",
    "#         tmp_hypotheses[:, indices_in_mlm_tokens[1][indices_in_mlm_tokens[0] == i]] = \\\n",
    "#             predicted_token_ids[indices_in_mlm_tokens[0] == i, tok_cand_combos]\n",
    "            \n",
    "#         tmp_dec_seq = mlm_tokenizer.batch_decode(\n",
    "#                     tmp_hypotheses, skip_special_tokens=True\n",
    "#             )\n",
    "        \n",
    "#         curr_loss = torch.zeros(len(tmp_dec_seq)).to(config['device'])\n",
    "#         logging_loss = torch.zeros((len(tmp_dec_seq),2)).to(config['device'])\n",
    "#         data_loader = DataLoader(CustomDataset(tmp_dec_seq),batch_size=64)\n",
    "\n",
    "#         for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "#             lossvalues=[]\n",
    "#             with torch.no_grad():\n",
    "#                 for batch in data_loader:\n",
    "#                     lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "#                         source_text, batch,\n",
    "#                         label_id=config['target_label_ids'][lossid],\n",
    "#                     )\n",
    "#                     lossvalues.append(lossvalue)\n",
    "#                     torch.cuda.empty_cache()\n",
    "#             lossvalue = torch.cat(lossvalues,dim=0)\n",
    "#             curr_loss += loss_weights[lossid] * lossvalue\n",
    "#             logging_loss[:, lossid] = lossvalue.clone()\n",
    "            \n",
    "#         allsat_ix = torch.where(logging_loss[:,1]> -np.log(config[\"min_epsilons\"][0]))[0].squeeze(0)\n",
    "#         if (allsat_ix.shape[0] > 0) and (config['selection_criteria'] == \"allsat_primary\"):\n",
    "#             best_ix = allsat_ix[curr_loss[allsat_ix].argmin()]\n",
    "#         else: ## in case config['selection_criteria'] == \"weighted_sum\" or allsat is all False\n",
    "#             best_ix = torch.argmin(curr_loss)\n",
    "\n",
    "#         hypotheses.append(tmp_dec_seq[best_ix])\n",
    "#         best_weighted_loss.append(curr_loss[best_ix].item())\n",
    "#         best_allsat.append(1 if best_ix in allsat_ix else 0)\n",
    "#         best_logging_loss.append(logging_loss[best_ix].cpu().numpy())\n",
    "        \n",
    "#         del curr_loss, logging_loss\n",
    "#         torch.cuda.empty_cache()\n",
    "#     return hypotheses, best_weighted_loss, best_allsat, best_logging_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:50<00:00,  5.05s/it]\n",
      "100%|██████████| 10/10 [00:51<00:00,  5.12s/it]\n",
      "100%|██████████| 10/10 [00:51<00:00,  5.14s/it]\n",
      "100%|██████████| 10/10 [00:50<00:00,  5.10s/it]\n",
      "100%|██████████| 10/10 [00:50<00:00,  5.08s/it]\n",
      "100%|██████████| 10/10 [00:50<00:00,  5.05s/it]\n",
      "100%|██████████| 10/10 [00:50<00:00,  5.08s/it]\n",
      "100%|██████████| 10/10 [00:50<00:00,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.9 s ± 310 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "# my_hypotheses = combi_rerank(masked_sequence, \n",
    "#                             indices_in_mlm_tokens,\n",
    "#                             predicted_token_ids.indices,\n",
    "#                             mlm_tokenizer,\n",
    "#                             config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/data/hyeryung/.conda/envs/loc-edit/lib/python3.8/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not going to be able to do that. I'm going to be able to do that\n",
      "I'm not going to be able to do that. I'll just have to go out and play\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Initializing the model and tokenizer for it\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "inputs = tokenizer([\"I'm not going to\"], return_tensors=\"pt\")\n",
    "\n",
    "# This shows a normal generate without any specific parameters\n",
    "summary_ids = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0])\n",
    "\n",
    "# This generates a penalty for repeated tokens\n",
    "penalized_ids = model.generate(**inputs, repetition_penalty=1.1)\n",
    "print(tokenizer.batch_decode(penalized_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aedb0a1cdf74bf1a362cddd7986c2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6fdd0393bf4a3b9e2232967c1d734b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36114f4f3e084cdfb410cc01793778dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2f56c046764b249fc29828af20d3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3a44c279014c2b91ad3a3eebf718a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ae47d43d664416bcaa686731185027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9ee581cfe34bd5b4513d0de8aeb3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/data/hyeryung/.conda/envs/loc-edit/lib/python3.8/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not going to be able to do that. I'm going to be able to do that\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Initializing the model and tokenizer for it\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "inputs = tokenizer([\"I'm not going to\"], return_tensors=\"pt\")\n",
    "\n",
    "# This shows a normal generate without any specific parameters\n",
    "summary_ids = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This generates a penalty for repeated tokens\n",
    "penalized_ids = model.generate(**inputs, repetition_penalty=1.1)\n",
    "print(tokenizer.batch_decode(penalized_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1301, 28628, 18435, 2159)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now let's control generation through a bias. Please note that the tokenizer is initialized differently!\n",
    "tokenizer_with_prefix_space = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", add_prefix_space=True)\n",
    "\n",
    "\n",
    "def get_tokens_as_tuple(word):\n",
    "    return tuple(tokenizer_with_prefix_space([word], add_special_tokens=False).input_ids[0])\n",
    "\n",
    "print(get_tokens_as_tuple(\"Trump\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.30.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['sequence_bias'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# If we add a negative bias without beam search, it may become \"stuck\" in a prefix without good continuations\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sequence_bias \u001b[38;5;241m=\u001b[39m {get_tokens_as_tuple(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrump\u001b[39m\u001b[38;5;124m\"\u001b[39m): \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10.0\u001b[39m}\n\u001b[0;32m----> 3\u001b[0m biased_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(biased_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/data/hyeryung/.conda/envs/loc-edit/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/hyeryung/.conda/envs/loc-edit/lib/python3.8/site-packages/transformers/generation/utils.py:1271\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1269\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m generation_config\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[0;32m-> 1271\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m logits_processor \u001b[38;5;28;01mif\u001b[39;00m logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m/data/hyeryung/.conda/envs/loc-edit/lib/python3.8/site-packages/transformers/generation/utils.py:1144\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1147\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['sequence_bias'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "# If we add a negative bias without beam search, it may become \"stuck\" in a prefix without good continuations\n",
    "sequence_bias = {get_tokens_as_tuple(\"Trump\"): -10.0}\n",
    "biased_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, sequence_bias=sequence_bias)\n",
    "print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m biased_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, sequence_bias\u001b[38;5;241m=\u001b[39msequence_bias)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(biased_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# We can also add a positive bias to nudge the model towards specific tokens or continuations\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "biased_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, num_beams=4, sequence_bias=sequence_bias)\n",
    "print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])\n",
    "\n",
    "# We can also add a positive bias to nudge the model towards specific tokens or continuations\n",
    "sequence_bias = {get_tokens_as_tuple(\"Donald Duck\"): 10.0}\n",
    "biased_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, num_beams=4, sequence_bias=sequence_bias)\n",
    "print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Check for discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from itertools import chain\n",
    "import math\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "os.chdir('/data/hyeryung/mucoco')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "from copy import deepcopy\n",
    "import new_module.losses as lossbuilder\n",
    "import new_module.losses_old as lossbuilder_old\n",
    "import wandb\n",
    "from new_module.decode_utils import (\n",
    "    beam_rerank_v0,\n",
    "    beam_rerank_v1,\n",
    "    beam_rerank_v2,\n",
    "    combi_rerank,\n",
    ")\n",
    "# from new_module.new_decode_utils import get_beam_hypotheses, get_combi_hypotheses, final_reranking\n",
    "from new_module.new_decode_utils import get_beam_hypotheses_v0, get_beam_hypotheses_v1, get_combi_hypotheses, final_reranking\n",
    "from new_module.evaluate_wandb import evaluate_main\n",
    "from new_module.locate.new_locate_utils import LocateMachine\n",
    "from new_module.locate.locate_utils import locate_main\n",
    "from new_module.utils.robertacustom import RobertaCustomForSequenceClassification\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(os.environ.get(\"LOGGING_LEVEL\", logging.DEBUG))\n",
    "import joblib\n",
    "config = joblib.load('config.pkl')\n",
    "config['device'] = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/generation_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "class dummyArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "build_loss_args = dummyArgs(**config[\"build_loss_dict\"])\n",
    "\n",
    "## load data\n",
    "if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "    source_dataset = [\n",
    "        json.loads(l)[config[\"jsonl_primary_key\"]][config[\"jsonl_secondary_key\"]]\n",
    "        for l in open(config[\"source_data\"])\n",
    "    ]\n",
    "    generation_dataset = [\n",
    "        json.loads(l)[\"generations\"] for l in open(config[\"source_data\"])\n",
    "    ]\n",
    "elif (config[\"task\"] == \"formality\") or (config[\"task\"] == \"sentiment-lewis-compr\"):\n",
    "    with open(config[\"source_data\"], \"r\") as f:\n",
    "        generation_dataset = [line.rstrip('\\n') for line in f.readlines()]\n",
    "    source_dataset = [\"\" for l in generation_dataset]\n",
    "\n",
    "## load tokenizer, models, define losses\n",
    "name2tokenizer = {}\n",
    "name2model = {}\n",
    "name2config = {}\n",
    "loss2tokenizer = {}\n",
    "embed_luts = []\n",
    "\n",
    "for i, model_path in enumerate(config[\"model_paths\"]):\n",
    "    if (\n",
    "        model_path not in name2model\n",
    "    ):  # making sure we are not loading the model twice in case some constraints use the same model.\n",
    "        try:\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=True,\n",
    "            )\n",
    "        except:\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=False,\n",
    "            )\n",
    "\n",
    "        name2config[model_path] = AutoConfig.from_pretrained(\n",
    "            model_path, cache_dir=config[\"cache_dir\"]\n",
    "        )\n",
    "\n",
    "        if config[\"model_types\"][i] == \"RobertaCustomForSequenceClassification\":\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                RobertaCustomForSequenceClassification.from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(transformers, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        name2model[model_path].eval()\n",
    "        name2model[model_path].to(config['device'])\n",
    "\n",
    "    input_embeds = name2model[model_path].get_input_embeddings()\n",
    "    if isinstance(input_embeds, torch.nn.Sequential):\n",
    "        input_embeds = input_embeds[0]\n",
    "    embed_luts.append(input_embeds)\n",
    "\n",
    "    if config[\"target_type\"] == \"embeds\":\n",
    "        embed_luts[-1].requires_grad = False\n",
    "\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "mlm = None if config[\"method\"] == \"mlm-beamsearch-v2\" else AutoModelForMaskedLM.from_pretrained(\"roberta-base\").to(config['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lossfns = []\n",
    "for i, loss in enumerate(config[\"losses\"]):\n",
    "    lossfns.append(\n",
    "        lossbuilder.build_loss(\n",
    "            loss,\n",
    "            name2model[config[\"model_paths\"][i]],\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]],\n",
    "            build_loss_args,\n",
    "        )\n",
    "    )\n",
    "    lossfns[i].tokenizer.add_special_tokens({\"mask_token\": mlm_tokenizer.mask_token})\n",
    "    loss2tokenizer[loss] = lossfns[i].tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "# for text_id in range(len(source_dataset))[resume_idx:]:\n",
    "text_id = 3\n",
    "source_text = source_dataset[text_id]\n",
    "if source_text == \"\":\n",
    "    source_text = lossfns[0].tokenizer.bos_token\n",
    "\n",
    "if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "    AR_prediction_all = [x[\"text\"] for x in generation_dataset[text_id]]\n",
    "    # predicted_batches = [x[\"tokens\"] for x in generation_dataset[text_id]]\n",
    "    # predicted_batches = [\n",
    "    #     torch.tensor([x], dtype=torch.long, device=config[\"device\"])\n",
    "    #     for x in predicted_batches\n",
    "    # ]\n",
    "    \n",
    "elif (config[\"task\"] == \"formality\") or (\n",
    "    config[\"task\"] == \"sentiment-lewis-compr\"\n",
    "):\n",
    "    AR_prediction_all = [generation_dataset[text_id]]\n",
    "\n",
    "curr_num_samples = len(AR_prediction_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define an object to locate problematic phrases\n",
    "locator = LocateMachine(lossfns[1].model, lossfns[1].tokenizer)\n",
    "running_text = best_text = deepcopy(AR_prediction_all)\n",
    "masked_text = locator.locate_main(running_text, \n",
    "                        method = config['locate_method'], \n",
    "                        max_num_tokens = config['num_edit_token_per_step'], \n",
    "                        unit = config['locate_unit'], \n",
    "                        num_layer = -2, #penultimate\n",
    "                        label_id = config['target_label_ids'][1])\n",
    "\n",
    "## replace tokens at the indices with mask tokens\n",
    "                \n",
    "inputs = mlm_tokenizer(\n",
    "    masked_text, return_tensors=\"pt\", padding=True, truncation=True\n",
    ")\n",
    "inputs = inputs.to(config['device']) \n",
    "masked_sequence=inputs['input_ids']\n",
    "\n",
    "\n",
    "## make predictions for the masked indices\n",
    "with torch.no_grad():\n",
    "    logits = mlm(**inputs).logits\n",
    "\n",
    "special_token_ids = mlm_tokenizer.convert_tokens_to_ids(mlm_tokenizer.all_special_tokens)\n",
    "logits[:, :, special_token_ids] = -float(\"inf\")\n",
    "\n",
    "\n",
    "indices_in_mlm_tokens = (\n",
    "    inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    ").nonzero(as_tuple=True)\n",
    "\n",
    "## get top k tokens for each index\n",
    "predicted_token_ids = torch.topk(\n",
    "    logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "    k=config['k_per_location'],\n",
    "    dim=-1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = get_beam_hypotheses(source_text, \n",
    "                            masked_sequence, \n",
    "                            indices_in_mlm_tokens,\n",
    "                            predicted_token_ids.indices,\n",
    "                            mlm_tokenizer, \n",
    "                            lossfns,\n",
    "                            config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace tokens at the indices with mask tokens\n",
    "inputs_old = []\n",
    "indices_in_mlm_tokens_old = []\n",
    "predicted_token_ids_old = []\n",
    "for masked_text_ in masked_text:\n",
    "    \n",
    "    ## replace tokens at the indices with mask tokens\n",
    "    inputs_old_ = mlm_tokenizer(\n",
    "        masked_text_, return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs_old_ = inputs_old_.to(config['device'])\n",
    "    ## make predictions for the masked indices\n",
    "    with torch.no_grad():\n",
    "        logits_old = mlm(**inputs_old_).logits\n",
    "    indices_in_mlm_tokens_old_ = (\n",
    "        inputs_old_.input_ids == mlm_tokenizer.mask_token_id\n",
    "    )[0].nonzero(as_tuple=True)[0]\n",
    "    # print(f\"indices_in_mlm_tokens: {indices_in_mlm_tokens}\")\n",
    "    ## get top k tokens for each index\n",
    "    inputs_old.append(inputs_old_)\n",
    "    \n",
    "    ## make logits for special tokens -inf.\n",
    "    special_token_ids = mlm_tokenizer.convert_tokens_to_ids(mlm_tokenizer.all_special_tokens)\n",
    "    logits_old[:, :, special_token_ids] = -np.inf\n",
    "    \n",
    "    predicted_token_ids_old_ = torch.topk(\n",
    "        logits_old[0, indices_in_mlm_tokens_old_],\n",
    "        k=config['k_per_location'],\n",
    "        dim=-1,\n",
    "    )\n",
    "    indices_in_mlm_tokens_old.append(indices_in_mlm_tokens_old_)\n",
    "    predicted_token_ids_old.append(predicted_token_ids_old_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def beam_rerank_v0(source_text, ## text (too arbitrary?)\n",
    "                    masked_sequence, ## in mlm tokenizer's tokens\n",
    "                    indices_in_mlm_tokens,\n",
    "                    predicted_token_ids,\n",
    "                    mlm_tokenizer, \n",
    "                    lossfns,\n",
    "                    config, \n",
    "                    beam_size = 5):\n",
    "    \n",
    "    hypotheses = [torch.LongTensor([]).to(config['device'])]\n",
    "    L = masked_sequence.size(-1)\n",
    "\n",
    "    for i in range(L):\n",
    "        if masked_sequence[0, i] != mlm_tokenizer.mask_token_id:\n",
    "            hypotheses = list(torch.cat([torch.stack(hypotheses,dim=0), \n",
    "                                        masked_sequence[:, i].unsqueeze(0).repeat((len(hypotheses),1)).to(config['device'])],dim=-1))\n",
    "        else:\n",
    "            num_hypotheses = len(hypotheses)\n",
    "            hypotheses = torch.stack(hypotheses,dim=0).unsqueeze(0)\n",
    "            hypotheses = hypotheses.repeat(config['k_per_location'], 1, 1)\n",
    "            candidates = predicted_token_ids.indices[torch.where(indices_in_mlm_tokens == i)[0], :].to(config['device']).T.unsqueeze(1)\n",
    "            candidates = candidates.repeat(1, num_hypotheses, 1)\n",
    "            hypotheses_exp = torch.cat([hypotheses, candidates], dim=-1)\n",
    "            hypotheses_exp = hypotheses_exp.view(-1, hypotheses_exp.shape[-1])\n",
    "            hypotheses_exp = list(hypotheses_exp)\n",
    "\n",
    "            losses = []\n",
    "            loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "            for hyp in hypotheses_exp:\n",
    "                curr_loss = 0.0\n",
    "                for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "                    with torch.no_grad():\n",
    "                        lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                            source_text, mlm_tokenizer.decode(hyp, skip_special_tokens=True),\n",
    "                            label_id=config['target_label_ids'][lossid],\n",
    "                        )\n",
    "                    curr_loss += loss_weights[lossid] * lossvalue.item()\n",
    "                losses.append(curr_loss)\n",
    "\n",
    "            hypotheses = sorted(zip(hypotheses_exp, losses), key=lambda x: x[1])[:beam_size]\n",
    "            hypotheses = [x[0] for x in hypotheses]\n",
    "            \n",
    "    return [mlm_tokenizer.decode(x, skip_special_tokens=True) for x in hypotheses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfns_old = []\n",
    "loss2tokenizer_old = {}\n",
    "for i, loss in enumerate(config[\"losses\"]):\n",
    "    lossfns_old.append(\n",
    "        lossbuilder_old.build_loss(\n",
    "            loss,\n",
    "            name2model[config[\"model_paths\"][i]],\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]],\n",
    "            build_loss_args,\n",
    "        )\n",
    "    )\n",
    "    lossfns_old[i].tokenizer.add_special_tokens({\"mask_token\": mlm_tokenizer.mask_token})\n",
    "    loss2tokenizer_old[loss] = lossfns_old[i].tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses_old = []\n",
    "for text_id in range(len(masked_text)):\n",
    "    hypotheses_old_ = beam_rerank_v0(source_text,\n",
    "                                inputs_old[text_id].input_ids,\n",
    "                                indices_in_mlm_tokens_old[text_id],\n",
    "                                predicted_token_ids_old[text_id],\n",
    "                                mlm_tokenizer, \n",
    "                                lossfns_old,\n",
    "                                config, \n",
    "                                beam_size = config['beam_size'])\n",
    "    hypotheses_old.append(hypotheses_old_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 결과가 다르다. 보다 자세히 디버깅 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디버깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_beam_hypotheses(source_text:str, \n",
    "#                     masked_sequence:torch.Tensor, \n",
    "#                     indices_in_mlm_tokens:Tuple[torch.Tensor],\n",
    "#                     predicted_token_ids:torch.Tensor,\n",
    "#                     mlm_tokenizer:transformers.AutoTokenizer, \n",
    "#                     lossfns:List[lossbuilder.BaseLoss],\n",
    "#                     config:dict) -> List[List[str]]:\n",
    "#     \"\"\"\n",
    "#     A function to get hypotheses of beam size via editing beam search with reranking.\n",
    "#     Run this function if config['method'] == 'mlm-beamsearch-v0' or config['method'] == 'mlm-beamsearch-v1'\n",
    "#     If config['method'] == 'mlm-beamsearch-v1', rerank beam only with fluency energy.\n",
    "#     If config['method'] == 'mlm-beamsearch-v0', rerank beam with a weighted sum of fluency and constraint energy.\n",
    "    \n",
    "#     #ToDo\n",
    "#     #Implement mlm-beamsearch-v0 with allsat-primary and compare \n",
    "    \n",
    "#     params: \n",
    "#         source_text: a prompt text \n",
    "#         masked_sequence: token ids of original generation text with located indices masked. tokenized by MLM's tokenizer.\n",
    "#         indices_in_mlm_tokens: a result of running \n",
    "#                                     `indices_in_mlm_tokens = (\n",
    "#                                                                 inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "#                                                                 ).nonzero(as_tuple=True)`\n",
    "#         predicted_token_ids: a result of running\n",
    "#                                     `predicted_token_ids = torch.topk(\n",
    "#                                                                 logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "#                                                                 k=config['k_per_location'],\n",
    "#                                                                 dim=-1,).indices`\n",
    "#         mlm_tokenizer: tokenizer of MLM\n",
    "#         lossfns: a list of loss functions\n",
    "#         config: a dictionary of configurations\n",
    "    \n",
    "#     returns:\n",
    "#         hypotheses: a list of a list of the beam number of hypotheses for each sample         \n",
    "#     \"\"\"\n",
    "    \n",
    "#     hypotheses = masked_sequence[:, None, :].repeat((1,config['beam_size'],1))\n",
    "#     edit_indices = sorted(list(set(indices_in_mlm_tokens[1].tolist())))\n",
    "\n",
    "#     for i in edit_indices:\n",
    "        \n",
    "#         batch_ids_to_edit = indices_in_mlm_tokens[0][indices_in_mlm_tokens[1]==i]\n",
    "\n",
    "#         tmp_hypotheses = hypotheses[batch_ids_to_edit].detach().clone()\n",
    "#         tmp_hypotheses=tmp_hypotheses.repeat((1,config['k_per_location'],1))\n",
    "\n",
    "#         candidates = predicted_token_ids[(indices_in_mlm_tokens[1]==i).nonzero().squeeze(-1),:]\n",
    "#         candidates = candidates[:, :, None].repeat((1,1, config['beam_size'])).reshape(candidates.shape[0], -1,1)\n",
    "\n",
    "#         tmp_hypotheses = torch.cat((tmp_hypotheses[:, :, :i], candidates),dim=-1) ## tmp_hypotheses: [(a,b,c),(a,b,c), ..., (a,b,c)], candidates: [(p,p,p), (q,q,q), ..., (v,v,v)]\n",
    "#         tmp_hypotheses = tmp_hypotheses.reshape(-1, tmp_hypotheses.shape[-1])\n",
    "        \n",
    "#         loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "#         curr_loss = torch.zeros(tmp_hypotheses.shape[0]).to(config['device'])\n",
    "#         for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "#             if config['method'] == 'mlm-beamsearch-v1' and lossid > 0:\n",
    "#                 continue\n",
    "#             with torch.no_grad():\n",
    "#                 lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "#                     source_text, mlm_tokenizer.batch_decode(tmp_hypotheses,skip_special_tokens=True),\n",
    "#                     label_id=config['target_label_ids'][lossid],\n",
    "#                 )\n",
    "#             torch.cuda.empty_cache()\n",
    "#             curr_loss += loss_weights[lossid] * lossvalue\n",
    "            \n",
    "#         curr_loss = torch.stack(torch.split(curr_loss, config['beam_size'] * config['k_per_location'], dim=0),dim=0)\n",
    "#         top_beams=torch.topk(curr_loss, k=(config['beam_size']*(config['k_per_location']-1)+1), dim=-1, largest=False).indices\n",
    "\n",
    "#         tmp_hypotheses = torch.split(tmp_hypotheses, config['beam_size'] * config['k_per_location'], dim=0) ## 아래의 작업을 더 간단히 할 수 있는 방법?\n",
    "#         tmp_hypotheses = torch.stack([x[top_beams[j]] for j, x in enumerate(tmp_hypotheses)],dim=0)\n",
    "#         tmp_hypotheses = torch.unique(tmp_hypotheses, dim=1)[:, :config['beam_size'], :]\n",
    "        \n",
    "#         hypotheses[batch_ids_to_edit,:, i]=tmp_hypotheses[:, :, i]\n",
    "            \n",
    "#     return [mlm_tokenizer.batch_decode(hypotheses[j], skip_special_tokens=True) for j in range(hypotheses.shape[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## debugged version\n",
    "def get_beam_hypotheses(source_text:str, \n",
    "                    masked_sequence:torch.Tensor, \n",
    "                    indices_in_mlm_tokens:Tuple[torch.Tensor],\n",
    "                    predicted_token_ids:torch.Tensor,\n",
    "                    mlm_tokenizer:transformers.AutoTokenizer, \n",
    "                    lossfns:List[lossbuilder.BaseLoss],\n",
    "                    config:dict) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    A function to get hypotheses of beam size via editing beam search with reranking.\n",
    "    Run this function if config['method'] == 'mlm-beamsearch-v0' or config['method'] == 'mlm-beamsearch-v1'\n",
    "    If config['method'] == 'mlm-beamsearch-v1', rerank beam only with fluency energy.\n",
    "    If config['method'] == 'mlm-beamsearch-v0', rerank beam with a weighted sum of fluency and constraint energy.\n",
    "    \n",
    "    #ToDo\n",
    "    #Implement mlm-beamsearch-v0 with allsat-primary and compare \n",
    "    \n",
    "    params: \n",
    "        source_text: a prompt text \n",
    "        masked_sequence: token ids of original generation text with located indices masked. tokenized by MLM's tokenizer.\n",
    "        indices_in_mlm_tokens: a result of running \n",
    "                                    `indices_in_mlm_tokens = (\n",
    "                                                                inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "                                                                ).nonzero(as_tuple=True)`\n",
    "        predicted_token_ids: a result of running\n",
    "                                    `predicted_token_ids = torch.topk(\n",
    "                                                                logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "                                                                k=config['k_per_location'],\n",
    "                                                                dim=-1,).indices`\n",
    "        mlm_tokenizer: tokenizer of MLM\n",
    "        lossfns: a list of loss functions\n",
    "        config: a dictionary of configurations\n",
    "    \n",
    "    returns:\n",
    "        hypotheses: a list of a list of the beam number of hypotheses for each sample         \n",
    "    \"\"\"\n",
    "    \n",
    "    def repeat_interleave_unravel(arr,split_blocks):\n",
    "        arr_ = torch.split(arr.T,1,dim=1)\n",
    "        arr_ = [x.repeat(1,split_blocks[i]).reshape(-1,1) for i,x in enumerate(arr_)]\n",
    "        arr_ = torch.cat(arr_,dim=0)\n",
    "        return arr_\n",
    "    \n",
    "    hypotheses = list(torch.split(masked_sequence,1,dim=0)) ## [torch.tensor([[a],[b],[c]]), torch.tensor([[d]])]\n",
    "    edit_indices = sorted(list(set(indices_in_mlm_tokens[1].tolist())))\n",
    "    for curr_edit_index in edit_indices:\n",
    "        \n",
    "        batch_ids_to_edit = indices_in_mlm_tokens[0][indices_in_mlm_tokens[1]==curr_edit_index].tolist()\n",
    "        num_initial_hypotheses = [len(hypotheses[i]) for i in batch_ids_to_edit] ## keep track of initial hypotheses count e.g. [3, 1]\n",
    "        tmp_hypotheses = [hypotheses[i].repeat((config['k_per_location'],1)) for i in batch_ids_to_edit] ## [torch.tensor([[a],[b],[c],[a],[b],[c],[a],[b],[c]]), torch.tensor([[d],[d],[d]])]\n",
    "        num_initial_tmp_hypotheses = [len(x) for x in tmp_hypotheses]\n",
    "        tmp_hypotheses = torch.cat(tmp_hypotheses,dim=0) ## torch.tensor([[a],[b],[c],[a],[b],[c],[a],[b],[c],[d],[d],[d]])\n",
    "        \n",
    "\n",
    "        new_func_candidates = predicted_token_ids[indices_in_mlm_tokens[1]==curr_edit_index] ## shape: (len(batch_ids_to_edit), k_per_location) e.g. [[x,y,z],[q,w,e]]\n",
    "        new_func_candidates = repeat_interleave_unravel(new_func_candidates,num_initial_hypotheses) ## shape: (sum(num_initial_hypotheses), k_per_location) e.g. [[x],[x],[x],[y],[y],[y],[z],[z],[z],[q],[w],[e]]\n",
    "        new_func_candidates = new_func_candidates.to(config['device'])\n",
    "        \n",
    "\n",
    "        tmp_hypotheses = torch.cat((tmp_hypotheses[ :, :curr_edit_index], new_func_candidates),dim=-1) ## tmp_hypotheses: [(a,b,c),(a,b,c), ..., (a,b,c)], new_func_candidates: [(p,p,p), (q,q,q), ..., (v,v,v)]\n",
    "\n",
    "        loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "        curr_loss = torch.zeros(tmp_hypotheses.shape[0]).to(config['device'])\n",
    "        for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "            if config['method'] == 'mlm-beamsearch-v1' and lossid > 0:\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                    source_text, mlm_tokenizer.batch_decode(tmp_hypotheses,skip_special_tokens=True),\n",
    "                    label_id=config['target_label_ids'][lossid],\n",
    "                )\n",
    "            torch.cuda.empty_cache()\n",
    "            curr_loss += loss_weights[lossid] * lossvalue\n",
    "        curr_loss = torch.split(curr_loss, num_initial_tmp_hypotheses, dim=0)\n",
    "        top_beams = [torch.topk(x, k=config['beam_size'], dim=-1, largest=False).indices for x in curr_loss]\n",
    "\n",
    "        tmp_hypotheses = torch.split(tmp_hypotheses, num_initial_tmp_hypotheses, dim=0)\n",
    "        for jx, ix in enumerate(batch_ids_to_edit):\n",
    "\n",
    "            hypotheses[ix] = torch.cat([tmp_hypotheses[jx][top_beams[jx]], masked_sequence[ix][curr_edit_index+1:].unsqueeze(0).repeat(config['beam_size'],1)], dim=-1)\n",
    "            \n",
    "    return [mlm_tokenizer.batch_decode(x, skip_special_tokens=True) for x in hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## old version code에서 score 결과 및 중간에 나오는 텐서들 추출\n",
    "sample_id = 0\n",
    "inside_func_masked_sequence= inputs_old[sample_id].input_ids\n",
    "inside_func_hypotheses = [torch.LongTensor([]).to(config['device'])]\n",
    "L = inside_func_masked_sequence.size(-1)\n",
    "\n",
    "for i in range(L):\n",
    "    if inside_func_masked_sequence[0, i] != mlm_tokenizer.mask_token_id:\n",
    "        inside_func_hypotheses = list(torch.cat([torch.stack(inside_func_hypotheses,dim=0), \n",
    "                                    inside_func_masked_sequence[:, i].unsqueeze(0).repeat((len(inside_func_hypotheses),1)).to(config['device'])],dim=-1))\n",
    "    else:\n",
    "        num_inside_func_hypotheses = len(inside_func_hypotheses)\n",
    "        inside_func_hypotheses = torch.stack(inside_func_hypotheses,dim=0).unsqueeze(0)\n",
    "        inside_func_hypotheses = inside_func_hypotheses.repeat(config['k_per_location'], 1, 1)\n",
    "        inside_func_candidates = predicted_token_ids_old[sample_id].indices[torch.where(indices_in_mlm_tokens_old[sample_id] == i)[0], :].to(config['device']).T.unsqueeze(1)\n",
    "        inside_func_candidates = inside_func_candidates.repeat(1, num_inside_func_hypotheses, 1)\n",
    "        inside_func_hypotheses_exp = torch.cat([inside_func_hypotheses, inside_func_candidates], dim=-1)\n",
    "        inside_func_hypotheses_exp = inside_func_hypotheses_exp.view(-1, inside_func_hypotheses_exp.shape[-1])\n",
    "        inside_func_hypotheses_exp = list(inside_func_hypotheses_exp)\n",
    "\n",
    "        inside_func_losses = []\n",
    "        loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "        for hyp in inside_func_hypotheses_exp:\n",
    "            curr_loss = 0.0\n",
    "            for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "                with torch.no_grad():\n",
    "                    lossvalue = lossfns_old[lossid].compute_gold_loss(\n",
    "                        source_text, mlm_tokenizer.decode(hyp, skip_special_tokens=True),\n",
    "                        label_id=config['target_label_ids'][lossid],\n",
    "                    )\n",
    "                curr_loss += loss_weights[lossid] * lossvalue.item()\n",
    "            inside_func_losses.append(curr_loss)\n",
    "\n",
    "        inside_func_hypotheses = sorted(zip(inside_func_hypotheses_exp, inside_func_losses), key=lambda x: x[1])[:config['beam_size']]\n",
    "        inside_func_hypotheses = [x[0] for x in inside_func_hypotheses]\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 새로 짠 함수에서 처리 중간에 나오는 텐서 추출\n",
    "hypotheses = masked_sequence[:, None, :].repeat((1,config['beam_size'],1))\n",
    "edit_indices = sorted(list(set(indices_in_mlm_tokens[1].tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in edit_indices:\n",
    "    print(i)\n",
    "    batch_ids_to_edit = indices_in_mlm_tokens[0][indices_in_mlm_tokens[1]==i]\n",
    "    print(batch_ids_to_edit)\n",
    "    print('-'*30)\n",
    "    tmp_hypotheses = hypotheses[batch_ids_to_edit].detach().clone()\n",
    "    print(tmp_hypotheses)\n",
    "    print('-'*30)\n",
    "    tmp_hypotheses=tmp_hypotheses.repeat((1,config['k_per_location'],1))\n",
    "    print(tmp_hypotheses)\n",
    "    print('-'*30)\n",
    "    print('-'*30)\n",
    "\n",
    "    new_func_candidates = predicted_token_ids.indices[(indices_in_mlm_tokens[1]==i).nonzero().squeeze(-1),:]\n",
    "    print(new_func_candidates)\n",
    "    print('-'*30)\n",
    "    new_func_candidates = new_func_candidates[:, :, None].repeat((1,1, config['beam_size'])).reshape(new_func_candidates.shape[0], -1,1)\n",
    "    print(new_func_candidates)\n",
    "    print('-'*30)\n",
    "\n",
    "    tmp_hypotheses = torch.cat((tmp_hypotheses[:, :, :i], new_func_candidates),dim=-1) ## tmp_hypotheses: [(a,b,c),(a,b,c), ..., (a,b,c)], new_func_candidates: [(p,p,p), (q,q,q), ..., (v,v,v)]\n",
    "    print(tmp_hypotheses)\n",
    "    print('-'*30)\n",
    "    tmp_hypotheses = tmp_hypotheses.reshape(-1, tmp_hypotheses.shape[-1])\n",
    "    print(tmp_hypotheses)\n",
    "    print('-'*30)\n",
    "    print('-'*30)\n",
    "\n",
    "    loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "    curr_loss = torch.zeros(tmp_hypotheses.shape[0]).to(config['device'])\n",
    "    for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "        if config['method'] == 'mlm-beamsearch-v1' and lossid > 0:\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                source_text, mlm_tokenizer.batch_decode(tmp_hypotheses,skip_special_tokens=True),\n",
    "                label_id=config['target_label_ids'][lossid],\n",
    "            )\n",
    "        torch.cuda.empty_cache()\n",
    "        curr_loss += loss_weights[lossid] * lossvalue\n",
    "    \n",
    "    curr_loss_for_backup = curr_loss.detach().clone()\n",
    "    curr_loss = torch.stack(torch.split(curr_loss, config['beam_size'] * config['k_per_location'], dim=0),dim=0)\n",
    "    print(curr_loss)\n",
    "    print('-'*30)    \n",
    "    top_beams=torch.topk(curr_loss, k=(config['beam_size']*(config['k_per_location']-1)+1), dim=-1, largest=False).indices\n",
    "    print(top_beams)\n",
    "    print('-'*30)   \n",
    "    print('-'*30)   \n",
    "\n",
    "    tmp_hypotheses = torch.split(tmp_hypotheses, config['beam_size'] * config['k_per_location'], dim=0) ## 아래의 작업을 더 간단히 할 수 있는 방법?\n",
    "    tmp_hypotheses_for_backup = deepcopy(tmp_hypotheses)\n",
    "    print(tmp_hypotheses)\n",
    "    print('-'*30)       \n",
    "    \n",
    "    tmp_hypotheses = torch.stack([x[top_beams[j]] for j, x in enumerate(tmp_hypotheses)],dim=0)\n",
    "    tmp_hypotheses_for_backup_2 = tmp_hypotheses.detach().clone()\n",
    "    print(tmp_hypotheses)\n",
    "    print('-'*30)  \n",
    "    \n",
    "    tmp_hypotheses = torch.unique(tmp_hypotheses, dim=1)[:, :config['beam_size'], :]\n",
    "    print(tmp_hypotheses)\n",
    "    print('-'*30)  \n",
    "    \n",
    "    hypotheses[batch_ids_to_edit,:, i]=tmp_hypotheses[:, :, i]\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.unique가 문제였다.\n",
    "torch.unique(torch.Tensor([[7,1,3,3,4,5],[2,2,2,3,3,3]]),dim=-1,sorted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제 해결 할 수 있도록 재코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypotheses = list(torch.split(masked_sequence,1,dim=0))\n",
    "# edit_indices = sorted(list(set(indices_in_mlm_tokens[1].tolist())))\n",
    "# i = edit_indices[0]\n",
    "# # for i in edit_indices:\n",
    "# batch_ids_to_edit = indices_in_mlm_tokens[0][indices_in_mlm_tokens[1]==i]\n",
    "\n",
    "# tmp_hypotheses = [hypotheses[i] for i in batch_ids_to_edit.tolist()]\n",
    "# tmp_hypotheses = torch.cat(tmp_hypotheses,dim=0).repeat((config['k_per_location'],1))\n",
    "\n",
    "# num_initial_hypotheses = [len(hypotheses[i]) for i in batch_ids_to_edit.tolist()]\n",
    "# print(tmp_hypotheses.shape)\n",
    "# print(num_initial_hypotheses)\n",
    "# new_func_candidates = predicted_token_ids.indices[(indices_in_mlm_tokens[1]==i).nonzero().squeeze(-1),:]\n",
    "# new_func_candidates = new_func_candidates.to(config['device'])\n",
    "# # %%timeit\n",
    "# # new_func_candidates_ = np.repeat(new_func_candidates.cpu(), num_initial_hypotheses, axis=0)\n",
    "# # %%timeit\n",
    "\n",
    "# # def func(arr,split_blocks):\n",
    "# #     arr_ = torch.split(arr,1,dim=0)\n",
    "# #     arr_ = [x.repeat(split_blocks[i],1) for i,x in enumerate(arr_)]\n",
    "# #     arr_ = torch.cat(arr_,dim=0)\n",
    "# #     return arr_\n",
    "    \n",
    "# # # new_func_candidates_ = torch.split(new_func_candidates,1,dim=0)\n",
    "# # # new_func_candidates_ = [x.repeat(num_initial_hypotheses[i],1) for i,x in enumerate(new_func_candidates_)]\n",
    "# # # new_func_candidates_ = torch.cat(new_func_candidates_,dim=0)\n",
    "# # new_func_candidates_ = func(new_func_candidates,num_initial_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def repeat_interleave_column_wise(arr,split_blocks):\n",
    "#     arr_ = torch.split(arr,1,dim=1)\n",
    "#     arr_ = [x.repeat(1,split_blocks[i]) for i,x in enumerate(arr_)]\n",
    "#     arr_ = torch.cat(arr_,dim=1)\n",
    "#     return arr_\n",
    "def repeat_interleave_unravel(arr,split_blocks):\n",
    "    arr_ = torch.split(arr.T,1,dim=1)\n",
    "    arr_ = [x.repeat(1,split_blocks[i]).reshape(-1,1) for i,x in enumerate(arr_)]\n",
    "    arr_ = torch.cat(arr_,dim=0)\n",
    "    return arr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = list(torch.split(masked_sequence,1,dim=0)) ## [torch.tensor([[a],[b],[c]]), torch.tensor([[d]])]\n",
    "edit_indices = sorted(list(set(indices_in_mlm_tokens[1].tolist())))\n",
    "for curr_edit_index in edit_indices:\n",
    "    \n",
    "    batch_ids_to_edit = indices_in_mlm_tokens[0][indices_in_mlm_tokens[1]==curr_edit_index].tolist()\n",
    "    num_initial_hypotheses = [len(hypotheses[i]) for i in batch_ids_to_edit] ## keep track of initial hypotheses count e.g. [3, 1]\n",
    "    tmp_hypotheses = [hypotheses[i].repeat((config['k_per_location'],1)) for i in batch_ids_to_edit] ## [torch.tensor([[a],[b],[c],[a],[b],[c],[a],[b],[c]]), torch.tensor([[d],[d],[d]])]\n",
    "    num_initial_tmp_hypotheses = [len(x) for x in tmp_hypotheses]\n",
    "    tmp_hypotheses = torch.cat(tmp_hypotheses,dim=0) ## torch.tensor([[a],[b],[c],[a],[b],[c],[a],[b],[c],[d],[d],[d]])\n",
    "    \n",
    "\n",
    "    new_func_candidates = predicted_token_ids.indices[indices_in_mlm_tokens[1]==curr_edit_index] ## shape: (len(batch_ids_to_edit), k_per_location) e.g. [[x,y,z],[q,w,e]]\n",
    "    new_func_candidates = repeat_interleave_unravel(new_func_candidates,num_initial_hypotheses) ## shape: (sum(num_initial_hypotheses), k_per_location) e.g. [[x],[x],[x],[y],[y],[y],[z],[z],[z],[q],[w],[e]]\n",
    "    new_func_candidates = new_func_candidates.to(config['device'])\n",
    "    \n",
    "\n",
    "    tmp_hypotheses = torch.cat((tmp_hypotheses[ :, :curr_edit_index], new_func_candidates),dim=-1) ## tmp_hypotheses: [(a,b,c),(a,b,c), ..., (a,b,c)], new_func_candidates: [(p,p,p), (q,q,q), ..., (v,v,v)]\n",
    "\n",
    "    loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "    curr_loss = torch.zeros(tmp_hypotheses.shape[0]).to(config['device'])\n",
    "    for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "        if config['method'] == 'mlm-beamsearch-v1' and lossid > 0:\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                source_text, mlm_tokenizer.batch_decode(tmp_hypotheses,skip_special_tokens=True),\n",
    "                label_id=config['target_label_ids'][lossid],\n",
    "            )\n",
    "        torch.cuda.empty_cache()\n",
    "        curr_loss += loss_weights[lossid] * lossvalue\n",
    "    curr_loss = torch.split(curr_loss, num_initial_tmp_hypotheses, dim=0)\n",
    "    top_beams = [torch.topk(x, k=config['beam_size'], dim=-1, largest=False).indices for x in curr_loss]\n",
    "\n",
    "    tmp_hypotheses = torch.split(tmp_hypotheses, num_initial_tmp_hypotheses, dim=0)\n",
    "    for jx, ix in enumerate(batch_ids_to_edit):\n",
    "\n",
    "        hypotheses[ix] = torch.cat([tmp_hypotheses[jx][top_beams[jx]], masked_sequence[ix][curr_edit_index+1:].unsqueeze(0).repeat(config['beam_size'],1)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' participating in a community project. AFFA is a, and a, for a.)',\n",
       "  ' participating in a community project. AFFA is a, and a, for a.,',\n",
       "  ' participating in a community project. AFFA is a, and a, for a..',\n",
       "  ' participating in a community project. AFFA is a, and the, for the.,',\n",
       "  ' participating in a community project. AFFA is a, and a, for a.]'],\n",
       " [' the fact that the word “s’ hates on ʳCriminally, the §$$$ by making \\u200ferr\\u200ft all girls,‼',\n",
       "  ' the fact that the word “s’ hates on ʳCriminally, the §$$$ by making \\u200ferr\\u200f, all girls,‼',\n",
       "  ' the fact that the word “s’ hates on ʳCriminally, in §$$$ by making \\u200ferr\\u200f, all girls,‼',\n",
       "  ' the fact that the word “s’ hates on ʳCriminally, and §$$$ by making \\u200ferr\\u200ft all girls,‼',\n",
       "  ' the fact that the word “s’ hates on ʳCriminally, and §$$$ by making \\u200ferr\\u200f, all girls,‼']]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[mlm_tokenizer.batch_decode(x,skip_special_tokens=True) for x in hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[mlm_tokenizer.batch_decode(x,skip_special_tokens=True) for x in hypotheses]==hypotheses_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 여전히 결과가 다르다.  -> 아래에서 문제 해결 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "## old version code에서 score 결과 및 중간에 나오는 텐서들 추출\n",
    "sample_id = 0\n",
    "inside_func_masked_sequence= inputs_old[sample_id].input_ids\n",
    "inside_func_hypotheses = [torch.LongTensor([]).to(config['device'])]\n",
    "L = inside_func_masked_sequence.size(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(6,L):\n",
    "    if inside_func_masked_sequence[0, i] != mlm_tokenizer.mask_token_id:\n",
    "        inside_func_hypotheses = list(torch.cat([torch.stack(inside_func_hypotheses,dim=0), \n",
    "                                    inside_func_masked_sequence[:, i].unsqueeze(0).repeat((len(inside_func_hypotheses),1)).to(config['device'])],dim=-1))\n",
    "    else:\n",
    "        num_inside_func_hypotheses = len(inside_func_hypotheses)\n",
    "        inside_func_hypotheses = torch.stack(inside_func_hypotheses,dim=0).unsqueeze(0)\n",
    "        inside_func_hypotheses = inside_func_hypotheses.repeat(config['k_per_location'], 1, 1)\n",
    "        inside_func_candidates = predicted_token_ids_old[sample_id].indices[torch.where(indices_in_mlm_tokens_old[sample_id] == i)[0], :].to(config['device']).T.unsqueeze(1)\n",
    "        inside_func_candidates = inside_func_candidates.repeat(1, num_inside_func_hypotheses, 1)\n",
    "        inside_func_hypotheses_exp = torch.cat([inside_func_hypotheses, inside_func_candidates], dim=-1)\n",
    "        inside_func_hypotheses_exp = inside_func_hypotheses_exp.view(-1, inside_func_hypotheses_exp.shape[-1])\n",
    "        inside_func_hypotheses_exp = list(inside_func_hypotheses_exp)\n",
    "\n",
    "        inside_func_losses = []\n",
    "        inside_func_logging_losses = []\n",
    "        loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "        for hyp in inside_func_hypotheses_exp:\n",
    "            inside_func_logging_losses_ = []\n",
    "            curr_loss = 0.0\n",
    "            for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "                with torch.no_grad():\n",
    "                    lossvalue = lossfns_old[lossid].compute_gold_loss(\n",
    "                        source_text, mlm_tokenizer.decode(hyp, skip_special_tokens=True),\n",
    "                        label_id=config['target_label_ids'][lossid],\n",
    "                    )\n",
    "                curr_loss += loss_weights[lossid] * lossvalue.item()\n",
    "                inside_func_logging_losses_.append(lossvalue.item())\n",
    "            inside_func_losses.append(curr_loss)\n",
    "            inside_func_logging_losses.append(inside_func_logging_losses_)\n",
    "\n",
    "        inside_func_hypotheses = sorted(zip(inside_func_hypotheses_exp, inside_func_losses), key=lambda x: x[1])[:config['beam_size']]\n",
    "        inside_func_hypotheses = [x[0] for x in inside_func_hypotheses]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[25.619842529296875, 0.006565547082573175],\n",
       " [27.66191291809082, 0.016244199126958847],\n",
       " [27.646846771240234, 0.0031171089503914118],\n",
       " [23.519668579101562, 0.0008852138998918235],\n",
       " [29.10457992553711, 0.004278791137039661],\n",
       " [20.562965393066406, 0.007706434931606054],\n",
       " [26.684307098388672, 0.014183899387717247],\n",
       " [27.279190063476562, 0.003786657238379121],\n",
       " [22.170913696289062, 0.0009748950251378119],\n",
       " [29.198673248291016, 0.003246515290811658],\n",
       " [24.92827606201172, 0.006835410837084055],\n",
       " [27.849733352661133, 0.04606309533119202],\n",
       " [21.119571685791016, 0.003120079869404435],\n",
       " [25.584442138671875, 0.0011926926672458649],\n",
       " [24.726680755615234, 0.0033429949544370174],\n",
       " [20.22182273864746, 0.007419057190418243],\n",
       " [22.00562286376953, 0.015473551116883755],\n",
       " [25.013351440429688, 0.0028388698119670153],\n",
       " [19.766510009765625, 0.0011063652345910668],\n",
       " [28.44176483154297, 0.0026339145842939615],\n",
       " [24.887611389160156, 0.004260985646396875],\n",
       " [26.682390213012695, 0.008359324187040329],\n",
       " [25.900714874267578, 0.0021347845904529095],\n",
       " [27.627681732177734, 0.0012502004392445087],\n",
       " [27.946819305419922, 0.003447662340477109],\n",
       " [23.970428466796875, 0.008097084239125252],\n",
       " [24.929168701171875, 0.023823320865631104],\n",
       " [22.916677474975586, 0.004054896999150515],\n",
       " [23.20785903930664, 0.0009964506607502699],\n",
       " [28.34528160095215, 0.005189045332372189],\n",
       " [24.1859130859375, 0.009623682126402855],\n",
       " [25.765213012695312, 0.022539155557751656],\n",
       " [26.45186996459961, 0.006152499467134476],\n",
       " [22.813440322875977, 0.00103586888872087],\n",
       " [28.533706665039062, 0.002687772735953331],\n",
       " [23.90086555480957, 0.049840085208415985],\n",
       " [25.147632598876953, 0.019413700327277184],\n",
       " [24.527904510498047, 0.002570184413343668],\n",
       " [21.337947845458984, 0.0010236029047518969],\n",
       " [26.7135066986084, 0.00527063338086009],\n",
       " [23.074371337890625, 0.006734057795256376],\n",
       " [25.223485946655273, 0.011142765171825886],\n",
       " [26.09442138671875, 0.003822284284979105],\n",
       " [21.157222747802734, 0.001069331425242126],\n",
       " [27.75186538696289, 0.0036582706961780787],\n",
       " [24.66539764404297, 0.00976369995623827],\n",
       " [26.16162872314453, 0.020490514114499092],\n",
       " [27.734710693359375, 0.006689534988254309],\n",
       " [23.698760986328125, 0.0008861667010933161],\n",
       " [30.29686737060547, 0.003001115983352065],\n",
       " [20.507442474365234, 0.017855733633041382],\n",
       " [20.507444381713867, 0.017855733633041382],\n",
       " [20.507442474365234, 0.017855733633041382],\n",
       " [17.213666915893555, 0.013107682578265667],\n",
       " [21.152908325195312, 0.033479660749435425],\n",
       " [21.152910232543945, 0.033479660749435425],\n",
       " [21.152908325195312, 0.033479660749435425],\n",
       " [21.152910232543945, 0.033479660749435425],\n",
       " [21.152908325195312, 0.033479660749435425],\n",
       " [22.64760971069336, 0.23641040921211243]]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging_loss.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[25.619840621948242, 0.006565547082573175],\n",
       " [27.661916732788086, 0.016244199126958847],\n",
       " [27.6468505859375, 0.0031171089503914118],\n",
       " [23.519678115844727, 0.0008852138998918235],\n",
       " [29.104576110839844, 0.004278909880667925],\n",
       " [24.928274154663086, 0.006835410837084055],\n",
       " [26.68430519104004, 0.014183899387717247],\n",
       " [27.279197692871094, 0.003786657238379121],\n",
       " [22.17091941833496, 0.0009748950251378119],\n",
       " [29.198673248291016, 0.003246515290811658],\n",
       " [20.221820831298828, 0.007419057190418243],\n",
       " [22.0056209564209, 0.015473551116883755],\n",
       " [21.119569778442383, 0.003120079869404435],\n",
       " [25.584447860717773, 0.0011926926672458649],\n",
       " [24.726682662963867, 0.0033429949544370174],\n",
       " [24.88760757446289, 0.004260985646396875],\n",
       " [26.68239402770996, 0.008359324187040329],\n",
       " [25.900724411010742, 0.0021347845904529095],\n",
       " [19.766511917114258, 0.0011063652345910668],\n",
       " [28.441770553588867, 0.0026339145842939615],\n",
       " [23.970430374145508, 0.008097084239125252],\n",
       " [24.929170608520508, 0.023823320865631104],\n",
       " [22.916683197021484, 0.004054778255522251],\n",
       " [23.20787239074707, 0.0009964506607502699],\n",
       " [27.946813583374023, 0.003447662340477109],\n",
       " [24.185914993286133, 0.009623682126402855],\n",
       " [25.765214920043945, 0.022539155557751656],\n",
       " [26.451871871948242, 0.006152499467134476],\n",
       " [22.813446044921875, 0.00103586888872087],\n",
       " [28.533700942993164, 0.002687772735953331],\n",
       " [23.074371337890625, 0.006734057795256376],\n",
       " [25.147634506225586, 0.019413582980632782],\n",
       " [24.527910232543945, 0.002570184413343668],\n",
       " [21.33795738220215, 0.0010236029047518969],\n",
       " [26.713510513305664, 0.00527063338086009],\n",
       " [24.665395736694336, 0.00976369995623827],\n",
       " [26.1616268157959, 0.020490514114499092],\n",
       " [26.09442710876465, 0.003822284284979105],\n",
       " [21.1572322845459, 0.001069331425242126],\n",
       " [27.75186538696289, 0.0036582706961780787],\n",
       " [26.260385513305664, 0.019046396017074585],\n",
       " [27.94380760192871, 0.03778718784451485],\n",
       " [28.125267028808594, 0.007830163463950157],\n",
       " [23.698762893676758, 0.0008861667010933161],\n",
       " [30.2968692779541, 0.003001115983352065],\n",
       " [23.331472396850586, 0.018242672085762024],\n",
       " [25.33942413330078, 0.04112106189131737],\n",
       " [24.737590789794922, 0.004179552663117647],\n",
       " [28.448789596557617, 0.0017421558732166886],\n",
       " [28.28904914855957, 0.006323330104351044]]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inside_func_logging_losses # 25.619842529296875 # 25.619840621948242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_decode_result = [mlm_tokenizer.decode(x, skip_special_tokens=True) for x in inside_func_hypotheses_exp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_decode_result = mlm_tokenizer.batch_decode(tmp_hypotheses[0],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_decode_result), len(indiv_decode_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in zip(sorted(indiv_decode_result),sorted(batch_decode_result)):\n",
    "    if a != b:\n",
    "        print(a)\n",
    "        print(b)\n",
    "        print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_interleave_unravel(arr,split_blocks):\n",
    "    arr_ = torch.split(arr.T,1,dim=1)\n",
    "    arr_ = [x.repeat(1,split_blocks[i]).reshape(-1,1) for i,x in enumerate(arr_)]\n",
    "    arr_ = torch.cat(arr_,dim=0)\n",
    "    return arr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = list(torch.split(masked_sequence,1,dim=0)) ## [torch.tensor([[a],[b],[c]]), torch.tensor([[d]])]\n",
    "edit_indices = sorted(list(set(indices_in_mlm_tokens[1].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "for curr_edit_index in edit_indices[2:]:\n",
    "    \n",
    "    batch_ids_to_edit = indices_in_mlm_tokens[0][indices_in_mlm_tokens[1]==curr_edit_index].tolist()\n",
    "    num_initial_hypotheses = [len(hypotheses[i]) for i in batch_ids_to_edit] ## keep track of initial hypotheses count e.g. [3, 1]\n",
    "    tmp_hypotheses = [hypotheses[i].repeat((config['k_per_location'],1)) for i in batch_ids_to_edit] ## [torch.tensor([[a],[b],[c],[a],[b],[c],[a],[b],[c]]), torch.tensor([[d],[d],[d]])]\n",
    "    num_initial_tmp_hypotheses = [len(x) for x in tmp_hypotheses]\n",
    "    tmp_hypotheses = torch.cat(tmp_hypotheses,dim=0) ## torch.tensor([[a],[b],[c],[a],[b],[c],[a],[b],[c],[d],[d],[d]])\n",
    "    \n",
    "    new_func_candidates = predicted_token_ids.indices[indices_in_mlm_tokens[1]==curr_edit_index] ## shape: (len(batch_ids_to_edit), k_per_location) e.g. [[x,y,z],[q,w,e]]\n",
    "    new_func_candidates = repeat_interleave_unravel(new_func_candidates,num_initial_hypotheses) ## shape: (sum(num_initial_hypotheses), k_per_location) e.g. [[x],[x],[x],[y],[y],[y],[z],[z],[z],[q],[w],[e]]\n",
    "    new_func_candidates = new_func_candidates.to(config['device'])\n",
    "    \n",
    "\n",
    "    tmp_hypotheses = torch.cat((tmp_hypotheses[ :, :curr_edit_index], new_func_candidates),dim=-1) ## tmp_hypotheses: [(a,b,c),(a,b,c), ..., (a,b,c)], new_func_candidates: [(p,p,p), (q,q,q), ..., (v,v,v)]\n",
    "\n",
    "    loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "    curr_loss = torch.zeros(tmp_hypotheses.shape[0]).to(config['device'])\n",
    "    logging_loss = torch.zeros((tmp_hypotheses.shape[0],len(lossfns))).to(config['device'])\n",
    "    for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "        if config['method'] == 'mlm-beamsearch-v1' and lossid > 0:\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                source_text, mlm_tokenizer.batch_decode(tmp_hypotheses,skip_special_tokens=True),\n",
    "                label_id=config['target_label_ids'][lossid],\n",
    "            )\n",
    "        torch.cuda.empty_cache()\n",
    "        curr_loss += loss_weights[lossid] * lossvalue\n",
    "        logging_loss[:, lossid] = lossvalue\n",
    "    curr_loss = torch.split(curr_loss, num_initial_tmp_hypotheses, dim=0)\n",
    "    top_beams = [torch.topk(x, k=config['beam_size'], dim=-1, largest=False).indices for x in curr_loss]\n",
    "\n",
    "    tmp_hypotheses = torch.split(tmp_hypotheses, num_initial_tmp_hypotheses, dim=0)\n",
    "    for jx, ix in enumerate(batch_ids_to_edit):\n",
    "\n",
    "        hypotheses[ix] = torch.cat([tmp_hypotheses[jx][top_beams[jx]], masked_sequence[ix][curr_edit_index+1:].unsqueeze(0).repeat(config['beam_size'],1)], dim=-1)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beam rerank v1 도 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## debugged version\n",
    "def get_beam_hypotheses(source_text:str, \n",
    "                    masked_sequence:torch.Tensor, \n",
    "                    indices_in_mlm_tokens:Tuple[torch.Tensor],\n",
    "                    predicted_token_ids:torch.Tensor,\n",
    "                    mlm_tokenizer:transformers.AutoTokenizer, \n",
    "                    lossfns:List[lossbuilder.BaseLoss],\n",
    "                    config:dict) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    A function to get hypotheses of beam size via editing beam search with reranking.\n",
    "    Run this function if config['method'] == 'mlm-beamsearch-v0' or config['method'] == 'mlm-beamsearch-v1'\n",
    "    If config['method'] == 'mlm-beamsearch-v1', rerank beam only with fluency energy.\n",
    "    If config['method'] == 'mlm-beamsearch-v0', rerank beam with a weighted sum of fluency and constraint energy.\n",
    "    \n",
    "    #ToDo\n",
    "    #Implement mlm-beamsearch-v0 with allsat-primary and compare \n",
    "    \n",
    "    params: \n",
    "        source_text: a prompt text \n",
    "        masked_sequence: token ids of original generation text with located indices masked. tokenized by MLM's tokenizer.\n",
    "        indices_in_mlm_tokens: a result of running \n",
    "                                    `indices_in_mlm_tokens = (\n",
    "                                                                inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "                                                                ).nonzero(as_tuple=True)`\n",
    "        predicted_token_ids: a result of running\n",
    "                                    `predicted_token_ids = torch.topk(\n",
    "                                                                logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "                                                                k=config['k_per_location'],\n",
    "                                                                dim=-1,).indices`\n",
    "        mlm_tokenizer: tokenizer of MLM\n",
    "        lossfns: a list of loss functions\n",
    "        config: a dictionary of configurations\n",
    "    \n",
    "    returns:\n",
    "        hypotheses: a list of a list of the beam number of hypotheses for each sample         \n",
    "    \"\"\"\n",
    "    \n",
    "    def repeat_interleave_unravel(arr,split_blocks):\n",
    "        arr_ = torch.split(arr.T,1,dim=1)\n",
    "        arr_ = [x.repeat(1,split_blocks[i]).reshape(-1,1) for i,x in enumerate(arr_)]\n",
    "        arr_ = torch.cat(arr_,dim=0)\n",
    "        return arr_\n",
    "    \n",
    "    hypotheses = list(torch.split(masked_sequence,1,dim=0)) ## [torch.tensor([[a],[b],[c]]), torch.tensor([[d]])]\n",
    "    edit_indices = sorted(list(set(indices_in_mlm_tokens[1].tolist())))\n",
    "    for curr_edit_index in edit_indices:\n",
    "        \n",
    "        batch_ids_to_edit = indices_in_mlm_tokens[0][indices_in_mlm_tokens[1]==curr_edit_index].tolist()\n",
    "        num_initial_hypotheses = [len(hypotheses[i]) for i in batch_ids_to_edit] ## keep track of initial hypotheses count e.g. [3, 1]\n",
    "        tmp_hypotheses = [hypotheses[i].repeat((config['k_per_location'],1)) for i in batch_ids_to_edit] ## [torch.tensor([[a],[b],[c],[a],[b],[c],[a],[b],[c]]), torch.tensor([[d],[d],[d]])]\n",
    "        num_initial_tmp_hypotheses = [len(x) for x in tmp_hypotheses]\n",
    "        tmp_hypotheses = torch.cat(tmp_hypotheses,dim=0) ## torch.tensor([[a],[b],[c],[a],[b],[c],[a],[b],[c],[d],[d],[d]])\n",
    "        \n",
    "\n",
    "        new_func_candidates = predicted_token_ids[indices_in_mlm_tokens[1]==curr_edit_index] ## shape: (len(batch_ids_to_edit), k_per_location) e.g. [[x,y,z],[q,w,e]]\n",
    "        new_func_candidates = repeat_interleave_unravel(new_func_candidates,num_initial_hypotheses) ## shape: (sum(num_initial_hypotheses), k_per_location) e.g. [[x],[x],[x],[y],[y],[y],[z],[z],[z],[q],[w],[e]]\n",
    "        new_func_candidates = new_func_candidates.to(config['device'])\n",
    "        \n",
    "\n",
    "        tmp_hypotheses = torch.cat((tmp_hypotheses[ :, :curr_edit_index], new_func_candidates),dim=-1) ## tmp_hypotheses: [(a,b,c),(a,b,c), ..., (a,b,c)], new_func_candidates: [(p,p,p), (q,q,q), ..., (v,v,v)]\n",
    "\n",
    "        loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "        curr_loss = torch.zeros(tmp_hypotheses.shape[0]).to(config['device'])\n",
    "        for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "            if config['method'] == 'mlm-beamsearch-v1' and lossid > 0:\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                    source_text, mlm_tokenizer.batch_decode(tmp_hypotheses,skip_special_tokens=True),\n",
    "                    label_id=config['target_label_ids'][lossid],\n",
    "                )\n",
    "            torch.cuda.empty_cache()\n",
    "            curr_loss += loss_weights[lossid] * lossvalue\n",
    "        curr_loss = torch.split(curr_loss, num_initial_tmp_hypotheses, dim=0)\n",
    "        top_beams = [torch.topk(x, k=config['beam_size'], dim=-1, largest=False).indices for x in curr_loss]\n",
    "\n",
    "        tmp_hypotheses = torch.split(tmp_hypotheses, num_initial_tmp_hypotheses, dim=0)\n",
    "        for jx, ix in enumerate(batch_ids_to_edit):\n",
    "\n",
    "            hypotheses[ix] = torch.cat([tmp_hypotheses[jx][top_beams[jx]], masked_sequence[ix][curr_edit_index+1:].unsqueeze(0).repeat(config['beam_size'],1)], dim=-1)\n",
    "            \n",
    "    return [mlm_tokenizer.batch_decode(x, skip_special_tokens=True) for x in hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## old beam rerank v1\n",
    "hypotheses_old = []\n",
    "for text_id in range(len(masked_text)):\n",
    "    hypotheses_old_ = beam_rerank_v1(source_text,\n",
    "                                inputs_old[text_id].input_ids,\n",
    "                                indices_in_mlm_tokens_old[text_id],\n",
    "                                predicted_token_ids_old[text_id],\n",
    "                                mlm_tokenizer, \n",
    "                                lossfns_old,\n",
    "                                config, \n",
    "                                beam_size = config['beam_size'])\n",
    "    hypotheses_old.append(hypotheses_old_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new \n",
    "config['method'] = 'mlm-beamsearch-v1'\n",
    "hypotheses = get_beam_hypotheses(source_text, \n",
    "                            masked_sequence, \n",
    "                            indices_in_mlm_tokens,\n",
    "                            predicted_token_ids.indices,\n",
    "                            mlm_tokenizer, \n",
    "                            lossfns,\n",
    "                            config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for comparison with v1\n",
    "config['method'] = 'mlm-beamsearch-v0'\n",
    "hypotheses_v0 = get_beam_hypotheses(source_text, \n",
    "                            masked_sequence, \n",
    "                            indices_in_mlm_tokens,\n",
    "                            predicted_token_ids.indices,\n",
    "                            mlm_tokenizer, \n",
    "                            lossfns,\n",
    "                            config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses==hypotheses_old ## v1 으로 뽑은 hypotheses에 대해서, new와 old가 같은 결과 뱉음. -> Good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses_v0 == hypotheses ## v0과 v1으로 뽑은 결과는 다르다! -> Good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' participating in a community project. AFFA is a, and a, for a.)',\n",
       "  ' participating in a community project. AFFA is a, and a, for a.,',\n",
       "  ' participating in a community project. AFFA is a, and a, for a..',\n",
       "  ' participating in a community project. AFFA is a, and the, for the.,',\n",
       "  ' participating in a community project. AFFA is a, and a, for a.]'],\n",
       " [' the fact that the word “s’ hates on ʳCriminally, the §$$$ by making \\u200ferr\\u200ft all girls,‼',\n",
       "  ' the fact that the word “s’ hates on ʳCriminally, the §$$$ by making \\u200ferr\\u200f, all girls,‼',\n",
       "  ' the fact that the word “s’ hates on ʳCriminally, in §$$$ by making \\u200ferr\\u200f, all girls,‼',\n",
       "  ' the fact that the word “s’ hates on ʳCriminally, and §$$$ by making \\u200ferr\\u200ft all girls,‼',\n",
       "  ' the fact that the word “s’ hates on ʳCriminally, and §$$$ by making \\u200ferr\\u200f, all girls,‼']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' participating in a community project. AFFA is a, and a, for a.)',\n",
       "  ' participating in a community project. AFFA is a, and a, for a.,',\n",
       "  ' participating in a community project. AFFA is a, and a, for a..',\n",
       "  ' participating in a community project. AFFA is a, and the, for the.,',\n",
       "  ' participating in a community project. AFFA is a, and a, for a.]'],\n",
       " [' the fact that the word, as is, hates on ʳCriminally, the §$$$ by making \\u200ferr\\u200ft all girls,‼',\n",
       "  ' the fact that the word, as is, hates on ʳCriminally, a §$$$ by making \\u200ferr\\u200ft all girls,‼',\n",
       "  ' the fact that the word, as is, hates on ʳCriminally, the §$$$ by making \\u200ferr\\u200f, all girls,‼',\n",
       "  ' the fact that the word, as is, hates on ʳCriminally, a §$$$ by making \\u200ferr\\u200f, all girls,‼',\n",
       "  ' the fact that the word, as is, hates on ʳCriminally, in §$$$ by making \\u200ferr\\u200ft all girls,‼']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combirerank도 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['k_per_location']=3\n",
    "config['num_edit_token_per_step']=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = get_combi_hypotheses(masked_sequence, \n",
    "                                indices_in_mlm_tokens,\n",
    "                                predicted_token_ids.indices,\n",
    "                                mlm_tokenizer,\n",
    "                                config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## old \n",
    "hypotheses_old = []\n",
    "for text_id in range(len(masked_text)):\n",
    "    hypotheses_old_ = combi_rerank(\n",
    "                                inputs_old[text_id].input_ids,\n",
    "                                indices_in_mlm_tokens_old[text_id],\n",
    "                                predicted_token_ids_old[text_id],\n",
    "                                mlm_tokenizer, \n",
    "                                config, \n",
    "                                )\n",
    "    hypotheses_old.append(hypotheses_old_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses == hypotheses_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## sorting의 문제였음. -> OK!\n",
    "sorted(hypotheses_old[0])==sorted(hypotheses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted(hypotheses_old[1])==sorted(hypotheses[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 final reranking도 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config['method'] = 'mlm-beamsearch-v1'\n",
    "hypotheses = get_beam_hypotheses_v1(source_text, \n",
    "                            masked_sequence, \n",
    "                            indices_in_mlm_tokens,\n",
    "                            predicted_token_ids.indices,\n",
    "                            mlm_tokenizer, \n",
    "                            lossfns,\n",
    "                            config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## old beam rerank v1\n",
    "hypotheses_old = []\n",
    "for text_id in range(len(masked_text)):\n",
    "    hypotheses_old_ = beam_rerank_v1(source_text,\n",
    "                                inputs_old[text_id].input_ids,\n",
    "                                indices_in_mlm_tokens_old[text_id],\n",
    "                                predicted_token_ids_old[text_id],\n",
    "                                mlm_tokenizer, \n",
    "                                lossfns_old,\n",
    "                                config, \n",
    "                                beam_size = config['beam_size'])\n",
    "    hypotheses_old.append(hypotheses_old_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses == hypotheses_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hypotheses_, new_best_weighted_loss_, new_best_allsat_, new_best_logging_loss_ = final_reranking(source_text,\n",
    "                                                                                                    hypotheses,\n",
    "                                                                                                    lossfns,\n",
    "                                                                                                    config,\n",
    "                                                                                                    batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[74.82612609863281, 0.00542679475620389], [75.46199035644531, 0.005077206529676914], [75.87005615234375, 0.005122513044625521], [76.58781433105469, 0.0029306341893970966], [76.87216186523438, 0.006557730957865715]]\n",
      "[[160.008544921875, 0.34055182337760925], [160.2454376220703, 0.36186039447784424], [161.38922119140625, 0.34558627009391785], [161.86358642578125, 0.35809406638145447], [161.68670654296875, 0.32997792959213257]]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "batch_size=64\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, hypotheses_data:List[str]):\n",
    "        self.hypotheses_data = hypotheses_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hypotheses_data)\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "        return self.hypotheses_data[idx]\n",
    "    \n",
    "    def __getitems__(self, idx:List[int]):\n",
    "        return [self.hypotheses_data[j] for j in idx]\n",
    "\n",
    "final_hypotheses = []\n",
    "best_weighted_loss = []\n",
    "best_allsat = []\n",
    "best_logging_loss = []\n",
    "\n",
    "loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "\n",
    "# for i in tqdm(range(len(hypotheses))):\n",
    "for i in range(len(hypotheses)):\n",
    "    curr_loss = torch.zeros(len(hypotheses[i])).to(config['device'])\n",
    "    logging_loss = torch.zeros((len(hypotheses[i]),2)).to(config['device'])\n",
    "    data_loader = DataLoader(CustomDataset(hypotheses[i]),batch_size=batch_size)\n",
    "\n",
    "    for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "        lossvalues=[]\n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "                lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                    source_text, batch,\n",
    "                    label_id=config['target_label_ids'][lossid],\n",
    "                )\n",
    "                lossvalues.append(lossvalue)\n",
    "                torch.cuda.empty_cache()\n",
    "        lossvalue = torch.cat(lossvalues,dim=0)\n",
    "        curr_loss += loss_weights[lossid] * lossvalue\n",
    "        logging_loss[:, lossid] = lossvalue.clone()\n",
    "    \n",
    "    print(logging_loss.tolist())\n",
    "    allsat_ix = torch.where(logging_loss[:,1]< -math.log(config[\"min_epsilons\"][0]))[0]\n",
    "    if (len(allsat_ix) > 0) and (config['selection_criteria'] == \"allsat_primary\"):\n",
    "    #if (allsat_ix.shape[0] > 0) and (config['selection_criteria'] == \"allsat_primary\"):\n",
    "        best_ix = allsat_ix[curr_loss[allsat_ix].argmin()]\n",
    "    else: ## in case config['selection_criteria'] == \"weighted_sum\" or allsat is all False\n",
    "        best_ix = torch.argmin(curr_loss)\n",
    "\n",
    "    final_hypotheses.append(hypotheses[i][best_ix])\n",
    "    best_weighted_loss.append(curr_loss[best_ix].item())\n",
    "    best_allsat.append(1 if best_ix in allsat_ix else 0)\n",
    "    best_logging_loss.append(logging_loss[best_ix].cpu().tolist())\n",
    "\n",
    "    del curr_loss, logging_loss\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "batch_size=64\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, hypotheses_data:List[str]):\n",
    "        self.hypotheses_data = hypotheses_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hypotheses_data)\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "        return self.hypotheses_data[idx]\n",
    "    \n",
    "    def __getitems__(self, idx:List[int]):\n",
    "        return [self.hypotheses_data[j] for j in idx]\n",
    "curr_loss = torch.zeros(len(hypotheses[i])).to(config['device'])\n",
    "logging_loss = torch.zeros((len(hypotheses[i]),2)).to(config['device'])\n",
    "data_loader = DataLoader(CustomDataset(hypotheses[i]),batch_size=batch_size)\n",
    "\n",
    "for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "    lossvalues=[]\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                source_text, batch,\n",
    "                label_id=config['target_label_ids'][lossid],\n",
    "            )\n",
    "            lossvalues.append(lossvalue)\n",
    "            torch.cuda.empty_cache()\n",
    "    lossvalue = torch.cat(lossvalues,dim=0)\n",
    "    curr_loss += loss_weights[lossid] * lossvalue\n",
    "    logging_loss[:, lossid] = lossvalue.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch == hypotheses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "            source_text, batch,\n",
    "            label_id=config['target_label_ids'][lossid],\n",
    "        )\n",
    "        lossvalues.append(lossvalue)\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossvalue = lossfns[0].compute_gold_loss(\n",
    "            source_text, [hypotheses[0][0]],\n",
    "            label_id=config['target_label_ids'][lossid],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[74.82614135742188]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossvalue.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[74.82612609863281,\n",
       " 75.46199035644531,\n",
       " 75.87005615234375,\n",
       " 76.58781433105469,\n",
       " 76.87216186523438]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossvalues[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[74.82612609863281, 0.00542679475620389],\n",
       " [75.46199035644531, 0.005077206529676914],\n",
       " [75.87005615234375, 0.005122513044625521],\n",
       " [76.58781433105469, 0.0029306341893970966],\n",
       " [76.87216186523438, 0.006557730957865715]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging_loss.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09999999999999998, 0.9]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' participating in a community project. AFFA is a, and a, for a.)',\n",
       " ' the fact that the word, as is, hates on ʳCriminally, the §$$$ by making \\u200ferr\\u200ft all girls,‼']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hypotheses_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' the fact that the word, as is, hates on ʳCriminally, the §$$$ by making \\u200ferr\\u200ft all girls,‼',\n",
       " ' the fact that the word, as is, hates on ʳCriminally, a §$$$ by making \\u200ferr\\u200ft all girls,‼',\n",
       " ' the fact that the word, as is, hates on ʳCriminally, the §$$$ by making \\u200ferr\\u200f, all girls,‼',\n",
       " ' the fact that the word, as is, hates on ʳCriminally, a §$$$ by making \\u200ferr\\u200f, all girls,‼',\n",
       " ' the fact that the word, as is, hates on ʳCriminally, in §$$$ by making \\u200ferr\\u200ft all girls,‼']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses_old[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = hypotheses_old[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossid = 0\n",
    "with torch.no_grad():\n",
    "    lossvalue = lossfns_old[lossid].compute_gold_loss(\n",
    "        source_text, hyp,\n",
    "        label_id=config['target_label_ids'][lossid],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.82614135742188"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossvalue.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossid = 1\n",
    "with torch.no_grad():\n",
    "    lossvalue = lossfns_old[lossid].compute_gold_loss(\n",
    "        source_text, hyp,\n",
    "        label_id=config['target_label_ids'][lossid],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3299781084060669"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossvalue.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74.82614135742188, 0.00542679475620389]\n",
      "[75.46200561523438, 0.005077206529676914]\n",
      "[75.87007141113281, 0.005122513044625521]\n",
      "[76.58781433105469, 0.0029306341893970966]\n",
      "[76.87217712402344, 0.006557730957865715]\n",
      "[160.00857543945312, 0.34055131673812866]\n",
      "[160.24545288085938, 0.3618605434894562]\n",
      "[161.38922119140625, 0.3455859124660492]\n",
      "[161.86361694335938, 0.3580939769744873]\n",
      "[161.68667602539062, 0.3299781084060669]\n"
     ]
    }
   ],
   "source": [
    "final_hypotheses_old_ = []\n",
    "new_best_weighted_loss_old_ = []\n",
    "new_best_allsat_old_ = []\n",
    "new_best_logging_loss_old_ = []\n",
    "for batch_id in range(len(hypotheses_old)):\n",
    "    candidate_total_losses = []\n",
    "    candidate_primary_losses = []\n",
    "    candidate_losses_for_loggings = []\n",
    "    candidate_allsats = []\n",
    "\n",
    "    for hyp in hypotheses_old[batch_id]:\n",
    "        curr_loss = 0.0\n",
    "        logging_loss = []\n",
    "        allsat = True\n",
    "        for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "            with torch.no_grad():\n",
    "                lossvalue = lossfns_old[lossid].compute_gold_loss(\n",
    "                    source_text, hyp,\n",
    "                    label_id=config['target_label_ids'][lossid],\n",
    "                )\n",
    "            curr_loss += loss_weights[lossid] * lossvalue.item()\n",
    "            logging_loss.append(lossvalue.item())\n",
    "            if lossid==0:\n",
    "                candidate_primary_losses.append(lossvalue.item())\n",
    "            elif (lossid >= 1) and (\n",
    "                lossvalue.item()\n",
    "                > -np.log(config[\"min_epsilons\"][lossid - 1])\n",
    "            ):\n",
    "                allsat = False\n",
    "        candidate_total_losses.append(curr_loss)\n",
    "        candidate_losses_for_loggings.append(logging_loss)\n",
    "        candidate_allsats.append(allsat)\n",
    "\n",
    "\n",
    "    if config['selection_criteria'] == \"weighted_sum\":\n",
    "        best_ix = np.argmin(np.array(candidate_total_losses))\n",
    "    elif config['selection_criteria'] == \"allsat_primary\":\n",
    "        allsat_ix = np.where(np.array(candidate_allsats) == True)[0]\n",
    "        if len(allsat_ix) > 0:\n",
    "            best_ix = np.argmin(\n",
    "                np.array(candidate_primary_losses)[allsat_ix]\n",
    "            )  # select min primary loss among allsats\n",
    "            best_ix = allsat_ix[best_ix]\n",
    "        else:  # if no candidate satisfying constraints, default to weighted_sum\n",
    "            best_ix = np.argmin(np.array(candidate_total_losses))\n",
    "    final_hypotheses_old_.append(hypotheses_old[batch_id][best_ix])\n",
    "    new_best_weighted_loss_old_.append(candidate_total_losses[best_ix])\n",
    "    new_best_allsat_old_.append(candidate_allsats[best_ix])\n",
    "    new_best_logging_loss_old_.append(candidate_losses_for_loggings[best_ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' participating in a community project. AFFA is a, and a, for a.)',\n",
       " ' the fact that the word, as is, hates on ʳCriminally, the §$$$ by making \\u200ferr\\u200ft all girls,‼']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hypotheses_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' participating in a community project. AFFA is a, and a, for a.)',\n",
       " ' the fact that the word, as is, hates on ʳCriminally, the §$$$ by making \\u200ferr\\u200ft all girls,‼']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hypotheses_old_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.487496852874756, 16.307350158691406]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_best_weighted_loss_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.48749825102277, 16.307353729009623]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_best_weighted_loss_old_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[74.82612609863281, 0.00542679475620389], [75.46199035644531, 0.005077206529676914], [75.87005615234375, 0.005122513044625521], [76.58781433105469, 0.0029306341893970966], [76.87216186523438, 0.006557730957865715]]\n",
    "[[160.008544921875, 0.34055182337760925], [160.2454376220703, 0.36186039447784424], [161.38922119140625, 0.34558627009391785], [161.86358642578125, 0.35809406638145447], [161.68670654296875, 0.32997792959213257]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossvalue = lossfns[0].compute_gold_loss(\n",
    "                    source_text, hypotheses[0],\n",
    "                    label_id=config['target_label_ids'][lossid],\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[74.82612609863281,\n",
       " 75.46199035644531,\n",
       " 75.87005615234375,\n",
       " 76.58781433105469,\n",
       " 76.87216186523438]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossvalue.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' participating in a community project. AFFA is a, and a, for a.)',\n",
       " ' the fact that the word, as is, hates on ʳCriminally, the §$$$ by making \\u200ferr\\u200ft all girls,‼']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hypotheses_old_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' participating in a community project. AFFA is a, and a, for a.)',\n",
       " ' the fact that the word, as is, hates on ʳCriminally, the §$$$ by making \\u200ferr\\u200ft all girls,‼']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hypotheses_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.48749825102277, 16.307353729009623]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_best_weighted_loss_old_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.487496852874756, 16.307350158691406]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_best_weighted_loss_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False], device='cuda:1')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_best_allsat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_best_allsat_old_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[74.82612609863281, 0.00542679475620389],\n",
       " [160.008544921875, 0.34055182337760925]]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_best_logging_loss_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[74.82614135742188, 0.00542679475620389],\n",
       " [160.00857543945312, 0.34055131673812866]]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_best_logging_loss_old_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seek extra ... ms reduction in time by getting rid of if statement within for loop (get_beam_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beam_hypotheses_v1(source_text:str, \n",
    "                    masked_sequence:torch.Tensor, \n",
    "                    indices_in_mlm_tokens:Tuple[torch.Tensor],\n",
    "                    predicted_token_ids:torch.Tensor,\n",
    "                    mlm_tokenizer:transformers.AutoTokenizer, \n",
    "                    lossfns:List[lossbuilder.BaseLoss],\n",
    "                    config:dict) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    A function to get hypotheses of beam size via editing beam search with reranking.\n",
    "    Run this function if config['method'] == 'mlm-beamsearch-v0' or config['method'] == 'mlm-beamsearch-v1'\n",
    "    If config['method'] == 'mlm-beamsearch-v1', rerank beam only with fluency energy.\n",
    "    If config['method'] == 'mlm-beamsearch-v0', rerank beam with a weighted sum of fluency and constraint energy.\n",
    "    \n",
    "    #ToDo\n",
    "    #Implement mlm-beamsearch-v0 with allsat-primary and compare \n",
    "    \n",
    "    params: \n",
    "        source_text: a prompt text \n",
    "        masked_sequence: token ids of original generation text with located indices masked. tokenized by MLM's tokenizer.\n",
    "        indices_in_mlm_tokens: a result of running \n",
    "                                    `indices_in_mlm_tokens = (\n",
    "                                                                inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "                                                                ).nonzero(as_tuple=True)`\n",
    "        predicted_token_ids: a result of running\n",
    "                                    `predicted_token_ids = torch.topk(\n",
    "                                                                logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "                                                                k=config['k_per_location'],\n",
    "                                                                dim=-1,).indices`\n",
    "        mlm_tokenizer: tokenizer of MLM\n",
    "        lossfns: a list of loss functions\n",
    "        config: a dictionary of configurations\n",
    "    \n",
    "    returns:\n",
    "        hypotheses: a list of a list of the beam number of hypotheses for each sample         \n",
    "    \"\"\"\n",
    "    \n",
    "    def repeat_interleave_unravel(arr,split_blocks):\n",
    "        arr_ = torch.split(arr.T,1,dim=1)\n",
    "        arr_ = [x.repeat(1,split_blocks[i]).reshape(-1,1) for i,x in enumerate(arr_)]\n",
    "        arr_ = torch.cat(arr_,dim=0)\n",
    "        return arr_\n",
    "    \n",
    "    hypotheses = list(torch.split(masked_sequence,1,dim=0)) ## [torch.tensor([[a],[b],[c]]), torch.tensor([[d]])]\n",
    "    edit_indices = sorted(list(set(indices_in_mlm_tokens[1].tolist())))\n",
    "    for curr_edit_index in edit_indices:\n",
    "        \n",
    "        batch_ids_to_edit = indices_in_mlm_tokens[0][indices_in_mlm_tokens[1]==curr_edit_index].tolist()\n",
    "        num_initial_hypotheses = [len(hypotheses[i]) for i in batch_ids_to_edit] ## keep track of initial hypotheses count e.g. [3, 1]\n",
    "        tmp_hypotheses = [hypotheses[i].repeat((config['k_per_location'],1)) for i in batch_ids_to_edit] ## [torch.tensor([[a],[b],[c],[a],[b],[c],[a],[b],[c]]), torch.tensor([[d],[d],[d]])]\n",
    "        num_initial_tmp_hypotheses = [len(x) for x in tmp_hypotheses]\n",
    "        tmp_hypotheses = torch.cat(tmp_hypotheses,dim=0) ## torch.tensor([[a],[b],[c],[a],[b],[c],[a],[b],[c],[d],[d],[d]])\n",
    "        \n",
    "\n",
    "        new_func_candidates = predicted_token_ids[indices_in_mlm_tokens[1]==curr_edit_index] ## shape: (len(batch_ids_to_edit), k_per_location) e.g. [[x,y,z],[q,w,e]]\n",
    "        new_func_candidates = repeat_interleave_unravel(new_func_candidates,num_initial_hypotheses) ## shape: (sum(num_initial_hypotheses), k_per_location) e.g. [[x],[x],[x],[y],[y],[y],[z],[z],[z],[q],[w],[e]]\n",
    "        new_func_candidates = new_func_candidates.to(config['device'])\n",
    "        \n",
    "\n",
    "        tmp_hypotheses = torch.cat((tmp_hypotheses[ :, :curr_edit_index], new_func_candidates),dim=-1) ## tmp_hypotheses: [(a,b,c),(a,b,c), ..., (a,b,c)], new_func_candidates: [(p,p,p), (q,q,q), ..., (v,v,v)]\n",
    "\n",
    "        # loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "        # curr_loss = torch.zeros(tmp_hypotheses.shape[0]).to(config['device'])\n",
    "        lossid = 0\n",
    "        # for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "        #     if config['method'] == 'mlm-beamsearch-v1' and lossid > 0:\n",
    "        #         continue\n",
    "        #     with torch.no_grad():\n",
    "        #         lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "        #             source_text, mlm_tokenizer.batch_decode(tmp_hypotheses,skip_special_tokens=True),\n",
    "        #             label_id=config['target_label_ids'][lossid],\n",
    "        #         )\n",
    "        #     torch.cuda.empty_cache()\n",
    "        #     curr_loss += loss_weights[lossid] * lossvalue\n",
    "        with torch.no_grad():\n",
    "            lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                source_text, mlm_tokenizer.batch_decode(tmp_hypotheses,skip_special_tokens=True),\n",
    "                label_id=config['target_label_ids'][lossid],\n",
    "            )\n",
    "        torch.cuda.empty_cache()\n",
    "        # curr_loss += loss_weights[lossid] * lossvalue\n",
    "        curr_loss = torch.split(lossvalue, num_initial_tmp_hypotheses, dim=0)\n",
    "        top_beams = [torch.topk(x, k=config['beam_size'], dim=-1, largest=False).indices for x in curr_loss]\n",
    "\n",
    "        tmp_hypotheses = torch.split(tmp_hypotheses, num_initial_tmp_hypotheses, dim=0)\n",
    "        for jx, ix in enumerate(batch_ids_to_edit):\n",
    "\n",
    "            hypotheses[ix] = torch.cat([tmp_hypotheses[jx][top_beams[jx]], masked_sequence[ix][curr_edit_index+1:].unsqueeze(0).repeat(config['beam_size'],1)], dim=-1)\n",
    "            \n",
    "    return [mlm_tokenizer.batch_decode(x, skip_special_tokens=True) for x in hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beam_hypotheses_v0(source_text:str, \n",
    "                    masked_sequence:torch.Tensor, \n",
    "                    indices_in_mlm_tokens:Tuple[torch.Tensor],\n",
    "                    predicted_token_ids:torch.Tensor,\n",
    "                    mlm_tokenizer:transformers.AutoTokenizer, \n",
    "                    lossfns:List[lossbuilder.BaseLoss],\n",
    "                    config:dict) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    A function to get hypotheses of beam size via editing beam search with reranking.\n",
    "    Run this function if config['method'] == 'mlm-beamsearch-v0' or config['method'] == 'mlm-beamsearch-v1'\n",
    "    If config['method'] == 'mlm-beamsearch-v1', rerank beam only with fluency energy.\n",
    "    If config['method'] == 'mlm-beamsearch-v0', rerank beam with a weighted sum of fluency and constraint energy.\n",
    "    \n",
    "    #ToDo\n",
    "    #Implement mlm-beamsearch-v0 with allsat-primary and compare \n",
    "    \n",
    "    params: \n",
    "        source_text: a prompt text \n",
    "        masked_sequence: token ids of original generation text with located indices masked. tokenized by MLM's tokenizer.\n",
    "        indices_in_mlm_tokens: a result of running \n",
    "                                    `indices_in_mlm_tokens = (\n",
    "                                                                inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "                                                                ).nonzero(as_tuple=True)`\n",
    "        predicted_token_ids: a result of running\n",
    "                                    `predicted_token_ids = torch.topk(\n",
    "                                                                logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "                                                                k=config['k_per_location'],\n",
    "                                                                dim=-1,).indices`\n",
    "        mlm_tokenizer: tokenizer of MLM\n",
    "        lossfns: a list of loss functions\n",
    "        config: a dictionary of configurations\n",
    "    \n",
    "    returns:\n",
    "        hypotheses: a list of a list of the beam number of hypotheses for each sample         \n",
    "    \"\"\"\n",
    "    \n",
    "    def repeat_interleave_unravel(arr,split_blocks):\n",
    "        arr_ = torch.split(arr.T,1,dim=1)\n",
    "        arr_ = [x.repeat(1,split_blocks[i]).reshape(-1,1) for i,x in enumerate(arr_)]\n",
    "        arr_ = torch.cat(arr_,dim=0)\n",
    "        return arr_\n",
    "    \n",
    "    hypotheses = list(torch.split(masked_sequence,1,dim=0)) ## [torch.tensor([[a],[b],[c]]), torch.tensor([[d]])]\n",
    "    edit_indices = sorted(list(set(indices_in_mlm_tokens[1].tolist())))\n",
    "    for curr_edit_index in edit_indices:\n",
    "        \n",
    "        batch_ids_to_edit = indices_in_mlm_tokens[0][indices_in_mlm_tokens[1]==curr_edit_index].tolist()\n",
    "        num_initial_hypotheses = [len(hypotheses[i]) for i in batch_ids_to_edit] ## keep track of initial hypotheses count e.g. [3, 1]\n",
    "        tmp_hypotheses = [hypotheses[i].repeat((config['k_per_location'],1)) for i in batch_ids_to_edit] ## [torch.tensor([[a],[b],[c],[a],[b],[c],[a],[b],[c]]), torch.tensor([[d],[d],[d]])]\n",
    "        num_initial_tmp_hypotheses = [len(x) for x in tmp_hypotheses]\n",
    "        tmp_hypotheses = torch.cat(tmp_hypotheses,dim=0) ## torch.tensor([[a],[b],[c],[a],[b],[c],[a],[b],[c],[d],[d],[d]])\n",
    "        \n",
    "\n",
    "        new_func_candidates = predicted_token_ids[indices_in_mlm_tokens[1]==curr_edit_index] ## shape: (len(batch_ids_to_edit), k_per_location) e.g. [[x,y,z],[q,w,e]]\n",
    "        new_func_candidates = repeat_interleave_unravel(new_func_candidates,num_initial_hypotheses) ## shape: (sum(num_initial_hypotheses), k_per_location) e.g. [[x],[x],[x],[y],[y],[y],[z],[z],[z],[q],[w],[e]]\n",
    "        new_func_candidates = new_func_candidates.to(config['device'])\n",
    "        \n",
    "\n",
    "        tmp_hypotheses = torch.cat((tmp_hypotheses[ :, :curr_edit_index], new_func_candidates),dim=-1) ## tmp_hypotheses: [(a,b,c),(a,b,c), ..., (a,b,c)], new_func_candidates: [(p,p,p), (q,q,q), ..., (v,v,v)]\n",
    "\n",
    "        loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "        curr_loss = torch.zeros(tmp_hypotheses.shape[0]).to(config['device'])\n",
    "        for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "            with torch.no_grad():\n",
    "                lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                    source_text, mlm_tokenizer.batch_decode(tmp_hypotheses,skip_special_tokens=True),\n",
    "                    label_id=config['target_label_ids'][lossid],\n",
    "                )\n",
    "            torch.cuda.empty_cache()\n",
    "            curr_loss += loss_weights[lossid] * lossvalue\n",
    "        curr_loss = torch.split(curr_loss, num_initial_tmp_hypotheses, dim=0)\n",
    "        top_beams = [torch.topk(x, k=config['beam_size'], dim=-1, largest=False).indices for x in curr_loss]\n",
    "\n",
    "        tmp_hypotheses = torch.split(tmp_hypotheses, num_initial_tmp_hypotheses, dim=0)\n",
    "        for jx, ix in enumerate(batch_ids_to_edit):\n",
    "\n",
    "            hypotheses[ix] = torch.cat([tmp_hypotheses[jx][top_beams[jx]], masked_sequence[ix][curr_edit_index+1:].unsqueeze(0).repeat(config['beam_size'],1)], dim=-1)\n",
    "            \n",
    "    return [mlm_tokenizer.batch_decode(x, skip_special_tokens=True) for x in hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2 s ± 12.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "config['method'] = 'mlm-beamsearch-v0'\n",
    "hypotheses = get_beam_hypotheses(source_text, \n",
    "                            masked_sequence, \n",
    "                            indices_in_mlm_tokens,\n",
    "                            predicted_token_ids.indices,\n",
    "                            mlm_tokenizer, \n",
    "                            lossfns,\n",
    "                            config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.58 s ± 19.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "config['method'] = 'mlm-beamsearch-v1'\n",
    "hypotheses = get_beam_hypotheses(source_text, \n",
    "                            masked_sequence, \n",
    "                            indices_in_mlm_tokens,\n",
    "                            predicted_token_ids.indices,\n",
    "                            mlm_tokenizer, \n",
    "                            lossfns,\n",
    "                            config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9 s ± 15.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "config['method'] = 'mlm-beamsearch-v0'\n",
    "hypotheses = get_beam_hypotheses_v0(source_text, \n",
    "                            masked_sequence, \n",
    "                            indices_in_mlm_tokens,\n",
    "                            predicted_token_ids.indices,\n",
    "                            mlm_tokenizer, \n",
    "                            lossfns,\n",
    "                            config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.59 s ± 5.35 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "config['method'] = 'mlm-beamsearch-v1'\n",
    "hypotheses = get_beam_hypotheses_v1(source_text, \n",
    "                            masked_sequence, \n",
    "                            indices_in_mlm_tokens,\n",
    "                            predicted_token_ids.indices,\n",
    "                            mlm_tokenizer, \n",
    "                            lossfns,\n",
    "                            config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc-edit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

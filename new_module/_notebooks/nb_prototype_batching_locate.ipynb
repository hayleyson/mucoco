{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import argparse\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "os.chdir('/data/hyeryung/mucoco')\n",
    "from itertools import repeat\n",
    "import torch.multiprocessing as mp\n",
    "from nb_test_module import Processor, f\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from new_module.utils.robertacustom import RobertaCustomForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(ckpt_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='abc'\n",
    "prediction=['dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(prediction, add_special_tokens=False, padding=True, truncation=True, return_tensors=\"pt\").to(device) # prediction이 list여도 처리가능함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch, output_attentions=True, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = output.attentions\n",
    "num_layer = 10\n",
    "max_num_tokens = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실제 locate하는 로직이 나오는 부분\n",
    "punctuations = string.punctuation + '\\n '\n",
    "punctuations = list(punctuations)\n",
    "punctuations.remove('-')\n",
    "stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in tokenizer.special_tokens_map.values()]\n",
    "stopwords_ids = tokenizer.batch_encode_plus(stopwords, return_tensors=\"pt\",add_special_tokens=False)['input_ids'].squeeze().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## attentions : tuple of length num hidden layers\n",
    "## attentions[i] : attention value of ith hidden layer of shape (batch, num_heads, query, value)\n",
    "lengths = batch.attention_mask.sum(dim=-1)\n",
    "# 보고자 하는 attention layer 만 가져옴\n",
    "attentions = attentions[\n",
    "    num_layer # originally 10\n",
    "]\n",
    "cls_attns = attentions.max(1)[0][:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## avg_value만 구하면 된다고 한다면, 이렇게도 가능하다.\n",
    "## NOTE: softmax를 안했음\n",
    "cls_attns[torch.isin(batch.input_ids, stopwords_ids)]=0.0\n",
    "no_punc_len=(~torch.isin(batch.input_ids, stopwords_ids)).sum(dim=-1) # tensor([2, 4], device='cuda:0')\n",
    "avg_values=cls_attns.sum(dim=-1)/no_punc_len # tensor([0.5120, 0.3744], device='cuda:0', grad_fn=<DivBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False, False]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## stopwords가 아닌 부분에서만 index를 찾아야 한다.\n",
    "cls_attns[torch.isin(batch.input_ids, stopwords_ids)]=-float(\"inf\") ## stopwords 인부분을 -inf로 세팅하면, avg랑 비교할 때랑 topk를 찾을때 빠질것으로 예상\n",
    "top_masks = (cls_attns >= avg_values.unsqueeze(1))# unsqueeze to allow implicit broadcasting : (N) -> (N, 1) -> (N, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_located_tokens = torch.minimum((lengths//3), torch.LongTensor([max_num_tokens]).to(device))\n",
    "max_num_located_tokens = torch.minimum(max_num_located_tokens, top_masks.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_masks_final = [x[:max_num_located_tokens[i]] for i,x in enumerate(cls_attns.argsort(dim=-1,descending=True).tolist())] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit=\"word\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unit == \"token\":\n",
    "    locate_ixes=top_masks_final\n",
    "\n",
    "elif unit == \"word\":\n",
    "    proc=Processor(tokenizer)\n",
    "\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "    with mp.Pool(3) as pool:\n",
    "        # https://stackoverflow.com/questions/5442910/how-to-use-multiprocessing-pool-map-with-multiple-arguments\n",
    "        locate_ixes = pool.starmap(f, zip(prediction,batch.input_ids.tolist(), lengths.tolist(), top_masks_final,repeat(proc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 µs ± 22.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "# [x[:max_num_located_tokens[i]] for i,x in enumerate(cls_attns.argsort(dim=-1,descending=True).tolist())] \n",
    "## 이 implementation이 더 빠르다\n",
    "# 375 µs ± 22.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
    "\n",
    "# from multiprocessing import Pool\n",
    "# def f(x, length):\n",
    "#     return x[:length]\n",
    "# os.environ['TOKENIZERS_PARALLELISM']='true'\n",
    "# %%timeit\n",
    "\n",
    "# with Pool(3) as pool:\n",
    "#     L = pool.starmap(f, zip(cls_attns.argsort(dim=-1,descending=True).tolist(), max_num_located_tokens.tolist()))\n",
    "# 346 ms ± 26.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint/'\n",
    "\n",
    "prompt='abc'\n",
    "prediction=['dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe']\n",
    "\n",
    "unit=\"word\"\n",
    "num_layer = 10\n",
    "max_num_tokens = 6\n",
    "device='cuda'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckpt_path)\n",
    "model = model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n",
    "\n",
    "punctuations = string.punctuation + '\\n '\n",
    "punctuations = list(punctuations)\n",
    "punctuations.remove('-')\n",
    "stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in tokenizer.special_tokens_map.values()]\n",
    "stopwords_ids = tokenizer.batch_encode_plus(stopwords, return_tensors=\"pt\",add_special_tokens=False)['input_ids'].squeeze().to(device)\n",
    "\n",
    "###\n",
    "batch = tokenizer(prediction, add_special_tokens=False, padding=True, truncation=True, return_tensors=\"pt\").to(device) # prediction이 list여도 처리가능함\n",
    "output = model(**batch, output_attentions=True, output_hidden_states=True)\n",
    "attentions = output.attentions\n",
    "\n",
    "## 실제 locate하는 로직이 나오는 부분\n",
    "## attentions : tuple of length num hidden layers\n",
    "## attentions[i] : attention value of ith hidden layer of shape (batch, num_heads, query, value)\n",
    "lengths = batch.attention_mask.sum(dim=-1)\n",
    "attentions = attentions[num_layer]\n",
    "cls_attns = attentions.max(1)[0][:, 0] # cls_attns's dimension: (N, L)\n",
    "\n",
    "## avg_value만 구하면 된다고 한다면, 이렇게도 가능하다.\n",
    "## NOTE: softmax를 안했음\n",
    "cls_attns[torch.isin(batch.input_ids, stopwords_ids)]=0.0\n",
    "no_punc_len=(~torch.isin(batch.input_ids, stopwords_ids)).sum(dim=-1) # tensor([2, 4], device='cuda:0')\n",
    "avg_values=cls_attns.sum(dim=-1)/no_punc_len # tensor([0.5120, 0.3744], device='cuda:0', grad_fn=<DivBackward0>)\n",
    "\n",
    "## stopwords가 아닌 부분에서만 index를 찾아야 한다.\n",
    "cls_attns[torch.isin(batch.input_ids, stopwords_ids)]=-float(\"inf\") ## stopwords 인부분을 -inf로 세팅하면, avg랑 비교할 때랑 topk를 찾을때 빠질것으로 예상\n",
    "top_masks = (cls_attns >= avg_values.unsqueeze(1))# unsqueeze to allow implicit broadcasting : (N) -> (N, 1) -> (N, L)\n",
    "max_num_located_tokens = torch.minimum((lengths//3), torch.LongTensor([max_num_tokens]).to(device))\n",
    "max_num_located_tokens = torch.minimum(max_num_located_tokens, top_masks.sum(dim=-1))\n",
    "top_masks_final = [x[:max_num_located_tokens[i]] for i,x in enumerate(cls_attns.argsort(dim=-1,descending=True).tolist())] \n",
    "\n",
    "if unit == \"token\":\n",
    "    locate_ixes=top_masks_final\n",
    "\n",
    "elif unit == \"word\":\n",
    "    proc=Processor(tokenizer)\n",
    "\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "    with mp.Pool(3) as pool:\n",
    "        # https://stackoverflow.com/questions/5442910/how-to-use-multiprocessing-pool-map-with-multiple-arguments\n",
    "        locate_ixes = pool.starmap(f, zip(prediction,batch.input_ids.tolist(), lengths.tolist(), top_masks_final,repeat(proc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = 0\n",
    "\n",
    "## output['hidden_states']: tuple of length num_hidden_layers\n",
    "## output['hidden_states'][0]: (batch_size, seq_len, hidden_size)\n",
    "layer = output['hidden_states'][0]\n",
    "layer.retain_grad()\n",
    "\n",
    "## output['logits'] : (batch_size, num_labels)\n",
    "softmax=torch.nn.Softmax(dim=-1)\n",
    "probs = softmax(output['logits'])[:, label_id]\n",
    "\n",
    "# print(f\"probs.shape:{probs.shape}\")\n",
    "\n",
    "probs.sum().backward(retain_graph=True) ## NOTE. https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments/47026836#47026836\n",
    "\n",
    "## layer.grad : (batch_size, seq_len, hidden_size)\n",
    "# print(f\"layer.grad.shape:{layer.grad.shape}\")\n",
    "norm = torch.norm(layer.grad, dim=-1)\n",
    "\n",
    "## norm : (batch_size, seq_len)\n",
    "# print(f\"norm.shape:{norm.shape}\")\n",
    "norm = torch.where(norm > 0, norm, torch.full_like(norm, 1e-10))\n",
    "# print(f\"norm:{norm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint/'\n",
    "\n",
    "prompt='abc'\n",
    "prediction=['dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe']\n",
    "\n",
    "unit=\"word\"\n",
    "num_layer = 10\n",
    "max_num_tokens = 6\n",
    "device='cuda'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckpt_path)\n",
    "model = model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n",
    "\n",
    "punctuations = string.punctuation + '\\n '\n",
    "punctuations = list(punctuations)\n",
    "punctuations.remove('-')\n",
    "stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in tokenizer.special_tokens_map.values()]\n",
    "stopwords_ids = tokenizer.batch_encode_plus(stopwords, return_tensors=\"pt\",add_special_tokens=False)['input_ids'].squeeze().to(device)\n",
    "\n",
    "###\n",
    "batch = tokenizer(prediction, add_special_tokens=False, padding=True, truncation=True, return_tensors=\"pt\").to(device) # prediction이 list여도 처리가능함\n",
    "output = model(**batch, output_attentions=True, output_hidden_states=True)\n",
    "attentions = output.attentions\n",
    "\n",
    "## 실제 locate하는 로직이 나오는 부분\n",
    "## attentions : tuple of length num hidden layers\n",
    "## attentions[i] : attention value of ith hidden layer of shape (batch, num_heads, query, value)\n",
    "lengths = batch.attention_mask.sum(dim=-1)\n",
    "attentions = attentions[num_layer]\n",
    "cls_attns = attentions.max(1)[0][:, 0] # cls_attns's dimension: (N, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_attns=norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocateMachine:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def locate_main(self, prediction: List[str], method, max_num_tokens = 6, unit=\"word\", device=\"cuda\", **kwargs):\n",
    "        \n",
    "        punctuations = string.punctuation + '\\n '\n",
    "        punctuations = list(punctuations)\n",
    "        punctuations.remove('-')\n",
    "        stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in self.tokenizer.special_tokens_map.values()]\n",
    "        stopwords_ids = self.tokenizer.batch_encode_plus(stopwords, return_tensors=\"pt\",add_special_tokens=False)['input_ids'].squeeze().to(device)\n",
    "        batch = self.tokenizer(prediction, add_special_tokens=False, padding=True, truncation=True, return_tensors=\"pt\").to(device) # prediction이 list여도 처리가능함\n",
    "        \n",
    "        if method == \"attention\":\n",
    "            output = self.model(**batch, output_attentions=True)\n",
    "            attentions = output.attentions\n",
    "            ## attentions : tuple of length num hidden layers\n",
    "            ## attentions[i] : attention value of ith hidden layer of shape (batch, num_heads, query, value)\n",
    "            lengths = batch.attention_mask.sum(dim=-1)\n",
    "            attentions = attentions[kwargs['num_layer']]\n",
    "            token_wise_scores = attentions.max(1)[0][:, 0] # cls_attns's dimension: (N, L)\n",
    "        elif method == \"grad_norm\":\n",
    "            output = self.model(**batch, output_hidden_states=True)\n",
    "            ## output['hidden_states']: tuple of length num_hidden_layers\n",
    "            ## output['hidden_states'][0]: (batch_size, seq_len, hidden_size)\n",
    "            layer = output['hidden_states'][0]\n",
    "            layer.retain_grad()\n",
    "\n",
    "            softmax=torch.nn.Softmax(dim=-1)\n",
    "            probs = softmax(output['logits'])[:, kwargs['label_id']]\n",
    "            probs.sum().backward(retain_graph=True) ## NOTE. https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments/47026836#47026836\n",
    "            ## layer.grad : (batch_size, seq_len, hidden_size)\n",
    "            norm = torch.norm(layer.grad, dim=-1)\n",
    "            ## norm : (batch_size, seq_len)\n",
    "            token_wise_scores = torch.where(norm > 0, norm, torch.full_like(norm, 1e-10))\n",
    "        \n",
    "        ## avg_value만 구하면 된다고 한다면, 이렇게도 가능하다.\n",
    "        ## NOTE: softmax를 안했음\n",
    "        token_wise_scores[torch.isin(batch.input_ids, stopwords_ids)]=0.0\n",
    "        no_punc_len=(~torch.isin(batch.input_ids, stopwords_ids)).sum(dim=-1) # tensor([2, 4], device='cuda:0')\n",
    "        avg_values=token_wise_scores.sum(dim=-1)/no_punc_len # tensor([0.5120, 0.3744], device='cuda:0', grad_fn=<DivBackward0>)\n",
    "\n",
    "        ## stopwords가 아닌 부분에서만 index를 찾아야 한다.\n",
    "        token_wise_scores[torch.isin(batch.input_ids, stopwords_ids)]=-float(\"inf\") ## stopwords 인부분을 -inf로 세팅하면, avg랑 비교할 때랑 topk를 찾을때 빠질것으로 예상\n",
    "        top_masks = (token_wise_scores >= avg_values.unsqueeze(1))# unsqueeze to allow implicit broadcasting : (N) -> (N, 1) -> (N, L)\n",
    "        max_num_located_tokens = torch.minimum((lengths//3), torch.LongTensor([max_num_tokens]).to(device))\n",
    "        max_num_located_tokens = torch.minimum(max_num_located_tokens, top_masks.sum(dim=-1))\n",
    "        top_masks_final = [x[:max_num_located_tokens[i]] for i,x in enumerate(token_wise_scores.argsort(dim=-1,descending=True).tolist())] \n",
    "\n",
    "        if unit == \"token\":\n",
    "            locate_ixes=top_masks_final\n",
    "\n",
    "        elif unit == \"word\":\n",
    "            proc=Processor(self.tokenizer)\n",
    "\n",
    "            try:\n",
    "                mp.set_start_method('spawn', force=True)\n",
    "            except RuntimeError:\n",
    "                pass\n",
    "\n",
    "            with mp.Pool(3) as pool:\n",
    "                # https://stackoverflow.com/questions/5442910/how-to-use-multiprocessing-pool-map-with-multiple-arguments\n",
    "                locate_ixes = pool.starmap(f, zip(prediction,batch.input_ids.tolist(), lengths.tolist(), top_masks_final,repeat(proc)))\n",
    "        \n",
    "        return locate_ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsxe<s> [11622, 42336, 0, 1] 3 [0] <nb_test_module.Processor object at 0x7f1724ef3d00>\n",
      "sdvbfe [28045, 705, 428, 7068] 4 [3] <nb_test_module.Processor object at 0x7f3662327d00>\n",
      "dsxe<s> [11622, 42336, 0, 1] 3 [0] <nb_test_module.Processor object at 0x7ff7ed375d00>\n",
      "sdvbfe [28045, 705, 428, 7068] 4 [3] <nb_test_module.Processor object at 0x7f170b936190>\n",
      "dsxe<s> [11622, 42336, 0, 1] 3 [0] <nb_test_module.Processor object at 0x7f170b9e1eb0>\n",
      "sdvbfe [28045, 705, 428, 7068] 4 [3] <nb_test_module.Processor object at 0x7f364cd7e190>\n",
      "dsxe<s> [11622, 42336, 0, 1] 3 [0] <nb_test_module.Processor object at 0x7ff7d3dc8190>\n",
      "sdvbfe [28045, 705, 428, 7068] 4 [3] <nb_test_module.Processor object at 0x7f170b936190>\n",
      "dsxe<s> [11622, 42336, 0, 1] 3 [0] <nb_test_module.Processor object at 0x7f364ce29eb0>\n",
      "sdvbfe [28045, 705, 428, 7068] 4 [3] <nb_test_module.Processor object at 0x7ff7d8067eb0>\n"
     ]
    }
   ],
   "source": [
    "## avg_value만 구하면 된다고 한다면, 이렇게도 가능하다.\n",
    "## NOTE: softmax를 안했음\n",
    "cls_attns[torch.isin(batch.input_ids, stopwords_ids)]=0.0\n",
    "no_punc_len=(~torch.isin(batch.input_ids, stopwords_ids)).sum(dim=-1) # tensor([2, 4], device='cuda:0')\n",
    "avg_values=cls_attns.sum(dim=-1)/no_punc_len # tensor([0.5120, 0.3744], device='cuda:0', grad_fn=<DivBackward0>)\n",
    "\n",
    "## stopwords가 아닌 부분에서만 index를 찾아야 한다.\n",
    "cls_attns[torch.isin(batch.input_ids, stopwords_ids)]=-float(\"inf\") ## stopwords 인부분을 -inf로 세팅하면, avg랑 비교할 때랑 topk를 찾을때 빠질것으로 예상\n",
    "top_masks = (cls_attns >= avg_values.unsqueeze(1))# unsqueeze to allow implicit broadcasting : (N) -> (N, 1) -> (N, L)\n",
    "max_num_located_tokens = torch.minimum((lengths//3), torch.LongTensor([max_num_tokens]).to(device))\n",
    "max_num_located_tokens = torch.minimum(max_num_located_tokens, top_masks.sum(dim=-1))\n",
    "top_masks_final = [x[:max_num_located_tokens[i]] for i,x in enumerate(cls_attns.argsort(dim=-1,descending=True).tolist())] \n",
    "\n",
    "if unit == \"token\":\n",
    "    locate_ixes=top_masks_final\n",
    "\n",
    "elif unit == \"word\":\n",
    "    proc=Processor(tokenizer)\n",
    "\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "    with mp.Pool(3) as pool:\n",
    "        # https://stackoverflow.com/questions/5442910/how-to-use-multiprocessing-pool-map-with-multiple-arguments\n",
    "        locate_ixes = pool.starmap(f, zip(prediction,batch.input_ids.tolist(), lengths.tolist(), top_masks_final,repeat(proc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lengths = [i.tolist().count(1) for i in batch[\"attention_mask\"]]\n",
    "# print(f\"lengths: {lengths}\")\n",
    "\n",
    "locate_ixes = []\n",
    "locate_scores = []\n",
    "for i in range(batch[\"input_ids\"].shape[0]):\n",
    "    \n",
    "    ## norm_ : (seq_len,)\n",
    "    current_norm = norm[i, :]\n",
    "    # print(f\"norm_ shape: {current_norm.shape}\")\n",
    "    \n",
    "    ## current_sent : (lengths[i], )\n",
    "    current_sent = batch[\"input_ids\"][i][: lengths[i]]\n",
    "    no_punc_indices = torch.where(~torch.isin(current_sent, torch.tensor(stopwords_ids).to(device)))[0]\n",
    "    \n",
    "    # print(f\"current_sent: {current_sent}\")\n",
    "    # print(f\"len(current_sent), lengths[i]: {len(current_sent), lengths[i]}\")\n",
    "    # print(f\"no_punc_indices: {no_punc_indices}\")\n",
    "    # print(f\"current_sent[no_punc_indices]: {current_sent[no_punc_indices]}\")\n",
    "    # print(f\"tokenizer.decode(current_sent[no_punc_indices]): {tokenizer.decode(current_sent[no_punc_indices])}\")\n",
    "    \n",
    "    ## normalize current_norm\n",
    "    current_norm = current_norm[: lengths[i]].softmax(-1) \n",
    "    # print(f\"current_norm after normalizing: {current_norm}\")\n",
    "    \n",
    "    current_locate_scores = torch.zeros_like(current_norm)\n",
    "    current_locate_scores[no_punc_indices] = current_norm[no_punc_indices].clone()\n",
    "    locate_scores.append(current_locate_scores.cpu().detach().tolist())\n",
    "    \n",
    "    current_norm = current_norm[no_punc_indices]\n",
    "    # print(f\"current_norm after selecting non stop words indices: {current_norm}\")\n",
    "    \n",
    "    ## calculate mean value within the sequence\n",
    "    avg_value = current_norm.view(-1).mean().item()\n",
    "    # print(\"avg_value\", avg_value)\n",
    "    \n",
    "    ## find indices of tokens whose norm value is greater than the mean value\n",
    "    top_masks = ((current_norm >= avg_value).nonzero().view(-1)) \n",
    "    top_masks = top_masks.cpu().tolist()\n",
    "    torch.cuda.empty_cache()\n",
    "    # print(\"indices of non stopwords tokens whose grad norm value is greater than the mean value\", top_masks)\n",
    "    \n",
    "    ## in case the number of above average gradient norm tokens is greater than the max_num_tokens or 1/3 of the lengths[i]\n",
    "    if len(top_masks) > min((lengths[i]) // 3, max_num_tokens):\n",
    "        # print(\"len(located_tokens) exceeds max_num_tokens or 1/3 of the lengths[i]. Taking top k.\")\n",
    "        top_masks = (\n",
    "            current_norm.topk(max(min((lengths[i]) // 3, max_num_tokens), 1))[1]\n",
    "        )\n",
    "        top_masks = top_masks.cpu().tolist()\n",
    "        # print(\"indices of non stopwords tokens located after taking top k\", top_masks)\n",
    "    \n",
    "    top_masks = no_punc_indices[top_masks].cpu().detach().tolist()\n",
    "    # print(\"indices of tokens located\", top_masks)\n",
    "    \n",
    "    if unit == \"token\":\n",
    "        locate_ixes.append(list(set(top_masks)))\n",
    "    \n",
    "    elif unit == \"word\":\n",
    "\n",
    "        ## group token indices that belong to the same word\n",
    "        words = tokenizer.decode(current_sent).strip().split()\n",
    "        word2tok_mapper=Processor(tokenizer)\n",
    "        # print(f\"input to word2tok: {pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})}\")\n",
    "        grouped_tokens = list(word2tok_mapper.get_word2tok(pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})).values())            # j, k = 0, 0\n",
    "        # grouped_tokens = []\n",
    "        # grouped_tokens_for_word = []\n",
    "        # while j < len(current_sent):\n",
    "        #     if (tokenizer.decode(current_sent[j]).strip() not in stopwords):\n",
    "        #         while k < len(words):\n",
    "        #             if tokenizer.decode(current_sent[j]).strip() in words[k]:\n",
    "        #                 grouped_tokens_for_word.append(j)\n",
    "        #                 break\n",
    "        #             else:\n",
    "        #                 grouped_tokens.append(grouped_tokens_for_word)\n",
    "        #                 grouped_tokens_for_word = []\n",
    "        #                 k += 1\n",
    "        #     j += 1\n",
    "        # grouped_tokens.append(grouped_tokens_for_word)\n",
    "        \n",
    "        ## expand located token indices to include adjacent token indices that belong to the same word as already located tokens\n",
    "        top_masks.sort()\n",
    "        top_masks_final = set()\n",
    "        for index in top_masks:\n",
    "            if index not in top_masks_final:\n",
    "                word = [grouped_ixes for grouped_ixes in grouped_tokens if index in grouped_ixes]\n",
    "                # print(\"word\", word)\n",
    "                if len(word) > 0:\n",
    "                    word = set(word[0])\n",
    "                else:\n",
    "                    print(f\"warning. {index} not in the word groups. decoded value: {tokenizer.decode(index)}\")\n",
    "                    word = set([index])\n",
    "                top_masks_final |= word\n",
    "        locate_ixes.append(sorted(list(top_masks_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint/'\n",
    "\n",
    "prompt='abc'\n",
    "prediction=['dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe']\n",
    "\n",
    "unit=\"word\"\n",
    "num_layer = 10\n",
    "max_num_tokens = 6\n",
    "device='cuda'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckpt_path)\n",
    "model = model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n",
    "\n",
    "punctuations = string.punctuation + '\\n '\n",
    "punctuations = list(punctuations)\n",
    "punctuations.remove('-')\n",
    "stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in tokenizer.special_tokens_map.values()]\n",
    "stopwords_ids = tokenizer.batch_encode_plus(stopwords, return_tensors=\"pt\",add_special_tokens=False)['input_ids'].squeeze().to(device)\n",
    "\n",
    "###\n",
    "batch = tokenizer(prediction, add_special_tokens=False, padding=True, truncation=True, return_tensors=\"pt\").to(device) # prediction이 list여도 처리가능함\n",
    "output = model(**batch, output_attentions=True, output_hidden_states=True)\n",
    "attentions = output.attentions\n",
    "\n",
    "## 실제 locate하는 로직이 나오는 부분\n",
    "## attentions : tuple of length num hidden layers\n",
    "## attentions[i] : attention value of ith hidden layer of shape (batch, num_heads, query, value)\n",
    "lengths = batch.attention_mask.sum(dim=-1)\n",
    "attentions = attentions[num_layer]\n",
    "cls_attns = attentions.max(1)[0][:, 0] # cls_attns's dimension: (N, L)\n",
    "\n",
    "## avg_value만 구하면 된다고 한다면, 이렇게도 가능하다.\n",
    "## NOTE: softmax를 안했음\n",
    "cls_attns[torch.isin(batch.input_ids, stopwords_ids)]=0.0\n",
    "no_punc_len=(~torch.isin(batch.input_ids, stopwords_ids)).sum(dim=-1) # tensor([2, 4], device='cuda:0')\n",
    "avg_values=cls_attns.sum(dim=-1)/no_punc_len # tensor([0.5120, 0.3744], device='cuda:0', grad_fn=<DivBackward0>)\n",
    "\n",
    "## stopwords가 아닌 부분에서만 index를 찾아야 한다.\n",
    "cls_attns[torch.isin(batch.input_ids, stopwords_ids)]=-float(\"inf\") ## stopwords 인부분을 -inf로 세팅하면, avg랑 비교할 때랑 topk를 찾을때 빠질것으로 예상\n",
    "top_masks = (cls_attns >= avg_values.unsqueeze(1))# unsqueeze to allow implicit broadcasting : (N) -> (N, 1) -> (N, L)\n",
    "max_num_located_tokens = torch.minimum((lengths//3), torch.LongTensor([max_num_tokens]).to(device))\n",
    "max_num_located_tokens = torch.minimum(max_num_located_tokens, top_masks.sum(dim=-1))\n",
    "top_masks_final = [x[:max_num_located_tokens[i]] for i,x in enumerate(cls_attns.argsort(dim=-1,descending=True).tolist())] \n",
    "\n",
    "if unit == \"token\":\n",
    "    locate_ixes=top_masks_final\n",
    "\n",
    "elif unit == \"word\":\n",
    "    proc=Processor(tokenizer)\n",
    "\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "    with mp.Pool(3) as pool:\n",
    "        # https://stackoverflow.com/questions/5442910/how-to-use-multiprocessing-pool-map-with-multiple-arguments\n",
    "        locate_ixes = pool.starmap(f, zip(prediction,batch.input_ids.tolist(), lengths.tolist(), top_masks_final,repeat(proc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_grad_norm(prediction, output, tokenizer, batch, max_num_tokens = 6, unit=\"word\", device=\"cuda\", label_id = 1):\n",
    "\n",
    "    punctuations = string.punctuation + '\\n '\n",
    "    punctuations = list(punctuations)\n",
    "    punctuations.remove('-')\n",
    "    stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in tokenizer.special_tokens_map.values()]\n",
    "    stopwords_ids = [tokenizer.encode(word,add_special_tokens=False)[-1] for word in stopwords]\n",
    "\n",
    "    ## output['hidden_states']: tuple of length num_hidden_layers\n",
    "    ## output['hidden_states'][0]: (batch_size, seq_len, hidden_size)\n",
    "    layer = output['hidden_states'][0]\n",
    "    layer.retain_grad()\n",
    "    \n",
    "    ## output['logits'] : (batch_size, num_labels)\n",
    "    softmax=torch.nn.Softmax(dim=-1)\n",
    "    probs = softmax(output['logits'])[:, label_id]\n",
    "    # print(f\"probs.shape:{probs.shape}\")\n",
    "    \n",
    "    probs.sum().backward(retain_graph=True) ## NOTE. https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments/47026836#47026836\n",
    "\n",
    "    ## layer.grad : (batch_size, seq_len, hidden_size)\n",
    "    # print(f\"layer.grad.shape:{layer.grad.shape}\")\n",
    "    norm = torch.norm(layer.grad, dim=-1)\n",
    "    ## norm : (batch_size, seq_len)\n",
    "    # print(f\"norm.shape:{norm.shape}\")\n",
    "    norm = torch.where(norm > 0, norm, torch.full_like(norm, 1e-10))\n",
    "    # print(f\"norm:{norm}\")\n",
    "    \n",
    "    lengths = [i.tolist().count(1) for i in batch[\"attention_mask\"]]\n",
    "    # print(f\"lengths: {lengths}\")\n",
    "    \n",
    "    locate_ixes = []\n",
    "    locate_scores = []\n",
    "    for i in range(batch[\"input_ids\"].shape[0]):\n",
    "        \n",
    "        ## norm_ : (seq_len,)\n",
    "        current_norm = norm[i, :]\n",
    "        # print(f\"norm_ shape: {current_norm.shape}\")\n",
    "        \n",
    "        ## current_sent : (lengths[i], )\n",
    "        current_sent = batch[\"input_ids\"][i][: lengths[i]]\n",
    "        no_punc_indices = torch.where(~torch.isin(current_sent, torch.tensor(stopwords_ids).to(device)))[0]\n",
    "        \n",
    "        # print(f\"current_sent: {current_sent}\")\n",
    "        # print(f\"len(current_sent), lengths[i]: {len(current_sent), lengths[i]}\")\n",
    "        # print(f\"no_punc_indices: {no_punc_indices}\")\n",
    "        # print(f\"current_sent[no_punc_indices]: {current_sent[no_punc_indices]}\")\n",
    "        # print(f\"tokenizer.decode(current_sent[no_punc_indices]): {tokenizer.decode(current_sent[no_punc_indices])}\")\n",
    "        \n",
    "        ## normalize current_norm\n",
    "        current_norm = current_norm[: lengths[i]].softmax(-1) \n",
    "        # print(f\"current_norm after normalizing: {current_norm}\")\n",
    "        \n",
    "        current_locate_scores = torch.zeros_like(current_norm)\n",
    "        current_locate_scores[no_punc_indices] = current_norm[no_punc_indices].clone()\n",
    "        locate_scores.append(current_locate_scores.cpu().detach().tolist())\n",
    "        \n",
    "        current_norm = current_norm[no_punc_indices]\n",
    "        # print(f\"current_norm after selecting non stop words indices: {current_norm}\")\n",
    "        \n",
    "        ## calculate mean value within the sequence\n",
    "        avg_value = current_norm.view(-1).mean().item()\n",
    "        # print(\"avg_value\", avg_value)\n",
    "        \n",
    "        ## find indices of tokens whose norm value is greater than the mean value\n",
    "        top_masks = ((current_norm >= avg_value).nonzero().view(-1)) \n",
    "        top_masks = top_masks.cpu().tolist()\n",
    "        torch.cuda.empty_cache()\n",
    "        # print(\"indices of non stopwords tokens whose grad norm value is greater than the mean value\", top_masks)\n",
    "        \n",
    "        ## in case the number of above average gradient norm tokens is greater than the max_num_tokens or 1/3 of the lengths[i]\n",
    "        if len(top_masks) > min((lengths[i]) // 3, max_num_tokens):\n",
    "            # print(\"len(located_tokens) exceeds max_num_tokens or 1/3 of the lengths[i]. Taking top k.\")\n",
    "            top_masks = (\n",
    "                current_norm.topk(max(min((lengths[i]) // 3, max_num_tokens), 1))[1]\n",
    "            )\n",
    "            top_masks = top_masks.cpu().tolist()\n",
    "            # print(\"indices of non stopwords tokens located after taking top k\", top_masks)\n",
    "        \n",
    "        top_masks = no_punc_indices[top_masks].cpu().detach().tolist()\n",
    "        # print(\"indices of tokens located\", top_masks)\n",
    "        \n",
    "        if unit == \"token\":\n",
    "            locate_ixes.append(list(set(top_masks)))\n",
    "        \n",
    "        elif unit == \"word\":\n",
    "\n",
    "            ## group token indices that belong to the same word\n",
    "            words = tokenizer.decode(current_sent).strip().split()\n",
    "            word2tok_mapper=Processor(tokenizer)\n",
    "            # print(f\"input to word2tok: {pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})}\")\n",
    "            grouped_tokens = list(word2tok_mapper.get_word2tok(pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})).values())            # j, k = 0, 0\n",
    "            # grouped_tokens = []\n",
    "            # grouped_tokens_for_word = []\n",
    "            # while j < len(current_sent):\n",
    "            #     if (tokenizer.decode(current_sent[j]).strip() not in stopwords):\n",
    "            #         while k < len(words):\n",
    "            #             if tokenizer.decode(current_sent[j]).strip() in words[k]:\n",
    "            #                 grouped_tokens_for_word.append(j)\n",
    "            #                 break\n",
    "            #             else:\n",
    "            #                 grouped_tokens.append(grouped_tokens_for_word)\n",
    "            #                 grouped_tokens_for_word = []\n",
    "            #                 k += 1\n",
    "            #     j += 1\n",
    "            # grouped_tokens.append(grouped_tokens_for_word)\n",
    "            \n",
    "            ## expand located token indices to include adjacent token indices that belong to the same word as already located tokens\n",
    "            top_masks.sort()\n",
    "            top_masks_final = set()\n",
    "            for index in top_masks:\n",
    "                if index not in top_masks_final:\n",
    "                    word = [grouped_ixes for grouped_ixes in grouped_tokens if index in grouped_ixes]\n",
    "                    # print(\"word\", word)\n",
    "                    if len(word) > 0:\n",
    "                        word = set(word[0])\n",
    "                    else:\n",
    "                        print(f\"warning. {index} not in the word groups. decoded value: {tokenizer.decode(index)}\")\n",
    "                        word = set([index])\n",
    "                    top_masks_final |= word\n",
    "            locate_ixes.append(sorted(list(top_masks_final)))\n",
    "\n",
    "            \n",
    "    return locate_ixes, locate_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "if method == \"attention\":\n",
    "    output = model(**batch, output_attentions=True) # batch여도 처리가능함\n",
    "    locate_ixes, locate_scores = locate_attn(output.attentions, tokenizer, batch, max_num_tokens, unit, device, kwargs['num_layer'])\n",
    "elif method == \"grad_norm\":\n",
    "    output = model(**batch, output_hidden_states=True) # batch여도 처리가능함\n",
    "    locate_ixes, locate_scores = locate_grad_norm(output, tokenizer, batch, max_num_tokens, unit, device, kwargs['label_id'])\n",
    "\n",
    "## assuming batch_size = 1\n",
    "masked_sequence = batch['input_ids'].clone().detach()\n",
    "masked_sequence[:, locate_ixes[0]] = tokenizer.mask_token_id ## 이부분에 대해서 처리방식 고민 필요\n",
    "masked_sequence_text = tokenizer.batch_decode(\n",
    "    masked_sequence.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_attn(prediction, attentions, tokenizer, batch, max_num_tokens = 6, unit=\"word\", device=\"cuda\", num_layer=10):\n",
    "\n",
    "    punctuations = string.punctuation + '\\n '\n",
    "    punctuations = list(punctuations)\n",
    "    punctuations.remove('-')\n",
    "\n",
    "    ## attentions : tuple of length num hidden layers\n",
    "    ## attentions[i] : attention value of ith hidden layer of shape (batch, num_heads, query, value)\n",
    "    lengths = [i.tolist().count(1) for i in batch[\"attention_mask\"]]\n",
    "    # 보고자 하는 attention layer 만 가져옴\n",
    "    attentions = attentions[\n",
    "        num_layer # originally 10\n",
    "    ]\n",
    "    # print(attentions.shape)\n",
    "    # print(attentions.max(1)[0].shape)\n",
    "    # print( batch[\"input_ids\"].shape)\n",
    "    # print( batch[\"attention_mask\"][0,:])\n",
    "    cls_attns = attentions.max(1)[0][:, 0]\n",
    "    \n",
    "    stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in tokenizer.special_tokens_map.values()]\n",
    "    stopwords_ids = [tokenizer.encode(word,add_special_tokens=False)[-1] for word in stopwords]\n",
    "    # print(\"stopwords_ids\", torch.tensor(stopwords_ids))\n",
    "\n",
    "    locate_ixes=[]\n",
    "    locate_scores = []\n",
    "    for i, attn in enumerate(cls_attns):\n",
    "        \n",
    "        # print(\"attn.shape\", attn.shape)\n",
    "        current_sent = batch[\"input_ids\"][i][: lengths[i]]\n",
    "        # print(\"current_sent\", current_sent)\n",
    "        if device == \"cuda\":\n",
    "            no_punc_indices = torch.where(~torch.isin(current_sent, torch.tensor(stopwords_ids).to(torch.device('cuda'))))[0]\n",
    "        else:\n",
    "            no_punc_indices = torch.where(~torch.isin(current_sent, torch.tensor(stopwords_ids)))[0]\n",
    "        # print(\"no_punc_indices\", no_punc_indices)\n",
    "        # print(f\"current_sent[no_punc_indices]: {current_sent[no_punc_indices]}\")\n",
    "        # print(f\"tokenizer.decode(current_sent[no_punc_indices]): {tokenizer.decode(current_sent[no_punc_indices])}\")\n",
    "        \n",
    "        # current tokenizer does not add <s> and </s> to the sentence.\n",
    "        current_attn = attn[: lengths[i]].softmax(-1) \n",
    "        \n",
    "        current_locate_scores = torch.zeros_like(current_attn)\n",
    "        current_locate_scores[no_punc_indices] = current_attn[no_punc_indices].clone()\n",
    "        locate_scores.append(current_locate_scores.cpu().detach().tolist())\n",
    "        \n",
    "        # print(\"current_attn\", current_attn)\n",
    "        current_attn = current_attn[no_punc_indices]\n",
    "        # print(\"current_attn\", current_attn)\n",
    "        \n",
    "        # 이 값의 평균을 구한다.\n",
    "        avg_value = current_attn.view(-1).mean().item()\n",
    "        # print(\"avg_value\", avg_value)\n",
    "        # 이 값 중에 평균보다 큰 값을 지니는 위치를 찾는다.\n",
    "        # fixed to reflect that sometimes the sequence length is 1.\n",
    "        top_masks = ((current_attn >= avg_value).nonzero().view(-1)) \n",
    "        torch.cuda.empty_cache()\n",
    "        top_masks = top_masks.cpu().tolist()\n",
    "        # print(\"top_masks\", top_masks)\n",
    "        \n",
    "        \n",
    "        # attention 값이 평균보다 큰 토큰의 수가 k개 또는 문장 전체 토큰 수의 1/3 보다 크면  \n",
    "        if len(top_masks) > min((lengths[i]) // 3, max_num_tokens):\n",
    "            # 그냥 attention 값 기준 k 개 또는 토큰 수/3 중 작은 수를 뽑는다.\n",
    "            top_masks = (\n",
    "                current_attn.topk(max(min((lengths[i]) // 3, max_num_tokens), 1))[1]\n",
    "            )\n",
    "            top_masks = top_masks.cpu().tolist()\n",
    "            # print(\"top k top_masks\", top_masks)\n",
    "        top_masks_final = no_punc_indices[top_masks]\n",
    "        # print(\"top_masks_final\", top_masks_final)\n",
    "        if unit == \"token\":\n",
    "            locate_ixes.append(list(set(top_masks_final.cpu().detach().tolist())))\n",
    "        \n",
    "        elif unit == \"word\":\n",
    "            # word의 일부만 locate 한 경우, word 전체를 locate 한다.\n",
    "            # 같은 word 안에 있는 token 끼리 묶음.\n",
    "            words = tokenizer.decode(current_sent).strip().split()\n",
    "            # print(\"words\", words)\n",
    "            word2tok_mapper=Processor(tokenizer)\n",
    "            # print(f\"input to word2tok: {pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})}\")\n",
    "            grouped_tokens = list(word2tok_mapper.get_word2tok(pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})).values())\n",
    "            # j, k = 0, 0\n",
    "            # grouped_tokens = []\n",
    "            # grouped_tokens_for_word = []\n",
    "            # while j < len(current_sent):\n",
    "            #     if (tokenizer.decode(current_sent[j]).strip() not in stopwords):\n",
    "            #         # print(\"tokenizer.decode(current_sent[j])\", tokenizer.decode(current_sent[j]))\n",
    "            #         while k < len(words):\n",
    "            #             if tokenizer.decode(current_sent[j]).strip() in words[k]:\n",
    "            #                 grouped_tokens_for_word.append(j)\n",
    "            #                 break\n",
    "            #             else:\n",
    "            #                 grouped_tokens.append(grouped_tokens_for_word)\n",
    "            #                 grouped_tokens_for_word = []\n",
    "            #                 k += 1\n",
    "            #     j += 1\n",
    "            # grouped_tokens.append(grouped_tokens_for_word)\n",
    "            # print(\"grouped_tokens\", grouped_tokens)\n",
    "            \n",
    "            top_masks_final.sort()\n",
    "            top_masks_final_final = []\n",
    "            for index in top_masks_final:\n",
    "                # print(\"index\", index)\n",
    "                if index not in top_masks_final_final:\n",
    "                    word = [grouped_ixes for grouped_ixes in grouped_tokens if index in grouped_ixes]\n",
    "                    # print(\"word\", word)\n",
    "                    if len(word) > 0:\n",
    "                        word = word[0]\n",
    "                    else:\n",
    "                        print(f\"!!! {index} not in the grouped_ixes {grouped_tokens}\")\n",
    "                        print(f\"!!! tokenizer.decode(index): {tokenizer.decode(index)}\")\n",
    "                    top_masks_final_final.extend(word)\n",
    "            locate_ixes.append(list(set(top_masks_final_final)))\n",
    "\n",
    "            \n",
    "    return locate_ixes, locate_scores\n",
    "\n",
    "def locate_grad_norm(prediction, output, tokenizer, batch, max_num_tokens = 6, unit=\"word\", device=\"cuda\", label_id = 1):\n",
    "\n",
    "    punctuations = string.punctuation + '\\n '\n",
    "    punctuations = list(punctuations)\n",
    "    punctuations.remove('-')\n",
    "    stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in tokenizer.special_tokens_map.values()]\n",
    "    stopwords_ids = [tokenizer.encode(word,add_special_tokens=False)[-1] for word in stopwords]\n",
    "\n",
    "    ## output['hidden_states']: tuple of length num_hidden_layers\n",
    "    ## output['hidden_states'][0]: (batch_size, seq_len, hidden_size)\n",
    "    layer = output['hidden_states'][0]\n",
    "    layer.retain_grad()\n",
    "    \n",
    "    ## output['logits'] : (batch_size, num_labels)\n",
    "    softmax=torch.nn.Softmax(dim=-1)\n",
    "    probs = softmax(output['logits'])[:, label_id]\n",
    "    # print(f\"probs.shape:{probs.shape}\")\n",
    "    \n",
    "    probs.sum().backward(retain_graph=True) ## NOTE. https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments/47026836#47026836\n",
    "\n",
    "    ## layer.grad : (batch_size, seq_len, hidden_size)\n",
    "    # print(f\"layer.grad.shape:{layer.grad.shape}\")\n",
    "    norm = torch.norm(layer.grad, dim=-1)\n",
    "    ## norm : (batch_size, seq_len)\n",
    "    # print(f\"norm.shape:{norm.shape}\")\n",
    "    norm = torch.where(norm > 0, norm, torch.full_like(norm, 1e-10))\n",
    "    # print(f\"norm:{norm}\")\n",
    "    \n",
    "    lengths = [i.tolist().count(1) for i in batch[\"attention_mask\"]]\n",
    "    # print(f\"lengths: {lengths}\")\n",
    "    \n",
    "    locate_ixes = []\n",
    "    locate_scores = []\n",
    "    for i in range(batch[\"input_ids\"].shape[0]):\n",
    "        \n",
    "        ## norm_ : (seq_len,)\n",
    "        current_norm = norm[i, :]\n",
    "        # print(f\"norm_ shape: {current_norm.shape}\")\n",
    "        \n",
    "        ## current_sent : (lengths[i], )\n",
    "        current_sent = batch[\"input_ids\"][i][: lengths[i]]\n",
    "        no_punc_indices = torch.where(~torch.isin(current_sent, torch.tensor(stopwords_ids).to(device)))[0]\n",
    "        \n",
    "        # print(f\"current_sent: {current_sent}\")\n",
    "        # print(f\"len(current_sent), lengths[i]: {len(current_sent), lengths[i]}\")\n",
    "        # print(f\"no_punc_indices: {no_punc_indices}\")\n",
    "        # print(f\"current_sent[no_punc_indices]: {current_sent[no_punc_indices]}\")\n",
    "        # print(f\"tokenizer.decode(current_sent[no_punc_indices]): {tokenizer.decode(current_sent[no_punc_indices])}\")\n",
    "        \n",
    "        ## normalize current_norm\n",
    "        current_norm = current_norm[: lengths[i]].softmax(-1) \n",
    "        # print(f\"current_norm after normalizing: {current_norm}\")\n",
    "        \n",
    "        current_locate_scores = torch.zeros_like(current_norm)\n",
    "        current_locate_scores[no_punc_indices] = current_norm[no_punc_indices].clone()\n",
    "        locate_scores.append(current_locate_scores.cpu().detach().tolist())\n",
    "        \n",
    "        current_norm = current_norm[no_punc_indices]\n",
    "        # print(f\"current_norm after selecting non stop words indices: {current_norm}\")\n",
    "        \n",
    "        ## calculate mean value within the sequence\n",
    "        avg_value = current_norm.view(-1).mean().item()\n",
    "        # print(\"avg_value\", avg_value)\n",
    "        \n",
    "        ## find indices of tokens whose norm value is greater than the mean value\n",
    "        top_masks = ((current_norm >= avg_value).nonzero().view(-1)) \n",
    "        top_masks = top_masks.cpu().tolist()\n",
    "        torch.cuda.empty_cache()\n",
    "        # print(\"indices of non stopwords tokens whose grad norm value is greater than the mean value\", top_masks)\n",
    "        \n",
    "        ## in case the number of above average gradient norm tokens is greater than the max_num_tokens or 1/3 of the lengths[i]\n",
    "        if len(top_masks) > min((lengths[i]) // 3, max_num_tokens):\n",
    "            # print(\"len(located_tokens) exceeds max_num_tokens or 1/3 of the lengths[i]. Taking top k.\")\n",
    "            top_masks = (\n",
    "                current_norm.topk(max(min((lengths[i]) // 3, max_num_tokens), 1))[1]\n",
    "            )\n",
    "            top_masks = top_masks.cpu().tolist()\n",
    "            # print(\"indices of non stopwords tokens located after taking top k\", top_masks)\n",
    "        \n",
    "        top_masks = no_punc_indices[top_masks].cpu().detach().tolist()\n",
    "        # print(\"indices of tokens located\", top_masks)\n",
    "        \n",
    "        if unit == \"token\":\n",
    "            locate_ixes.append(list(set(top_masks)))\n",
    "        \n",
    "        elif unit == \"word\":\n",
    "\n",
    "            ## group token indices that belong to the same word\n",
    "            words = tokenizer.decode(current_sent).strip().split()\n",
    "            word2tok_mapper=Processor(tokenizer)\n",
    "            # print(f\"input to word2tok: {pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})}\")\n",
    "            grouped_tokens = list(word2tok_mapper.get_word2tok(pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})).values())            # j, k = 0, 0\n",
    "            # grouped_tokens = []\n",
    "            # grouped_tokens_for_word = []\n",
    "            # while j < len(current_sent):\n",
    "            #     if (tokenizer.decode(current_sent[j]).strip() not in stopwords):\n",
    "            #         while k < len(words):\n",
    "            #             if tokenizer.decode(current_sent[j]).strip() in words[k]:\n",
    "            #                 grouped_tokens_for_word.append(j)\n",
    "            #                 break\n",
    "            #             else:\n",
    "            #                 grouped_tokens.append(grouped_tokens_for_word)\n",
    "            #                 grouped_tokens_for_word = []\n",
    "            #                 k += 1\n",
    "            #     j += 1\n",
    "            # grouped_tokens.append(grouped_tokens_for_word)\n",
    "            \n",
    "            ## expand located token indices to include adjacent token indices that belong to the same word as already located tokens\n",
    "            top_masks.sort()\n",
    "            top_masks_final = set()\n",
    "            for index in top_masks:\n",
    "                if index not in top_masks_final:\n",
    "                    word = [grouped_ixes for grouped_ixes in grouped_tokens if index in grouped_ixes]\n",
    "                    # print(\"word\", word)\n",
    "                    if len(word) > 0:\n",
    "                        word = set(word[0])\n",
    "                    else:\n",
    "                        print(f\"warning. {index} not in the word groups. decoded value: {tokenizer.decode(index)}\")\n",
    "                        word = set([index])\n",
    "                    top_masks_final |= word\n",
    "            locate_ixes.append(sorted(list(top_masks_final)))\n",
    "\n",
    "            \n",
    "    return locate_ixes, locate_scores\n",
    "\n",
    "def locate_main(prediction: List[str], method, model, tokenizer, max_num_tokens = 6, unit=\"word\", device=\"cuda\", **kwargs): #label_id = 1, num_layer=10):\n",
    "# def locate_main(prompt, prediction, method, model, tokenizer, max_num_tokens = 6, unit=\"word\", device=\"cuda\", **kwargs): #label_id = 1, num_layer=10):\n",
    "    \n",
    "#     ## prompt에 대한 처리를 어떻게 할지 고민이 필요\n",
    "#     if isinstance(prompt, list) and isinstance(prediction, list):\n",
    "#         prompt_prediction = [f\"{p}{g}\" for p, g in zip(prompt, prediction)]\n",
    "#     else:\n",
    "#         prompt_prediction = [f\"{prompt}{prediction}\"]\n",
    "#     batch = tokenizer(prompt_prediction, add_special_tokens=False, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    batch = tokenizer(prediction, add_special_tokens=False, padding=True, truncation=True, return_tensors=\"pt\").to(device) # prediction이 list여도 처리가능함\n",
    "    \n",
    "    if method == \"attention\":\n",
    "        output = model(**batch, output_attentions=True) # batch여도 처리가능함\n",
    "        locate_ixes, locate_scores = locate_attn(prediction, output.attentions, tokenizer, batch, max_num_tokens, unit, device, kwargs['num_layer'])\n",
    "    elif method == \"grad_norm\":\n",
    "        output = model(**batch, output_hidden_states=True) # batch여도 처리가능함\n",
    "        locate_ixes, locate_scores = locate_grad_norm(prediction, output, tokenizer, batch, max_num_tokens, unit, device, kwargs['label_id'])\n",
    "    \n",
    "    ## assuming batch_size = 1\n",
    "    masked_sequence = batch['input_ids'].clone().detach()\n",
    "    masked_sequence[:, locate_ixes[0]] = tokenizer.mask_token_id ## 이부분에 대해서 처리방식 고민 필요\n",
    "    masked_sequence_text = tokenizer.batch_decode(\n",
    "        masked_sequence.tolist()\n",
    "    )\n",
    "    return masked_sequence_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"\"\"Program to locate spans that contributed the most to the prediction of a model\\n\\\n",
    "Input format: jsonl or csv with a column named \"text\" containing the text to be analyzed\\n\\\n",
    "Output format: input dataframe with a new column named \"located_indices\" each of which is a list of indices of tokens. e.g. [2,3,4,8,10]\\n\\\n",
    "\"\"\")\n",
    "    parser.add_argument(\"--method\", type=str, choices=[\"attention\",\"grad_norm\"], help=\"method to use for locating tokens to edit\")\n",
    "    parser.add_argument(\"--input_path\", type=str, help=\"path to input file\")\n",
    "    parser.add_argument(\"--output_path\", type=str, help=\"path to output file\")\n",
    "    parser.add_argument(\"--model_name_or_path\", type=str, help=\"name of model to use or path to the checkpoint to use\")\n",
    "    parser.add_argument(\"--model_type\", type=str, choices=[\"AutoModelForSequenceClassification\", \"RobertaCustomForSequenceClassification\"], help=\"name of model to use\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"batch size to use\")\n",
    "    parser.add_argument(\"--text_column_name\", type=str, default=\"text\", help=\"name of the column containing text for analysis\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    if args.model_type == \"AutoModelForSequenceClassification\":\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(args.model_name_or_path)\n",
    "    elif args.model_type == \"RobertaCustomForSequenceClassification\":\n",
    "        model = RobertaCustomForSequenceClassification.from_pretrained(args.model_name_or_path)\n",
    "    model.to(device)\n",
    "        \n",
    "    if args.input_path.endswith(\".jsonl\"):\n",
    "        data = pd.read_json(args.input_path, lines=True)\n",
    "    elif args.input_path.endswith(\".csv\"):\n",
    "        data = pd.read_csv(args.input_path)\n",
    "        \n",
    "    if \"gpt2\" in args.input_path: ## unravel data\n",
    "        print(\"GPT2 in args.input_path\")\n",
    "        ## unravel the file \n",
    "        data['prompt']=data['prompt'].apply(lambda x: x['text'])\n",
    "        data = data.explode('generations')\n",
    "\n",
    "        data['text']=data['generations'].apply(lambda x: x['text'])\n",
    "        data['tokens']=data['generations'].apply(lambda x: x['tokens'])\n",
    "        data['locate_labels']=data['generations'].apply(lambda x: x.get('locate_labels', np.nan))\n",
    "        data = data.dropna(subset=['locate_labels'])\n",
    "        \n",
    "        del data['generations']\n",
    "        del data['locate_labels']\n",
    "        print(data.head())\n",
    "    \n",
    "    dataset = Dataset.from_pandas(data)\n",
    "    if \"gpt2\" in args.input_path:\n",
    "        print(\"GPT2 in args.input_path\")\n",
    "        def collate_fn(batch):\n",
    "            input_ids = pad_sequence([torch.LongTensor(example[\"tokens\"]) for example in batch], padding_value=tokenizer.pad_token_id, batch_first=True) \n",
    "            # print(f\"input_ids: {input_ids}\")\n",
    "            batch = {\"input_ids\": input_ids,\n",
    "                    \"attention_mask\": (input_ids != tokenizer.pad_token_id).long()}\n",
    "            return transformers.tokenization_utils_base.BatchEncoding(batch)\n",
    "    else:\n",
    "        def collate_fn(batch):\n",
    "            batch = tokenizer([example[args.text_column_name] for example in batch], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            return batch\n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "    \n",
    "    pred_indices = []\n",
    "    pred_scores = []\n",
    "    for batch in dataloader:\n",
    "        batch.to(device)\n",
    "        results, scores = locate_main(args.method, model, tokenizer, batch, max_num_tokens = 6, num_layer=10, unit=\"word\", use_cuda=True)\n",
    "        pred_indices.extend(results)\n",
    "        pred_scores.extend(scores)\n",
    "    \n",
    "    data[f'pred_indices_{args.method}'] = pred_indices\n",
    "    data[f'pred_scores_{args.method}'] = pred_scores\n",
    "    os.makedirs(os.path.dirname(args.output_path), exist_ok=True)\n",
    "    data.to_json(args.output_path, lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from itertools import chain\n",
    "import math\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "os.chdir('/data/hyeryung/mucoco')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "from copy import deepcopy\n",
    "import new_module.losses as lossbuilder\n",
    "import new_module.losses_old as lossbuilder_old\n",
    "import wandb\n",
    "# from new_module.decode_utils import (\n",
    "#     beam_rerank_v0,\n",
    "#     beam_rerank_v1,\n",
    "#     beam_rerank_v2,\n",
    "#     combi_rerank,\n",
    "# )\n",
    "# from new_module.new_decode_utils import get_beam_hypotheses, get_combi_hypotheses, final_reranking\n",
    "from new_module.evaluation.evaluate_wandb import evaluate_main\n",
    "from new_module.locate.new_locate_utils import LocateMachine\n",
    "from new_module.locate.locate_utils import locate_main\n",
    "from new_module.utils.robertacustom import RobertaCustomForSequenceClassification\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(os.environ.get(\"LOGGING_LEVEL\", logging.DEBUG))\n",
    "import joblib\n",
    "config = joblib.load('config.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 1830\n",
      "https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 377\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhayleyson\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Popen(['git', 'cat-file', '--batch-check'], cwd=/data/hyeryung/mucoco, stdin=<valid stream>, shell=False, universal_newlines=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/hyeryung/mucoco/wandb/run-20240411_025950-iaw0i3hv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hayleyson/toxicity-decoding/runs/iaw0i3hv' target=\"_blank\">pious-pine-188</a></strong> to <a href='https://wandb.ai/hayleyson/toxicity-decoding' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hayleyson/toxicity-decoding' target=\"_blank\">https://wandb.ai/hayleyson/toxicity-decoding</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hayleyson/toxicity-decoding/runs/iaw0i3hv' target=\"_blank\">https://wandb.ai/hayleyson/toxicity-decoding/runs/iaw0i3hv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/generation_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "main_start_time = time.time()\n",
    "\n",
    "if not config.get(\"model_tag\", None):\n",
    "    if \"energy-training\" in config[\"model_paths\"][1]:\n",
    "        config[\"model_tag\"] = \"em\"\n",
    "    else:\n",
    "        config[\"model_tag\"] = \"clsf\"\n",
    "\n",
    "    if (config[\"task\"] == \"formality\") and (\"gyafc\" in config[\"model_paths\"][1]):\n",
    "        config[\"model_tag\"] += \"-gyafc\"\n",
    "\n",
    "if config[\"resume\"]:\n",
    "    logger.info(\"resuming from a previous run\")\n",
    "    run = wandb.init(\n",
    "        project=config[\"wandb_project\"],\n",
    "        entity=config[\"wandb_entity\"],\n",
    "        id=config[\"wandb_run_id\"],\n",
    "        resume=\"must\",\n",
    "    )\n",
    "else:\n",
    "    run = wandb.init(\n",
    "        project=config[\"wandb_project\"],\n",
    "        entity=config[\"wandb_entity\"],\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "run_id = run.path.split(\"/\")[-1]\n",
    "display_name = f\"{run_id}\"\n",
    "\n",
    "\n",
    "outdir = os.path.join(config[\"output_dir_prefix\"], display_name)\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "outfile = f\"{outdir}/outputs_epsilon{config['min_epsilons'][0]}.txt\"\n",
    "run.summary[\"outfile_path\"] = outfile\n",
    "\n",
    "\n",
    "class dummyArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "build_loss_args = dummyArgs(**config[\"build_loss_dict\"])\n",
    "\n",
    "## load data\n",
    "if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "    source_dataset = [\n",
    "        json.loads(l)[config[\"jsonl_primary_key\"]][config[\"jsonl_secondary_key\"]]\n",
    "        for l in open(config[\"source_data\"])\n",
    "    ]\n",
    "    generation_dataset = [\n",
    "        json.loads(l)[\"generations\"] for l in open(config[\"source_data\"])\n",
    "    ]\n",
    "elif (config[\"task\"] == \"formality\") or (config[\"task\"] == \"sentiment-lewis-compr\"):\n",
    "    with open(config[\"source_data\"], \"r\") as f:\n",
    "        generation_dataset = [line.rstrip('\\n') for line in f.readlines()]\n",
    "    source_dataset = [\"\" for l in generation_dataset]\n",
    "\n",
    "# check if outfile exists\n",
    "if (config[\"resume\"]) and (os.path.exists(outfile)):\n",
    "\n",
    "    with open(outfile, \"r\") as f:\n",
    "        existing_gens = [x.rstrip(\"\\n\") for x in f.readlines()]\n",
    "    resume_idx = len(existing_gens)\n",
    "    if resume_idx == len(source_dataset):\n",
    "        logger.debug(\"output file is already complete. skipping this run.\")\n",
    "        raise\n",
    "    elif resume_idx < len(source_dataset):\n",
    "        logger.info(\n",
    "            f\"output file already exists but is incomplete. resuming from index: {resume_idx}\"\n",
    "        )\n",
    "        outf = open(outfile, \"a\")\n",
    "        int_outf = open(outfile+\".intermediate\", \"a\")\n",
    "    else:\n",
    "        logger.critical(\n",
    "            f\"output file seems to be corrupted. The file length is {resume_idx}, where the size of source_dataset is {len(source_dataset)}\"\n",
    "        )\n",
    "        raise\n",
    "else:\n",
    "    resume_idx = 0\n",
    "    outf = open(outfile, \"w\")\n",
    "    int_outf = open(outfile+\".intermediate\", \"w\")\n",
    "\n",
    "\n",
    "## load tokenizer, models, define losses\n",
    "name2tokenizer = {}\n",
    "name2model = {}\n",
    "name2config = {}\n",
    "loss2tokenizer = {}\n",
    "embed_luts = []\n",
    "\n",
    "for i, model_path in enumerate(config[\"model_paths\"]):\n",
    "    if (\n",
    "        model_path not in name2model\n",
    "    ):  # making sure we are not loading the model twice in case some constraints use the same model.\n",
    "        try:\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=True,\n",
    "            )\n",
    "        except:\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=False,\n",
    "            )\n",
    "\n",
    "        name2config[model_path] = AutoConfig.from_pretrained(\n",
    "            model_path, cache_dir=config[\"cache_dir\"]\n",
    "        )\n",
    "\n",
    "        if config[\"model_types\"][i] == \"RobertaCustomForSequenceClassification\":\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                RobertaCustomForSequenceClassification.from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(transformers, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        name2model[model_path].eval()\n",
    "        name2model[model_path].to(config['device'])\n",
    "\n",
    "    input_embeds = name2model[model_path].get_input_embeddings()\n",
    "    if isinstance(input_embeds, torch.nn.Sequential):\n",
    "        input_embeds = input_embeds[0]\n",
    "    embed_luts.append(input_embeds)\n",
    "\n",
    "    if config[\"target_type\"] == \"embeds\":\n",
    "        embed_luts[-1].requires_grad = False\n",
    "\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "mlm = None if config[\"method\"] == \"mlm-beamsearch-v2\" else AutoModelForMaskedLM.from_pretrained(\"roberta-base\").to(config['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lossfns = []\n",
    "for i, loss in enumerate(config[\"losses\"]):\n",
    "    lossfns.append(\n",
    "        lossbuilder.build_loss(\n",
    "            loss,\n",
    "            name2model[config[\"model_paths\"][i]],\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]],\n",
    "            build_loss_args,\n",
    "        )\n",
    "    )\n",
    "    lossfns[i].tokenizer.add_special_tokens({\"mask_token\": mlm_tokenizer.mask_token})\n",
    "    loss2tokenizer[loss] = lossfns[i].tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_weights = [1 - wandb.config.closs_weight, wandb.config.closs_weight]\n",
    "interrupted = False\n",
    "# for text_id in range(len(source_dataset))[resume_idx:]:\n",
    "text_id = 0\n",
    "source_text = source_dataset[text_id]\n",
    "if source_text == \"\":\n",
    "    source_text = lossfns[0].tokenizer.bos_token\n",
    "\n",
    "if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "    AR_prediction_all = [x[\"text\"] for x in generation_dataset[text_id]]\n",
    "    # predicted_batches = [x[\"tokens\"] for x in generation_dataset[text_id]]\n",
    "    # predicted_batches = [\n",
    "    #     torch.tensor([x], dtype=torch.long, device=config[\"device\"])\n",
    "    #     for x in predicted_batches\n",
    "    # ]\n",
    "    \n",
    "elif (config[\"task\"] == \"formality\") or (\n",
    "    config[\"task\"] == \"sentiment-lewis-compr\"\n",
    "):\n",
    "    AR_prediction_all = [generation_dataset[text_id]]\n",
    "\n",
    "curr_num_samples = len(AR_prediction_all)\n",
    "\n",
    "curr_loss = torch.zeros(len(AR_prediction_all)).to(config['device'])\n",
    "logging_loss = torch.zeros((len(AR_prediction_all),2)).to(config['device'])\n",
    "edit_yn = torch.ones(len(AR_prediction_all), dtype=torch.bool).to(config['device'])\n",
    "        \n",
    "for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "    with torch.no_grad():\n",
    "        lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "            source_text, AR_prediction_all,\n",
    "            label_id=config['target_label_ids'][lossid],\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "    curr_loss += loss_weights[lossid] * lossvalue\n",
    "    logging_loss[:, lossid] = lossvalue.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define an object to locate problematic phrases\n",
    "locator = LocateMachine(lossfns[1].model, lossfns[1].tokenizer)\n",
    "\n",
    "label_ids = config[\"target_label_ids\"]  # target label's ids for each loss\n",
    "\n",
    "run.summary[\"prep_time\"] = time.time() - main_start_time\n",
    "## beginning of main logic\n",
    "decode_start_time = time.time()\n",
    "# text_id = 0\n",
    "if config[\"resume\"]:\n",
    "    num_skipped = run.summary.get(\"num_skipped\", 0)\n",
    "    num_edited = run.summary.get(\"num_edited\", 0)\n",
    "    num_decoded_tokens = run.summary.get(\"num_decoded_tokens\", 0)\n",
    "else:\n",
    "    num_skipped = 0\n",
    "    num_edited = 0\n",
    "    num_decoded_tokens = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_weights = [1 - wandb.config.closs_weight, wandb.config.closs_weight]\n",
    "interrupted = False\n",
    "# for text_id in range(len(source_dataset))[resume_idx:]:\n",
    "text_id = 0\n",
    "source_text = source_dataset[text_id]\n",
    "if source_text == \"\":\n",
    "    source_text = lossfns[0].tokenizer.bos_token\n",
    "\n",
    "if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "    AR_prediction_all = [x[\"text\"] for x in generation_dataset[text_id]]\n",
    "    # predicted_batches = [x[\"tokens\"] for x in generation_dataset[text_id]]\n",
    "    # predicted_batches = [\n",
    "    #     torch.tensor([x], dtype=torch.long, device=config[\"device\"])\n",
    "    #     for x in predicted_batches\n",
    "    # ]\n",
    "    \n",
    "elif (config[\"task\"] == \"formality\") or (\n",
    "    config[\"task\"] == \"sentiment-lewis-compr\"\n",
    "):\n",
    "    AR_prediction_all = [generation_dataset[text_id]]\n",
    "\n",
    "curr_num_samples = len(AR_prediction_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "running_text = best_text = deepcopy(AR_prediction_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_text = [' wearing games and how ****ing much do I hate horse wearing games.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_text = locator.locate_main(running_text, \n",
    "                        method = config['locate_method'], \n",
    "                        max_num_tokens = wandb.config.num_edit_token_per_step, \n",
    "                        unit = config['locate_unit'], \n",
    "                        num_layer = -2, #penultimate\n",
    "                        label_id = config['target_label_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' wearing<mask> and how<mask><mask> much do I hate<mask> wearing games.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_text_old  = []\n",
    "for example in running_text:\n",
    "    masked_text_old_  = locate_main(example, \n",
    "                                config[\"locate_method\"], \n",
    "                                name2model[config[\"model_paths\"][1]], \n",
    "                                name2tokenizer[config[\"tokenizer_paths\"][1]], \n",
    "                                max_num_tokens = wandb.config.num_edit_token_per_step, \n",
    "                                unit=config[\"locate_unit\"], \n",
    "                                device=\"cuda\", \n",
    "                                label_id=config[\"target_label_ids\"][1],\n",
    "                                num_layer=10)\n",
    "    masked_text_old.append(masked_text_old_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' wearing games and how<mask><mask> much do I hate<mask> wearing games.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_text_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_text_old == masked_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug for discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = name2model[config[\"model_paths\"][1]]\n",
    "tokenizer = name2tokenizer[config[\"tokenizer_paths\"][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = running_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' wearing games and how ****ing much do I hate horse wearing games.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## old\n",
    "# def locate_main(prediction, method, model, tokenizer, max_num_tokens = 6, unit=\"word\", device=\"cuda\", **kwargs): #label_id = 1, num_layer=10):\n",
    "batch_old = tokenizer(prediction, add_special_tokens=False, padding=True, truncation=True, return_tensors=\"pt\").to(config['device'])\n",
    "output_old = model(**batch_old, output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new\n",
    "batch_new = tokenizer([prediction], add_special_tokens=False, padding=True, truncation=True, return_tensors=\"pt\").to(config['device']) # prediction이 list여도 처리가능함\n",
    "lengths = batch_new.attention_mask.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 2498,   426,     8,   141, 31095,   154,   203,   109,    38,  4157,\n",
       "          5253,  2498,   426,     4]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 2498,   426,     8,   141, 31095,   154,   203,   109,    38,  4157,\n",
       "          5253,  2498,   426,     4]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## old locate_gradnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## old\n",
    "# def locate_grad_norm(output, tokenizer, batch, max_num_tokens = 6, unit=\"word\", device=\"cuda\", label_id = 1):\n",
    "import string\n",
    "punctuations = string.punctuation + '\\n '\n",
    "punctuations = list(punctuations)\n",
    "punctuations.remove('-')\n",
    "stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in tokenizer.special_tokens_map.values()]\n",
    "stopwords_ids = [tokenizer.encode(word,add_special_tokens=False)[-1] for word in stopwords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## output['hidden_states']: tuple of length num_hidden_layers\n",
    "## output['hidden_states'][0]: (batch_size, seq_len, hidden_size)\n",
    "layer_old = output_old['hidden_states'][0]\n",
    "layer_old.retain_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.030, -0.345, -0.056,  ...,  0.108,  0.017,  0.061],\n",
       "         [-0.092,  0.105,  0.213,  ...,  0.420, -0.381,  0.088],\n",
       "         [ 0.101,  0.066,  0.025,  ..., -0.029, -0.155,  0.407],\n",
       "         ...,\n",
       "         [ 0.166, -0.305,  0.111,  ...,  0.303, -0.069, -0.004],\n",
       "         [ 0.054,  0.213, -0.006,  ...,  0.527, -0.356, -0.161],\n",
       "         [ 0.519,  0.014, -0.069,  ..., -0.721,  0.081, -0.006]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = 1\n",
    "## output['logits'] : (batch_size, num_labels)\n",
    "softmax=torch.nn.Softmax(dim=-1)\n",
    "probs_old = softmax(output_old['logits'])[:, label_id]\n",
    "# print(f\"probs.shape:{probs.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probs_old.sum().backward(retain_graph=True) ## NOTE. https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments/47026836#47026836\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.439], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## layer.grad : (batch_size, seq_len, hidden_size)\n",
    "# print(f\"layer.grad.shape:{layer.grad.shape}\")\n",
    "norm_old = torch.norm(layer_old.grad, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.090, 0.093, 0.069, 0.058, 0.319, 0.092, 0.057, 0.060, 0.040, 0.054,\n",
       "         0.113, 0.069, 0.061, 0.076]], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## norm : (batch_size, seq_len)\n",
    "# print(f\"norm.shape:{norm.shape}\")\n",
    "norm_old = torch.where(norm_old > 0, norm_old, torch.full_like(norm_old, 1e-10))\n",
    "# print(f\"norm:{norm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.090, 0.093, 0.069, 0.058, 0.319, 0.092, 0.057, 0.060, 0.040, 0.054,\n",
       "         0.113, 0.069, 0.061, 0.076]], device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lengths_old = [i.tolist().count(1) for i in batch[\"attention_mask\"]]\n",
    "# print(f\"lengths: {lengths}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "locate_ixes = []\n",
    "locate_scores = []\n",
    "# for i in range(batch[\"input_ids\"].shape[0]):\n",
    "i = 0\n",
    "\n",
    "## norm_ : (seq_len,)\n",
    "current_norm_old = norm_old[i, :]\n",
    "# print(f\"norm_ shape: {current_norm.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.090, 0.093, 0.069, 0.058, 0.319, 0.092, 0.057, 0.060, 0.040, 0.054,\n",
       "        0.113, 0.069, 0.061, 0.076], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_norm_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## current_sent : (lengths[i], )\n",
    "current_sent_old = batch_old[\"input_ids\"][i][: lengths_old[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2498,   426,     8,   141, 31095,   154,   203,   109,    38,  4157,\n",
       "         5253,  2498,   426,     4], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_sent_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " wearing\n",
      " games\n",
      " and\n",
      " how\n",
      " ****\n",
      "ing\n",
      " much\n",
      " do\n",
      " I\n",
      " hate\n",
      " horse\n",
      " wearing\n",
      " games\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for tok in current_sent_old:\n",
    "    print(tokenizer.decode(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punc_indices_old = torch.where(~torch.isin(current_sent_old, torch.tensor(stopwords_ids).to(config['device'])))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punc_indices_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(f\"current_sent: {current_sent}\")\n",
    "# print(f\"len(current_sent), lengths[i]: {len(current_sent), lengths[i]}\")\n",
    "# print(f\"no_punc_indices: {no_punc_indices}\")\n",
    "# print(f\"current_sent[no_punc_indices]: {current_sent[no_punc_indices]}\")\n",
    "# print(f\"tokenizer.decode(current_sent[no_punc_indices]): {tokenizer.decode(current_sent[no_punc_indices])}\")\n",
    "\n",
    "## normalize current_norm\n",
    "current_norm_old = current_norm_old[: lengths_old[i]].softmax(-1) \n",
    "# print(f\"current_norm after normalizing: {current_norm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.071, 0.072, 0.070, 0.069, 0.090, 0.071, 0.069, 0.069, 0.068, 0.069,\n",
       "        0.073, 0.070, 0.069, 0.070], device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_norm_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_locate_scores_old = torch.zeros_like(current_norm_old)\n",
    "current_locate_scores_old[no_punc_indices_old] = current_norm_old[no_punc_indices_old].clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.071, 0.072, 0.000, 0.069, 0.090, 0.071, 0.069, 0.069, 0.068, 0.069,\n",
       "        0.073, 0.070, 0.069, 0.000], device='cuda:0')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_locate_scores_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "locate_scores.append(current_locate_scores_old.cpu().detach().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_norm_old = current_norm_old[no_punc_indices]\n",
    "# print(f\"current_norm after selecting non stop words indices: {current_norm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.071, 0.072, 0.069, 0.090, 0.071, 0.069, 0.069, 0.068, 0.069, 0.073,\n",
       "        0.070, 0.069], device='cuda:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_norm_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## calculate mean value within the sequence\n",
    "avg_value_old = current_norm_old.view(-1).mean().item()\n",
    "# print(\"avg_value\", avg_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07165762782096863"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_value_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## find indices of tokens whose norm value is greater than the mean value\n",
    "top_masks_old = ((current_norm_old >= avg_value_old).nonzero().view(-1)) \n",
    "top_masks_old = top_masks_old.cpu().tolist()\n",
    "torch.cuda.empty_cache()\n",
    "# print(\"indices of non stopwords tokens whose grad norm value is greater than the mean value\", top_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_masks_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lengths_old[i]) // 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_tokens=5\n",
    "## in case the number of above average gradient norm tokens is greater than the max_num_tokens or 1/3 of the lengths[i]\n",
    "if len(top_masks_old) > min((lengths_old[i]) // 3, max_num_tokens):\n",
    "    # print(\"len(located_tokens) exceeds max_num_tokens or 1/3 of the lengths[i]. Taking top k.\")\n",
    "    top_masks_old = (\n",
    "        current_norm.topk(max(min((lengths_old[i]) // 3, max_num_tokens), 1))[1]\n",
    "    )\n",
    "    top_masks_old = top_masks_old.cpu().tolist()\n",
    "    # print(\"indices of non stopwords tokens located after taking top k\", top_masks)\n",
    "\n",
    "top_masks_old = no_punc_indices[top_masks_old].cpu().detach().tolist()\n",
    "# print(\"indices of tokens located\", top_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 10]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_masks_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_module.locate.locate_utils import Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group token indices that belong to the same word\n",
    "words_old = tokenizer.decode(current_sent_old).strip().split()\n",
    "word2tok_mapper=Processor(tokenizer)\n",
    "# print(f\"input to word2tok: {pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})}\")\n",
    "grouped_tokens_old = list(word2tok_mapper.get_word2tok(pd.Series({'words':words_old, 'tokens':current_sent_old.cpu().tolist()})).values())            # j, k = 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [1], [2], [3], [4, 5], [6], [7], [8], [9], [10], [11], [12, 13]]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_tokens_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [1], [2], [3], [4, 5], [6], [7], [8], [9], [10], [11], [12, 13]]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_tokens_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_tokens_old = []\n",
    "# grouped_tokens_old_for_word = []\n",
    "# while j < len(current_sent):\n",
    "#     if (tokenizer.decode(current_sent[j]).strip() not in stopwords):\n",
    "#         while k < len(words):\n",
    "#             if tokenizer.decode(current_sent[j]).strip() in words[k]:\n",
    "#                 grouped_tokens_old_for_word.append(j)\n",
    "#                 break\n",
    "#             else:\n",
    "#                 grouped_tokens_old.append(grouped_tokens_old_for_word)\n",
    "#                 grouped_tokens_old_for_word = []\n",
    "#                 k += 1\n",
    "#     j += 1\n",
    "# grouped_tokens_old.append(grouped_tokens_old_for_word)\n",
    "\n",
    "## expand located token indices to include adjacent token indices that belong to the same word as already located tokens\n",
    "top_masks_old.sort()\n",
    "top_masks_old_final = set()\n",
    "for index in top_masks_old:\n",
    "    if index not in top_masks_old_final:\n",
    "        word = [grouped_ixes for grouped_ixes in grouped_tokens_old if index in grouped_ixes]\n",
    "        # print(\"word\", word)\n",
    "        if len(word) > 0:\n",
    "            word = set(word[0])\n",
    "        else:\n",
    "            print(f\"warning. {index} not in the word groups. decoded value: {tokenizer.decode(index)}\")\n",
    "            word = set([index])\n",
    "        top_masks_old_final |= word\n",
    "locate_ixes.append(sorted(list(top_masks_old_final)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 5, 10]]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locate_ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 2498,   426,     8,   141, 31095,   154,   203,   109,    38,  4157,\n",
       "          5253,  2498,   426,     4]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_new = model(**batch_new, output_hidden_states=True)\n",
    "## output['hidden_states']: tuple of length num_hidden_layers\n",
    "## output['hidden_states'][0]: (batch_size, seq_len, hidden_size)\n",
    "layer_new = output_new['hidden_states'][0]\n",
    "layer_new.retain_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(layer_new != layer_old).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "softmax=torch.nn.Softmax(dim=-1)\n",
    "probs_new = softmax(output_new['logits'])[:, label_id]\n",
    "probs_new.sum().backward(retain_graph=True) ## NOTE. https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments/47026836#47026836\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.439], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.439], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## layer.grad : (batch_size, seq_len, hidden_size)\n",
    "norm_new = torch.norm(layer_new.grad, dim=-1)\n",
    "## norm : (batch_size, seq_len)\n",
    "token_wise_scores_new = torch.where(norm_new > 0, norm_new, torch.full_like(norm_new, 1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True]], device='cuda:0')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_old == token_wise_scores_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.090, 0.093, 0.069, 0.058, 0.319, 0.092, 0.057, 0.060, 0.040, 0.054,\n",
       "         0.113, 0.069, 0.061, 0.076]], device='cuda:0')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_wise_scores_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation + '\\n '\n",
    "punctuations = list(punctuations)\n",
    "punctuations.remove('-')\n",
    "stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in tokenizer.special_tokens_map.values()]\n",
    "stopwords_ids_new = tokenizer.batch_encode_plus(stopwords, return_tensors=\"pt\",add_special_tokens=False)['input_ids'].squeeze().to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    8,     9,    50,    98,   328,   113, 10431,  1629,   207,   947,\n",
       "          108,  1640,    43,  3226,  2744,     6,     4,    73,    35,   131,\n",
       "        41552,  5214, 15698,   116,  1039, 10975, 37457,   742, 35227,  1215,\n",
       "        12905, 45152, 15483, 24303, 34437, 50118,  1437,     0,     2,     3,\n",
       "            2,     1,     0, 50264], device='cuda:0')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_ids_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.090, 0.093, 0.069, 0.058, 0.319, 0.092, 0.057, 0.060, 0.040, 0.054,\n",
       "         0.113, 0.069, 0.061, 0.076]], device='cuda:0')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_wise_scores_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## softmax 먼저 해주기 ## <- 이것때문에 결과가 달라졌을 것이다.\n",
    "token_wise_scores_new[batch_new.attention_mask == 0] = -float(\"inf\")\n",
    "token_wise_scores_new = token_wise_scores_new.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.071, 0.072, 0.069, 0.090, 0.071, 0.069, 0.069, 0.068, 0.069, 0.073,\n",
       "        0.070, 0.069], device='cuda:0')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_norm_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.071, 0.072, 0.070, 0.069, 0.090, 0.071, 0.069, 0.069, 0.068, 0.069,\n",
       "         0.073, 0.070, 0.069, 0.070]], device='cuda:0')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_wise_scores_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "## avg_value만 구하면 된다고 한다면, 이렇게도 가능하다.\n",
    "token_wise_scores_new[torch.isin(batch_new.input_ids, stopwords_ids_new)]=0.0\n",
    "no_punc_len_new=((~torch.isin(batch_new.input_ids, stopwords_ids_new))*batch_new.attention_mask).sum(dim=-1) # tensor([2, 4], device='cuda:0')\n",
    "avg_values_new=token_wise_scores_new.sum(dim=-1)/no_punc_len_new # tensor([0.5120, 0.3744], device='cuda:0', grad_fn=<DivBackward0>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07165762782096863"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_value_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07165762782096863"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_values_new[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## stopwords가 아닌 부분에서만 index를 찾아야 한다.\n",
    "token_wise_scores_new[torch.isin(batch_new.input_ids, stopwords_ids_new)]=-float(\"inf\") ## stopwords 인부분을 -inf로 세팅하면, avg랑 비교할 때랑 topk를 찾을때 빠질것으로 예상\n",
    "top_masks_new = (token_wise_scores_new >= avg_values_new.unsqueeze(1))# unsqueeze to allow implicit broadcasting : (N) -> (N, 1) -> (N, L)\n",
    "max_num_located_tokens_new = torch.minimum((lengths//3), torch.LongTensor([max_num_tokens]).to(model.device))\n",
    "max_num_located_tokens_new = torch.minimum(max_num_located_tokens_new, top_masks_new.sum(dim=-1))\n",
    "top_masks_new_final = [x[:max_num_located_tokens_new[i]] for i,x in enumerate(token_wise_scores_new.argsort(dim=-1,descending=True).tolist())] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_new = lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_module.locate.new_locate_utils import get_word_level_locate_indices\n",
    "from itertools import repeat \n",
    "for i, arguments in enumerate(zip(prediction,batch_new.input_ids.tolist(), lengths_new.tolist(), top_masks_new_final, repeat(tokenizer))):\n",
    "    locate_ixes = get_word_level_locate_indices(*arguments)\n",
    "    batch_new.input_ids[i, locate_ixes] = tokenizer.mask_token_id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 4]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locate_ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_word2tok(row: pd.Series, tokenizer: AutoTokenizer) -> dict:\n",
    "    \"\"\"\n",
    "    A function that take a list of words and a corresponding list of tokens \n",
    "    into a mapping between each word's index and its corresponding token indexes.\n",
    "    @param row: A row from dataframe\n",
    "    @return word2char: A dictionary with word's location index as keys and tuples of corresponding token location indexes as values.\n",
    "\n",
    "    Example:\n",
    "    row=pd.Series()\n",
    "    row['words']=['wearing', 'games', 'and', 'holy', '****ing', 'shit', 'do', 'I', 'hate', 'horse', 'wearing', 'games.']\n",
    "    row['tokens']=[86, 6648, 1830, 290, 11386, 25998, 278, 7510, 466, 314, 5465, 8223, 5762, 1830, 13]\n",
    "    word2tok=get_word2tok(row)\n",
    "    word2tok\n",
    "    {0: [0, 1],\n",
    "    1: [2],\n",
    "    2: [3],\n",
    "    ...\n",
    "    10: [12],\n",
    "    11: [13, 14]}\n",
    "    \"\"\"\n",
    "    \n",
    "    jl, jr, k = 0, 0, 0\n",
    "    grouped_tokens = []\n",
    "    tok2word=dict()\n",
    "    while jr <= len(row['tokens'])+1 and k < len(row['words']):\n",
    "        \n",
    "        if tokenizer.decode(row['tokens'][jl:jr]).strip() == row['words'][k]:\n",
    "            grouped_tokens.append(list(range(jl,jr)))\n",
    "            for ix in range(jl,jr):\n",
    "                tok2word[ix] = k\n",
    "            k += 1\n",
    "            jl = jr\n",
    "            jr += 1\n",
    "        else:\n",
    "            jr += 1\n",
    "    # word2tok = dict(zip(range(len(grouped_tokens)), grouped_tokens))\n",
    "    # return word2tok\n",
    "    return tok2word, grouped_tokens\n",
    "\n",
    "def get_word_level_locate_indices(current_sent:str,prediction:list,length:int, top_masks_final:list, tokenizer:AutoTokenizer):\n",
    "    # word의 일부만 locate 한 경우, word 전체를 locate 한다.\n",
    "    # 같은 word 안에 있는 token 끼리 묶음.\n",
    "    words = current_sent.strip().split()\n",
    "    prediction = prediction[:length]\n",
    "    tok2word, grouped_tokens = get_word2tok(pd.Series({'words':words, 'tokens':prediction}), tokenizer)\n",
    "    \n",
    "    top_masks_final.sort()\n",
    "    top_masks_final_final = []\n",
    "    for index in top_masks_final:\n",
    "        if index not in top_masks_final_final:\n",
    "            # word = [grouped_ixes for grouped_ixes in grouped_tokens if index in grouped_ixes]\n",
    "            word_index = tok2word.get(index, None)\n",
    "            # if len(word) > 0:\n",
    "            if word_index is not None:\n",
    "                top_masks_final_final.extend(grouped_tokens[word_index])\n",
    "            else:\n",
    "                top_masks_final_final.extend([index])    \n",
    "    return list(set(top_masks_final_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Processor():\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    def get_word2tok(self, row: pd.Series) -> dict:\n",
    "        \"\"\"\n",
    "        A function that take a list of words and a corresponding list of tokens \n",
    "        into a mapping between each word's index and its corresponding token indexes.\n",
    "        @param row: A row from dataframe\n",
    "        @return word2char: A dictionary with word's location index as keys and tuples of corresponding token location indexes as values.\n",
    "\n",
    "        Example:\n",
    "        row=pd.Series()\n",
    "        row['words']=['wearing', 'games', 'and', 'holy', '****ing', 'shit', 'do', 'I', 'hate', 'horse', 'wearing', 'games.']\n",
    "        row['tokens']=[86, 6648, 1830, 290, 11386, 25998, 278, 7510, 466, 314, 5465, 8223, 5762, 1830, 13]\n",
    "        word2tok=get_word2tok(row)\n",
    "        word2tok\n",
    "        {0: [0, 1],\n",
    "        1: [2],\n",
    "        2: [3],\n",
    "        ...\n",
    "        10: [12],\n",
    "        11: [13, 14]}\n",
    "        \"\"\"\n",
    "        \n",
    "        jl, jr, k = 0, 0, 0\n",
    "        grouped_tokens = []\n",
    "        while jr <= len(row['tokens'])+1 and k < len(row['words']):\n",
    "            # print(f\"{jl}, {jr}, {k}: {self.tokenizer.decode(row['tokens'][jl:jr]).strip()}\")\n",
    "            if self.tokenizer.decode(row['tokens'][jl:jr]).strip() == row['words'][k]:\n",
    "                grouped_tokens.append(list(range(jl,jr)))\n",
    "                k += 1\n",
    "                jl = jr\n",
    "                jr += 1\n",
    "            else:\n",
    "                jr += 1\n",
    "        word2tok = dict(zip(range(len(grouped_tokens)), grouped_tokens))\n",
    "        return word2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_old = tokenizer.decode(current_sent_old).strip().split()\n",
    "word2tok_mapper_old=Processor(tokenizer)\n",
    "# print(f\"input to word2tok: {pd.Series({'words':words, 'tokens':current_sent.cpu().tolist()})}\")\n",
    "grouped_tokens_old = list(word2tok_mapper_old.get_word2tok(pd.Series({'words':words_old, 'tokens':current_sent_old.cpu().tolist()})).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [1], [2], [3], [4, 5], [6], [7], [8], [9], [10], [11], [12, 13]]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_tokens_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_new = prediction.strip().split()\n",
    "tokens_new = batch_new.input_ids.tolist()[0][:lengths[0]] ## batch_new가 마스킹 되기 전 깨끗한 버전이여야 함!\n",
    "tok2word, grouped_tokens_new = get_word2tok(pd.Series({'words':words_new, 'tokens':tokens_new}), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 2: 2,\n",
       " 3: 3,\n",
       " 4: 4,\n",
       " 5: 4,\n",
       " 6: 5,\n",
       " 7: 6,\n",
       " 8: 7,\n",
       " 9: 8,\n",
       " 10: 9,\n",
       " 11: 10,\n",
       " 12: 11,\n",
       " 13: 11}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if debugging worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from itertools import chain\n",
    "import math\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "os.chdir('/data/hyeryung/mucoco')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "from copy import deepcopy\n",
    "import new_module.losses as lossbuilder\n",
    "import new_module.losses_old as lossbuilder_old\n",
    "import wandb\n",
    "# from new_module.decode_utils import (\n",
    "#     beam_rerank_v0,\n",
    "#     beam_rerank_v1,\n",
    "#     beam_rerank_v2,\n",
    "#     combi_rerank,\n",
    "# )\n",
    "# from new_module.new_decode_utils import get_beam_hypotheses, get_combi_hypotheses, final_reranking\n",
    "from new_module.evaluation.evaluate_wandb import evaluate_main\n",
    "from new_module.locate.new_locate_utils import LocateMachine\n",
    "from new_module.locate.locate_utils import locate_main\n",
    "from new_module.utils.robertacustom import RobertaCustomForSequenceClassification\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(os.environ.get(\"LOGGING_LEVEL\", logging.DEBUG))\n",
    "import joblib\n",
    "config = joblib.load('config.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 1830\n",
      "https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 377\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhayleyson\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Popen(['git', 'cat-file', '--batch-check'], cwd=/data/hyeryung/mucoco, stdin=<valid stream>, shell=False, universal_newlines=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/hyeryung/mucoco/wandb/run-20240411_041904-d3gwtgu0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hayleyson/toxicity-decoding/runs/d3gwtgu0' target=\"_blank\">valiant-dew-190</a></strong> to <a href='https://wandb.ai/hayleyson/toxicity-decoding' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hayleyson/toxicity-decoding' target=\"_blank\">https://wandb.ai/hayleyson/toxicity-decoding</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hayleyson/toxicity-decoding/runs/d3gwtgu0' target=\"_blank\">https://wandb.ai/hayleyson/toxicity-decoding/runs/d3gwtgu0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/generation_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "main_start_time = time.time()\n",
    "\n",
    "if not config.get(\"model_tag\", None):\n",
    "    if \"energy-training\" in config[\"model_paths\"][1]:\n",
    "        config[\"model_tag\"] = \"em\"\n",
    "    else:\n",
    "        config[\"model_tag\"] = \"clsf\"\n",
    "\n",
    "    if (config[\"task\"] == \"formality\") and (\"gyafc\" in config[\"model_paths\"][1]):\n",
    "        config[\"model_tag\"] += \"-gyafc\"\n",
    "\n",
    "if config[\"resume\"]:\n",
    "    logger.info(\"resuming from a previous run\")\n",
    "    run = wandb.init(\n",
    "        project=config[\"wandb_project\"],\n",
    "        entity=config[\"wandb_entity\"],\n",
    "        id=config[\"wandb_run_id\"],\n",
    "        resume=\"must\",\n",
    "    )\n",
    "else:\n",
    "    run = wandb.init(\n",
    "        project=config[\"wandb_project\"],\n",
    "        entity=config[\"wandb_entity\"],\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "run_id = run.path.split(\"/\")[-1]\n",
    "display_name = f\"{run_id}\"\n",
    "\n",
    "\n",
    "outdir = os.path.join(config[\"output_dir_prefix\"], display_name)\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "outfile = f\"{outdir}/outputs_epsilon{config['min_epsilons'][0]}.txt\"\n",
    "run.summary[\"outfile_path\"] = outfile\n",
    "\n",
    "\n",
    "class dummyArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "build_loss_args = dummyArgs(**config[\"build_loss_dict\"])\n",
    "\n",
    "## load data\n",
    "if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "    source_dataset = [\n",
    "        json.loads(l)[config[\"jsonl_primary_key\"]][config[\"jsonl_secondary_key\"]]\n",
    "        for l in open(config[\"source_data\"])\n",
    "    ]\n",
    "    generation_dataset = [\n",
    "        json.loads(l)[\"generations\"] for l in open(config[\"source_data\"])\n",
    "    ]\n",
    "elif (config[\"task\"] == \"formality\") or (config[\"task\"] == \"sentiment-lewis-compr\"):\n",
    "    with open(config[\"source_data\"], \"r\") as f:\n",
    "        generation_dataset = [line.rstrip('\\n') for line in f.readlines()]\n",
    "    source_dataset = [\"\" for l in generation_dataset]\n",
    "\n",
    "# check if outfile exists\n",
    "if (config[\"resume\"]) and (os.path.exists(outfile)):\n",
    "\n",
    "    with open(outfile, \"r\") as f:\n",
    "        existing_gens = [x.rstrip(\"\\n\") for x in f.readlines()]\n",
    "    resume_idx = len(existing_gens)\n",
    "    if resume_idx == len(source_dataset):\n",
    "        logger.debug(\"output file is already complete. skipping this run.\")\n",
    "        raise\n",
    "    elif resume_idx < len(source_dataset):\n",
    "        logger.info(\n",
    "            f\"output file already exists but is incomplete. resuming from index: {resume_idx}\"\n",
    "        )\n",
    "        outf = open(outfile, \"a\")\n",
    "        int_outf = open(outfile+\".intermediate\", \"a\")\n",
    "    else:\n",
    "        logger.critical(\n",
    "            f\"output file seems to be corrupted. The file length is {resume_idx}, where the size of source_dataset is {len(source_dataset)}\"\n",
    "        )\n",
    "        raise\n",
    "else:\n",
    "    resume_idx = 0\n",
    "    outf = open(outfile, \"w\")\n",
    "    int_outf = open(outfile+\".intermediate\", \"w\")\n",
    "\n",
    "\n",
    "## load tokenizer, models, define losses\n",
    "name2tokenizer = {}\n",
    "name2model = {}\n",
    "name2config = {}\n",
    "loss2tokenizer = {}\n",
    "embed_luts = []\n",
    "\n",
    "for i, model_path in enumerate(config[\"model_paths\"]):\n",
    "    if (\n",
    "        model_path not in name2model\n",
    "    ):  # making sure we are not loading the model twice in case some constraints use the same model.\n",
    "        try:\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=True,\n",
    "            )\n",
    "        except:\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=False,\n",
    "            )\n",
    "\n",
    "        name2config[model_path] = AutoConfig.from_pretrained(\n",
    "            model_path, cache_dir=config[\"cache_dir\"]\n",
    "        )\n",
    "\n",
    "        if config[\"model_types\"][i] == \"RobertaCustomForSequenceClassification\":\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                RobertaCustomForSequenceClassification.from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(transformers, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        name2model[model_path].eval()\n",
    "        name2model[model_path].to(config['device'])\n",
    "\n",
    "    input_embeds = name2model[model_path].get_input_embeddings()\n",
    "    if isinstance(input_embeds, torch.nn.Sequential):\n",
    "        input_embeds = input_embeds[0]\n",
    "    embed_luts.append(input_embeds)\n",
    "\n",
    "    if config[\"target_type\"] == \"embeds\":\n",
    "        embed_luts[-1].requires_grad = False\n",
    "\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "mlm = None if config[\"method\"] == \"mlm-beamsearch-v2\" else AutoModelForMaskedLM.from_pretrained(\"roberta-base\").to(config['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lossfns = []\n",
    "for i, loss in enumerate(config[\"losses\"]):\n",
    "    lossfns.append(\n",
    "        lossbuilder.build_loss(\n",
    "            loss,\n",
    "            name2model[config[\"model_paths\"][i]],\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]],\n",
    "            build_loss_args,\n",
    "        )\n",
    "    )\n",
    "    lossfns[i].tokenizer.add_special_tokens({\"mask_token\": mlm_tokenizer.mask_token})\n",
    "    loss2tokenizer[loss] = lossfns[i].tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_weights = [1 - wandb.config.closs_weight, wandb.config.closs_weight]\n",
    "interrupted = False\n",
    "# for text_id in range(len(source_dataset))[resume_idx:]:\n",
    "text_id = 0\n",
    "source_text = source_dataset[text_id]\n",
    "if source_text == \"\":\n",
    "    source_text = lossfns[0].tokenizer.bos_token\n",
    "\n",
    "if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "    AR_prediction_all = [x[\"text\"] for x in generation_dataset[text_id]]\n",
    "    # predicted_batches = [x[\"tokens\"] for x in generation_dataset[text_id]]\n",
    "    # predicted_batches = [\n",
    "    #     torch.tensor([x], dtype=torch.long, device=config[\"device\"])\n",
    "    #     for x in predicted_batches\n",
    "    # ]\n",
    "    \n",
    "elif (config[\"task\"] == \"formality\") or (\n",
    "    config[\"task\"] == \"sentiment-lewis-compr\"\n",
    "):\n",
    "    AR_prediction_all = [generation_dataset[text_id]]\n",
    "\n",
    "curr_num_samples = len(AR_prediction_all)\n",
    "\n",
    "curr_loss = torch.zeros(len(AR_prediction_all)).to(config['device'])\n",
    "logging_loss = torch.zeros((len(AR_prediction_all),2)).to(config['device'])\n",
    "edit_yn = torch.ones(len(AR_prediction_all), dtype=torch.bool).to(config['device'])\n",
    "        \n",
    "for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "    with torch.no_grad():\n",
    "        lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "            source_text, AR_prediction_all,\n",
    "            label_id=config['target_label_ids'][lossid],\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "    curr_loss += loss_weights[lossid] * lossvalue\n",
    "    logging_loss[:, lossid] = lossvalue.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define an object to locate problematic phrases\n",
    "locator = LocateMachine(lossfns[1].model, lossfns[1].tokenizer)\n",
    "\n",
    "label_ids = config[\"target_label_ids\"]  # target label's ids for each loss\n",
    "\n",
    "run.summary[\"prep_time\"] = time.time() - main_start_time\n",
    "## beginning of main logic\n",
    "decode_start_time = time.time()\n",
    "# text_id = 0\n",
    "if config[\"resume\"]:\n",
    "    num_skipped = run.summary.get(\"num_skipped\", 0)\n",
    "    num_edited = run.summary.get(\"num_edited\", 0)\n",
    "    num_decoded_tokens = run.summary.get(\"num_decoded_tokens\", 0)\n",
    "else:\n",
    "    num_skipped = 0\n",
    "    num_edited = 0\n",
    "    num_decoded_tokens = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_weights = [1 - wandb.config.closs_weight, wandb.config.closs_weight]\n",
    "interrupted = False\n",
    "# for text_id in range(len(source_dataset))[resume_idx:]:\n",
    "text_id = 0\n",
    "source_text = source_dataset[text_id]\n",
    "if source_text == \"\":\n",
    "    source_text = lossfns[0].tokenizer.bos_token\n",
    "\n",
    "if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "    AR_prediction_all = [x[\"text\"] for x in generation_dataset[text_id]]\n",
    "    # predicted_batches = [x[\"tokens\"] for x in generation_dataset[text_id]]\n",
    "    # predicted_batches = [\n",
    "    #     torch.tensor([x], dtype=torch.long, device=config[\"device\"])\n",
    "    #     for x in predicted_batches\n",
    "    # ]\n",
    "    \n",
    "elif (config[\"task\"] == \"formality\") or (\n",
    "    config[\"task\"] == \"sentiment-lewis-compr\"\n",
    "):\n",
    "    AR_prediction_all = [generation_dataset[text_id]]\n",
    "\n",
    "curr_num_samples = len(AR_prediction_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "running_text = best_text = deepcopy(AR_prediction_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['locate_method'] = 'attention' ## 'grad_norm' ## 두 가지 경우 모두 결과가 동일하게 잘 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_text = [' wearing games and how ****ing much do I hate horse wearing games.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_text = locator.locate_main(running_text, \n",
    "                        method = config['locate_method'], \n",
    "                        max_num_tokens = wandb.config.num_edit_token_per_step, \n",
    "                        unit = config['locate_unit'], \n",
    "                        num_layer = -2, #penultimate\n",
    "                        label_id = config['target_label_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<mask> games and how<mask><mask> much do I hate<mask> wearing games.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_text_old  = []\n",
    "for example in running_text:\n",
    "    masked_text_old_  = locate_main(example, \n",
    "                                config[\"locate_method\"], \n",
    "                                name2model[config[\"model_paths\"][1]], \n",
    "                                name2tokenizer[config[\"tokenizer_paths\"][1]], \n",
    "                                max_num_tokens = wandb.config.num_edit_token_per_step, \n",
    "                                unit=config[\"locate_unit\"], \n",
    "                                device=\"cuda\", \n",
    "                                label_id=config[\"target_label_ids\"][1],\n",
    "                                num_layer=10)\n",
    "    masked_text_old.append(masked_text_old_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<mask> games and how<mask><mask> much do I hate<mask> wearing games.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_text_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_text_old == masked_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "running_text = best_text = deepcopy(AR_prediction_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_text = locator.locate_main(running_text, \n",
    "                        method = config['locate_method'], \n",
    "                        max_num_tokens = wandb.config.num_edit_token_per_step, \n",
    "                        unit = config['locate_unit'], \n",
    "                        num_layer = -2, #penultimate\n",
    "                        label_id = config['target_label_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<mask><mask><mask> the majority of<mask> end up in<mask> that you had to drive yourself. My only recourse is to feed it<mask><mask> What would be the happy tale of my life then?',\n",
       " '<mask> games and holy ****ing<mask><mask> I hate<mask> wearing games.',\n",
       " \"<mask><mask><mask> it<mask> makes me want to<mask><mask> every time I see it on the internet, even though it's not worth a thing because I am a<mask><mask>\"]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_text_old  = []\n",
    "for example in running_text:\n",
    "    masked_text_old_  = locate_main(example, \n",
    "                                config[\"locate_method\"], \n",
    "                                name2model[config[\"model_paths\"][1]], \n",
    "                                name2tokenizer[config[\"tokenizer_paths\"][1]], \n",
    "                                max_num_tokens = wandb.config.num_edit_token_per_step, \n",
    "                                unit=config[\"locate_unit\"], \n",
    "                                device=\"cuda\", \n",
    "                                label_id=config[\"target_label_ids\"][1],\n",
    "                                num_layer=10)\n",
    "    masked_text_old.append(masked_text_old_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<mask><mask><mask> the majority of<mask> end up in<mask> that you had to drive yourself. My only recourse is to feed it<mask><mask> What would be the happy tale of my life then?',\n",
       " '<mask> games and holy ****ing<mask><mask> I hate<mask> wearing games.',\n",
       " \"<mask><mask><mask> it<mask> makes me want to<mask><mask> every time I see it on the internet, even though it's not worth a thing because I am a<mask><mask>\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_text_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_text_old == masked_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for detail for logics handling batch with samples with different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' dirt. Unfortunately the majority of horses end up in shit that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " ' wearing games and holy ****ing shit do I hate horse wearing games.',\n",
       " \" fetishes: it just makes me want to puke every time I see it on the internet, even though it's not worth a thing because I am a furry.\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "prediction = running_text\n",
    "tokenizer = lossfns[1].tokenizer\n",
    "model = lossfns[1].model\n",
    "device = config['device']\n",
    "label_id = 1\n",
    "\n",
    "punctuations = string.punctuation + '\\n '\n",
    "punctuations = list(punctuations)\n",
    "punctuations.remove('-')\n",
    "stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations + [token for token in tokenizer.special_tokens_map.values()]\n",
    "stopwords_ids = tokenizer.batch_encode_plus(stopwords, return_tensors=\"pt\",add_special_tokens=False)['input_ids'].squeeze().to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(prediction, add_special_tokens=False, padding=True, truncation=True, return_tensors=\"pt\").to(model.device) # prediction이 list여도 처리가능함\n",
    "lengths = batch.attention_mask.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch, output_hidden_states=True)\n",
    "## output['hidden_states']: tuple of length num_hidden_layers\n",
    "## output['hidden_states'][0]: (batch_size, seq_len, hidden_size)\n",
    "layer = output['hidden_states'][0]\n",
    "layer.retain_grad()\n",
    "\n",
    "softmax=torch.nn.Softmax(dim=-1)\n",
    "probs = softmax(output['logits'])[:,label_id]\n",
    "probs.sum().backward(retain_graph=True) ## NOTE. https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments/47026836#47026836\n",
    "## layer.grad : (batch_size, seq_len, hidden_size)\n",
    "norm = torch.norm(layer.grad, dim=-1)\n",
    "## norm : (batch_size, seq_len)\n",
    "token_wise_scores = torch.where(norm > 0, norm, torch.full_like(norm, 1e-10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0.029,     0.015,     0.030,     0.014,     0.027,     0.027,\n",
       "             0.098,     0.037,     0.032,     0.034,     0.156,     0.030,\n",
       "             0.021,     0.018,     0.018,     0.058,     0.043,     0.021,\n",
       "             0.013,     0.011,     0.034,     0.014,     0.012,     0.038,\n",
       "             0.015,     0.022,     0.016,     0.020,     0.009,     0.008,\n",
       "             0.010,     0.018,     0.019,     0.011,     0.010,     0.018,\n",
       "             0.009,     0.013],\n",
       "        [    0.006,     0.005,     0.004,     0.008,     0.006,     0.005,\n",
       "             0.017,     0.006,     0.003,     0.005,     0.005,     0.006,\n",
       "             0.004,     0.003,     0.000,     0.000,     0.000,     0.000,\n",
       "             0.000,     0.000,     0.000,     0.000,     0.000,     0.000,\n",
       "             0.000,     0.000,     0.000,     0.000,     0.000,     0.000,\n",
       "             0.000,     0.000,     0.000,     0.000,     0.000,     0.000,\n",
       "             0.000,     0.000],\n",
       "        [    0.025,     0.020,     0.018,     0.014,     0.016,     0.020,\n",
       "             0.016,     0.027,     0.021,     0.041,     0.136,     0.023,\n",
       "             0.015,     0.012,     0.018,     0.020,     0.015,     0.012,\n",
       "             0.040,     0.014,     0.012,     0.010,     0.009,     0.008,\n",
       "             0.012,     0.018,     0.011,     0.014,     0.019,     0.017,\n",
       "             0.021,     0.033,     0.157,     0.113,     0.000,     0.000,\n",
       "             0.000,     0.000]], device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_wise_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_wise_scores_wrong = token_wise_scores.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.028, 0.027, 0.026, 0.026,\n",
       "         0.030, 0.026, 0.026, 0.026, 0.026, 0.027, 0.027, 0.026, 0.026, 0.026,\n",
       "         0.026, 0.026, 0.026, 0.027, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026,\n",
       "         0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026],\n",
       "        [0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.027, 0.026, 0.026, 0.026,\n",
       "         0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026,\n",
       "         0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026,\n",
       "         0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026],\n",
       "        [0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.027,\n",
       "         0.029, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.027, 0.026,\n",
       "         0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026,\n",
       "         0.026, 0.026, 0.030, 0.029, 0.026, 0.026, 0.026, 0.026]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_wise_scores_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_wise_scores[batch.attention_mask == 0] = -float(\"inf\")\n",
    "token_wise_scores = token_wise_scores.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.028, 0.027, 0.026, 0.026,\n",
       "         0.030, 0.026, 0.026, 0.026, 0.026, 0.027, 0.027, 0.026, 0.026, 0.026,\n",
       "         0.026, 0.026, 0.026, 0.027, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026,\n",
       "         0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026],\n",
       "        [0.071, 0.071, 0.071, 0.072, 0.071, 0.071, 0.072, 0.071, 0.071, 0.071,\n",
       "         0.071, 0.071, 0.071, 0.071, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
       "         0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
       "         0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "        [0.029, 0.029, 0.029, 0.029, 0.029, 0.029, 0.029, 0.029, 0.029, 0.030,\n",
       "         0.033, 0.029, 0.029, 0.029, 0.029, 0.029, 0.029, 0.029, 0.030, 0.029,\n",
       "         0.029, 0.029, 0.029, 0.029, 0.029, 0.029, 0.029, 0.029, 0.029, 0.029,\n",
       "         0.029, 0.030, 0.033, 0.032, 0.000, 0.000, 0.000, 0.000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_wise_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_wise_scores[torch.isin(batch.input_ids, stopwords_ids)]=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punc_len_wrong=(~torch.isin(batch.input_ids, stopwords_ids)).sum(dim=-1) # tensor([2, 4], device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32, 12, 31], device='cuda:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punc_len_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punc_len=(~torch.isin(batch.input_ids, stopwords_ids)*batch.attention_mask).sum(dim=-1) # tensor([2, 4], device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32, 12, 31], device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punc_len ## 차이 없음 이유: stopwords에 special tokens들도 들어가서 pad token도 포함되어서 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_values=token_wise_scores.sum(dim=-1)/no_punc_len # tensor([0.5120, 0.3744], device='cuda:0', grad_fn=<DivBackward0>)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc-edit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

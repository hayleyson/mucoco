{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c0144f-2626-49d3-b11e-40e07076cee6",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Notebook to prototype \n",
    "1. How to turn EM into a MLM. (With LM head initialized with vanilla Roberta MLM head)\n",
    "2. How to add \"mask\" token to the EM-based MLM when the EM shares GPT2 embedding and has no mask token.\n",
    "3. Quickly check the candidate EM-based MLM generates for located indices.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c274fa4-b8fd-45ef-aabe-c45ef9b97708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from collections import namedtuple\n",
    "project_dir = \"/home/s3/hyeryung/mucoco\" #'/home/hyeryung_son/mucoco' # \n",
    "sys.path.append(project_dir)\n",
    "os.chdir(project_dir)\n",
    "\n",
    "# installed packages\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "# custom libraries\n",
    "import mucoco.losses as lossbuilder\n",
    "import mucoco.options as options\n",
    "import mucoco.utils as utils\n",
    "from mucoco.utils import TargetProbability, TargetEmbeddings, TargetSimplex, Lambda, Optimizer, OptimizerLE, get_epsilon, locate\n",
    "from new_module.evaluation.evaluate_wandb import evaluate\n",
    "from new_module.decode_utils import score_hypotheses, constrained_beam_search_v0\n",
    "from new_module.utils.robertacustom_em_for_cand_gen import RobertaCustomForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0446b3a8-19a7-4913-a01a-fed9b9cbcf2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# How to turn EM into a MLM. (With LM head initialized with vanilla Roberta MLM head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac73b35-3e3a-43f3-9b7d-a419ad861153",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I implemented RobertaCustomForMaskedLM class. That is a Roberta MLM architecture that shares gpt2 embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fce71f-b7c8-4968-ba70-83ef4f462eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Double checking the implementation\n",
    "## Seems alright. (forward is just to meet the requirement. __init__ is what's important.)\n",
    "## The key is where I define new_embeds and set it as input_embeddings in __init__\n",
    "# class RobertaCustomForMaskedLM(RobertaCustomPreTrainedModel):\n",
    "#     _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n",
    "\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "\n",
    "#         if config.is_decoder:\n",
    "#             logger.warning(\n",
    "#                 \"If you want to use `RobertaForMaskedLM` make sure `config.is_decoder=False` for \"\n",
    "#                 \"bi-directional self-attention.\"\n",
    "#             )\n",
    "\n",
    "\n",
    "#         self.num_labels = config.num_labels\n",
    "#         self.config = config\n",
    "#         # print(config.vocab_size)\n",
    "\n",
    "\n",
    "#         self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "#         embeds = self.roberta.get_input_embeddings()\n",
    "#         old_dim = getattr(config,'n_embd', embeds.embedding_dim)\n",
    "#         new_dim = getattr(config,'new_n_embd', None)\n",
    "#         new_vocab_size = getattr(config,'new_vocab_size', config.vocab_size)\n",
    "#         if new_dim is not None:\n",
    "#             new_embeds = nn.Sequential(nn.Embedding(new_vocab_size, new_dim), nn.Linear(new_dim, old_dim, bias=False))\n",
    "#             self.roberta.set_input_embeddings(new_embeds)\n",
    "\n",
    "#         self.lm_head = RobertaLMHead(config)\n",
    "\n",
    "#         # Initialize weights and apply final processing\n",
    "#         self.post_init()\n",
    "\n",
    "#     def forward(\n",
    "#             self,\n",
    "#             input_ids: Optional[torch.LongTensor] = None,\n",
    "#             attention_mask: Optional[torch.FloatTensor] = None,\n",
    "#             token_type_ids: Optional[torch.LongTensor] = None,\n",
    "#             position_ids: Optional[torch.LongTensor] = None,\n",
    "#             head_mask: Optional[torch.FloatTensor] = None,\n",
    "#             inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "#             encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "#             encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "#             labels: Optional[torch.LongTensor] = None,\n",
    "#             output_attentions: Optional[bool] = None,\n",
    "#             output_hidden_states: Optional[bool] = None,\n",
    "#             return_dict: Optional[bool] = None,\n",
    "#         ) -> Union[Tuple[torch.Tensor], MaskedLMOutput]:\n",
    "#             r\"\"\"\n",
    "#             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "#                 Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n",
    "#                 config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n",
    "#                 loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n",
    "#             kwargs (`Dict[str, any]`, optional, defaults to *{}*):\n",
    "#                 Used to hide legacy arguments that have been deprecated.\n",
    "#             \"\"\"\n",
    "#             return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "#             outputs = self.roberta(\n",
    "#                 input_ids,\n",
    "#                 attention_mask=attention_mask,\n",
    "#                 token_type_ids=token_type_ids,\n",
    "#                 position_ids=position_ids,\n",
    "#                 head_mask=head_mask,\n",
    "#                 inputs_embeds=inputs_embeds,\n",
    "#                 encoder_hidden_states=encoder_hidden_states,\n",
    "#                 encoder_attention_mask=encoder_attention_mask,\n",
    "#                 output_attentions=output_attentions,\n",
    "#                 output_hidden_states=output_hidden_states,\n",
    "#                 return_dict=return_dict,\n",
    "#             )\n",
    "#             sequence_output = outputs[0]\n",
    "#             prediction_scores = self.lm_head(sequence_output)\n",
    "\n",
    "#             masked_lm_loss = None\n",
    "#             if labels is not None:\n",
    "#                 # move labels to correct device to enable model parallelism\n",
    "#                 labels = labels.to(prediction_scores.device)\n",
    "#                 loss_fct = CrossEntropyLoss()\n",
    "#                 masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "#             if not return_dict:\n",
    "#                 output = (prediction_scores,) + outputs[2:]\n",
    "#                 return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "#             return MaskedLMOutput(\n",
    "#                 loss=masked_lm_loss,\n",
    "#                 logits=prediction_scores,\n",
    "#                 hidden_states=outputs.hidden_states,\n",
    "#                 attentions=outputs.attentions,\n",
    "#             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a380d3-5ef8-481d-be76-3656022b37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={'model_paths': ['gpt2-large', \"models/roberta-base-jigsaw-toxicity-mlm-with-gpt2-large-embeds/checkpoint_best\"],\n",
    "       'device': 'cuda',\n",
    "       'k_per_location': 10}\n",
    "\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(config['model_paths'][1])\n",
    "mlm_config = AutoConfig.from_pretrained(config['model_paths'][1])\n",
    "mlm = RobertaCustomForMaskedLM.from_pretrained(config['model_paths'][1], config=mlm_config)\n",
    "\n",
    "original_mlm_model = AutoModelForMaskedLM.from_pretrained('roberta-base', cache_dir='hf_cache')\n",
    "mlm.lm_head = original_mlm_model.lm_head ## <- This line does the magic of replacing the custom mlm's lm head with that of roberta-base.\n",
    "## This trick does work b/c when I take the regular RobertaForSequenceClassification and does the trick, it works pretty well as a mlm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b464aef-be35-43d8-864a-3131f27b9215",
   "metadata": {},
   "source": [
    "# How to add \"mask\" token to the EM-based MLM when the EM shares GPT2 embedding and has no mask token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ee42b631-0734-4d36-b76a-b428cbe351e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "config={'model_paths': ['gpt2-large', \"models/roberta-base-jigsaw-toxicity-mlm-with-gpt2-large-embeds/checkpoint_best\"],\n",
    "       'device': 'cuda',\n",
    "       'k_per_location': 10}\n",
    "\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(config['model_paths'][1])\n",
    "mlm_config = AutoConfig.from_pretrained(config['model_paths'][1])\n",
    "mlm = RobertaCustomForMaskedLM.from_pretrained(config['model_paths'][1], config=mlm_config)\n",
    "\n",
    "original_mlm_model = AutoModelForMaskedLM.from_pretrained('roberta-base', cache_dir='hf_cache')\n",
    "mlm.lm_head = original_mlm_model.lm_head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "027f2802-58cc-4fb1-a1e7-18f4bf35dbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/vocab.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Add mask token to tokenizers\n",
    "original_mlm_tokenizer = AutoTokenizer.from_pretrained('roberta-base', cache_dir='hf_cache')\n",
    "\n",
    "mlm_tokenizer.add_special_tokens({'mask_token': original_mlm_tokenizer.mask_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "03a40879-5113-43bf-a1b4-006d5a4e69b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50258. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/vocab.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Embedding(50258, 1280)\n",
       "  (1): Linear(in_features=1280, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Add embedding for mask token\n",
    "## I implemented this method for RobertaCustomPreTrainedModel which RobertaCustomForMaskedLM inherits.\n",
    "mlm.add_mask_token_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "50e9a14d-7b56-4f49-a0ee-cc7a89d02bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaCustomForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Sequential(\n",
       "        (0): Embedding(50258, 1280)\n",
       "        (1): Linear(in_features=1280, out_features=768, bias=False)\n",
       "      )\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm.to(config['device'])\n",
    "mlm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e61c01e8-69d9-4376-b0fe-dc8c90ec3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Double checking if the implementation is correct. \n",
    "## The code seems correct to me.\n",
    "# class RobertaCustomPreTrainedModel(PreTrainedModel):\n",
    "\n",
    "#     \"\"\"\n",
    "#     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "#     models.\n",
    "#     \"\"\"\n",
    "\n",
    "#     config_class = RobertaConfig\n",
    "#     base_model_prefix = \"roberta\"\n",
    "#     supports_gradient_checkpointing = True\n",
    "#     _no_split_modules = [\"RobertaEmbeddings\", \"RobertaSelfAttention\"]\n",
    "        \n",
    "#     def add_mask_token_embedding(\n",
    "#         self, pad_to_multiple_of: Optional[int] = None\n",
    "#     ) -> nn.Sequential:\n",
    "        \n",
    "#         model_embeds = self._add_mask_token_embedding(pad_to_multiple_of)\n",
    "\n",
    "#         # Update base model and current model config\n",
    "#         self.config.vocab_size = model_embeds[0].weight.shape[0]\n",
    "#         self.vocab_size = model_embeds[0].weight.shape[0]\n",
    "\n",
    "#         # Tie weights again if needed\n",
    "#         self.tie_weights()\n",
    "\n",
    "#         return model_embeds\n",
    "\n",
    "#     def _add_mask_token_embedding(self, pad_to_multiple_of=None):\n",
    "#         old_embeddings = self.get_input_embeddings()\n",
    "#         new_num_tokens = old_embeddings[0].weight.shape[0] + 1\n",
    "#         new_embeddings = self._get_embeddings_with_mask_token_embedding(old_embeddings, new_num_tokens, pad_to_multiple_of)\n",
    "        \n",
    "#         self.set_input_embeddings(new_embeddings)\n",
    "\n",
    "#         new_num_tokens = new_embeddings[0].weight.shape[0]\n",
    "\n",
    "#         # if word embeddings are not tied, make sure that lm head is resized as well\n",
    "#         if self.get_output_embeddings() is not None and not self.config.tie_word_embeddings:\n",
    "#             old_lm_head = self.get_output_embeddings()\n",
    "#             new_lm_head = self._get_resized_lm_head(old_lm_head, new_num_tokens)\n",
    "#             if hasattr(old_lm_head, \"_hf_hook\"):\n",
    "#                 hook = old_lm_head._hf_hook\n",
    "#                 add_hook_to_module(new_lm_head, hook)\n",
    "#             old_lm_head_requires_grad = old_lm_head.weight.requires_grad\n",
    "#             new_lm_head.requires_grad_(old_lm_head_requires_grad)\n",
    "#             self.set_output_embeddings(new_lm_head)\n",
    "\n",
    "#         return self.get_input_embeddings()\n",
    "\n",
    "#     def _get_embeddings_with_mask_token_embedding(\n",
    "#         self,\n",
    "#         old_embeddings: nn.Sequential,\n",
    "#         new_num_tokens: Optional[int] = None,\n",
    "#         pad_to_multiple_of: Optional[int] = None,\n",
    "#     ) -> nn.Sequential:\n",
    "\n",
    "#         if pad_to_multiple_of is not None:\n",
    "#             if not isinstance(pad_to_multiple_of, int):\n",
    "#                 raise ValueError(\n",
    "#                     f\"Asking to pad the embedding matrix to a multiple of `{pad_to_multiple_of}`, which is not and integer. Please make sure to pass an integer\"\n",
    "#                 )\n",
    "#             new_num_tokens = ((new_num_tokens + pad_to_multiple_of - 1) // pad_to_multiple_of) * pad_to_multiple_of\n",
    "#         else:\n",
    "#             logger.info(\n",
    "#                 \"You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding\"\n",
    "#                 f\" dimension will be {new_num_tokens}. This might induce some performance reduction as *Tensor Cores* will not be available.\"\n",
    "#                 \" For more details about this, or help on choosing the correct value for resizing, refer to this guide:\"\n",
    "#                 \" https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\"\n",
    "#             )\n",
    "\n",
    "#         # Sequential(\n",
    "#         #   (0): Embedding(50257, 1280)\n",
    "#         #   (1): Linear(in_features=1280, out_features=768, bias=False)\n",
    "#         # )\n",
    "#         old_num_tokens, old_intermediate_embedding_dim = old_embeddings[0].weight.size()\n",
    "        \n",
    "#         # Build new embeddings\n",
    "#         new_intermediate_embeddings = nn.Embedding(\n",
    "#             new_num_tokens,\n",
    "#             old_intermediate_embedding_dim,\n",
    "#             device=old_embeddings[0].weight.device,\n",
    "#             dtype=old_embeddings[0].weight.dtype,\n",
    "#         )\n",
    "\n",
    "#         # initialize all new embeddings (in particular added tokens)\n",
    "#         self._init_weights(new_intermediate_embeddings)\n",
    "\n",
    "#         # Copy token embeddings from the previous weights\n",
    "\n",
    "#         # numbers of tokens to copy\n",
    "#         n = min(old_num_tokens, new_num_tokens)\n",
    "\n",
    "#         new_intermediate_embeddings.weight.data[:n, :] = old_embeddings[0].weight.data[:n, :]\n",
    "        \n",
    "#         # also set the weight of the last token by taking inverse. y W^T (W W^T)^{-1} \n",
    "#         ## -> for numerical stability, instead of taking inverse, use linalg.solve\n",
    "#         ## 1) get the mask token embedding of Roberta\n",
    "#         mlm = AutoModelForMaskedLM.from_pretrained('roberta-base')\n",
    "#         mlm_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "#         y = mlm.get_input_embeddings().weight.data[mlm_tokenizer.mask_token_id, :].unsqueeze(-1)\n",
    "        \n",
    "#         ## 2) calculate this -> we want to find solution x that satisfies W x = y where W is the embedding projection layer in the custom roberta that projects gpt2 embedding into roberta embedding\n",
    "#         ## What we hope is to find x such that it is projected to a mask token embedding and EM converted to MLM can hopefully recognize the embedding and treat it properly by generating candidates to fill it in. \n",
    "#         ## Since W is not a square matrix, first rewrite the equation as WTW x = WTy -> Now, it can be seen as Ax = B where A = WTW, B = WTy\n",
    "#         W = old_embeddings[1].weight\n",
    "#         A = W.t().matmul(W)\n",
    "#         B = W.t().matmul(y)\n",
    "#         x = torch.linalg.solve(A, B)\n",
    "        \n",
    "#         ## 3) set the weight of the last token\n",
    "#         new_intermediate_embeddings.weight.data[-1, :] = x.data.squeeze()\n",
    "\n",
    "#         # update requires_grad\n",
    "#         old_embeddings_requires_grad = old_embeddings[0].weight.requires_grad\n",
    "#         new_intermediate_embeddings.requires_grad_(old_embeddings_requires_grad)\n",
    "\n",
    "#         new_embeddings = nn.Sequential(new_intermediate_embeddings, \n",
    "#                                        old_embeddings[1])\n",
    "            \n",
    "#         return new_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8bbbff8b-a612-4b3f-8924-de21a82c1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cross checking if our embedding for mask token is close to the roberta's mask token embedding.\n",
    "\n",
    "mask_token_embedding = mlm.get_input_embeddings()(torch.LongTensor([mlm_tokenizer.mask_token_id]).to(config['device']))\n",
    "mask_token_embedding_original = original_mlm_model.get_input_embeddings()(torch.LongTensor([original_mlm_tokenizer.mask_token_id]).to(config['device']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ff678b36-7aac-42b9-afea-441f609f069f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 0.0051\n",
      "min: -0.0041\n",
      "abs max: 0.0051\n",
      "abs min: 0.0000\n",
      "abs mean: 0.0007\n",
      "abs std: 0.0006\n"
     ]
    }
   ],
   "source": [
    "## Difference of -0.004~0.005 seems reasonable to me.\n",
    "embed_diff = (mask_token_embedding - mask_token_embedding_original)\n",
    "\n",
    "print(f\"max: {embed_diff.max():.4f}\")\n",
    "print(f\"min: {embed_diff.min():.4f}\")\n",
    "print(f\"abs max: {embed_diff.abs().max():.4f}\")\n",
    "print(f\"abs min: {embed_diff.abs().min():.4f}\")\n",
    "print(f\"abs mean: {embed_diff.abs().mean():.4f}\")\n",
    "print(f\"abs std: {embed_diff.abs().std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9c501eed-605a-4b41-89cb-eae99e6c49da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x,y):\n",
    "    x = x.squeeze()\n",
    "    y = y.squeeze()\n",
    "    num = x.dot(y)\n",
    "    denom = torch.sqrt(x.dot(x)*y.dot(y))\n",
    "    print(f\"x dot y: {num:.10f}\")\n",
    "    print(f\"||x|| * ||y||: {denom:.10f}\")\n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "359d0cfa-1c31-4830-8fb4-288693c003fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: tensor([1.000], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "x dot y: 4.0875220299\n",
      "||x|| * ||y||: 4.0878634453\n",
      "Cosine similarity: 0.9999164938926697\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cosine similarity: {nn.functional.cosine_similarity(mask_token_embedding, mask_token_embedding_original)}\")\n",
    "print(f\"Cosine similarity: {cosine_similarity(mask_token_embedding, mask_token_embedding_original)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "399fcc37-b6c7-4a1e-925a-588b30d060e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How far is the embedding from other embeddings? \n",
    "# random_embedding = mlm.get_input_embeddings()(torch.LongTensor([random.randint(0, len(mlm_tokenizer))]).to(config['device']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "92f53eaa-e8f3-43bf-baf3-883c732a3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the median of other embeddings of robertacustom mlm (except for mask token embedding)\n",
    "embeddings = []\n",
    "for index in range(0, len(mlm_tokenizer)-1):\n",
    "    embeddings.append(mlm.get_input_embeddings()(torch.LongTensor([index]).to(config['device'])))\n",
    "embeddings = torch.stack(embeddings, dim=0)\n",
    "median_embedding = torch.median(embeddings, dim=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "431ac9ea-9f88-4d5d-be5f-4fae0e1bce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 0.9583\n",
      "min: -1.0497\n",
      "abs max: 1.0497\n",
      "abs min: 0.0002\n",
      "abs mean: 0.0291\n",
      "abs std: 0.0720\n",
      "tensor(-0.112, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "tensor(1.126, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
      "Cosine similarity: -0.09932808578014374\n"
     ]
    }
   ],
   "source": [
    "## The gap is larger than that from original mask token embedding \n",
    "## -> Is this a reason why even after initializing x properly, mask token does not seem to be recognized? \n",
    "## (To understand what I mean, please look at the below section where I checked candidates the EM-based MLM generates)\n",
    "embed_diff = (mask_token_embedding - median_embedding)\n",
    "\n",
    "print(f\"max: {embed_diff.max():.4f}\")\n",
    "print(f\"min: {embed_diff.min():.4f}\")\n",
    "print(f\"abs max: {embed_diff.abs().max():.4f}\")\n",
    "print(f\"abs min: {embed_diff.abs().min():.4f}\")\n",
    "print(f\"abs mean: {embed_diff.abs().mean():.4f}\")\n",
    "print(f\"abs std: {embed_diff.abs().std():.4f}\")\n",
    "print(f\"Cosine similarity: {cosine_similarity(mask_token_embedding, median_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1fbfbbb4-73b1-4883-9ca6-4db20a4fb827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint were not used when initializing RobertaForMaskedLM: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at /shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint and are newly initialized: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## Added 23/01/19 : check if the mask token embedding in RobertaForSequenceClassification trained as an EM \n",
    "##                             diverged a lot from the original mask token embedding and\n",
    "##                               check how different it is from the other token embeddings that are likely be gradient-descended.\n",
    "config={'device': 'cuda',\n",
    "       'k_per_location': 3}\n",
    "config['model_paths'] = ['gpt2-large', \"/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint\"]\n",
    "\n",
    "roberta_em_mlm = AutoModelForMaskedLM.from_pretrained(config['model_paths'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b4b41565-b3fb-46f3-866a-26135034f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained(config['model_paths'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b447f048-d24c-4b8b-9a0f-da923629e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_mlm_tokenizer = AutoTokenizer.from_pretrained(config['model_paths'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "056fefa2-4841-405d-ae3f-cc4026a57b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "original_mlm_model = AutoModelForMaskedLM.from_pretrained('roberta-base', cache_dir='hf_cache')\n",
    "roberta_em_mlm.lm_head = original_mlm_model.lm_head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "616564bb-f68a-47f9-8296-16c642ad28f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_em_mlm.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "413d9391-f444-498c-9cf7-cfbfe27a30a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_embedding = roberta_em_mlm.get_input_embeddings()(torch.LongTensor([roberta_tokenizer.mask_token_id]).to(config['device']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "419b9e23-2bd4-47bd-911d-07b9846e9698",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_embedding_original = original_mlm_model.get_input_embeddings()(torch.LongTensor([original_mlm_tokenizer.mask_token_id]).to(config['device']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "013261cc-7c18-481d-b8cf-2b017cb56368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 0.0001\n",
      "min: -0.0001\n",
      "abs max: 0.0001\n",
      "abs min: 0.0000\n",
      "abs mean: 0.0000\n",
      "abs std: 0.0000\n",
      "tensor(4.086, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "tensor(4.086, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
      "Cosine similarity: 1.0\n"
     ]
    }
   ],
   "source": [
    "## Difference of -0.004~0.005 seems reasonable to me.\n",
    "embed_diff = (mask_token_embedding - mask_token_embedding_original)\n",
    "\n",
    "print(f\"max: {embed_diff.max():.4f}\")\n",
    "print(f\"min: {embed_diff.min():.4f}\")\n",
    "print(f\"abs max: {embed_diff.abs().max():.4f}\")\n",
    "print(f\"abs min: {embed_diff.abs().min():.4f}\")\n",
    "print(f\"abs mean: {embed_diff.abs().mean():.4f}\")\n",
    "print(f\"abs std: {embed_diff.abs().std():.4f}\")\n",
    "print(f\"Cosine similarity: {cosine_similarity(mask_token_embedding, mask_token_embedding_original)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7d89d618-da3e-4e94-8f98-e4da3afe28c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the median of other embeddings of robertacustom mlm (except for mask token embedding)\n",
    "embeddings = []\n",
    "for index in range(0, len(roberta_tokenizer)-1):\n",
    "    embeddings.append(roberta_em_mlm.get_input_embeddings()(torch.LongTensor([index]).to(config['device'])))\n",
    "embeddings = torch.stack(embeddings, dim=0)\n",
    "median_embedding = torch.median(embeddings, dim=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "179dcf35-14fc-4a9b-9caa-1fff3d65ac0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 0.9452\n",
      "min: -1.0451\n",
      "abs max: 1.0451\n",
      "abs min: 0.0000\n",
      "abs mean: 0.0283\n",
      "abs std: 0.0716\n",
      "tensor(-0.090, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "tensor(1.068, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
      "Cosine similarity: -0.0846225693821907\n"
     ]
    }
   ],
   "source": [
    "## The gap is larger than that from original mask token embedding \n",
    "## -> Is this a reason why even after initializing x properly, mask token does not seem to be recognized? \n",
    "## (To understand what I mean, please look at the below section where I checked candidates the EM-based MLM generates)\n",
    "embed_diff = (mask_token_embedding - median_embedding)\n",
    "\n",
    "print(f\"max: {embed_diff.max():.4f}\")\n",
    "print(f\"min: {embed_diff.min():.4f}\")\n",
    "print(f\"abs max: {embed_diff.abs().max():.4f}\")\n",
    "print(f\"abs min: {embed_diff.abs().min():.4f}\")\n",
    "print(f\"abs mean: {embed_diff.abs().mean():.4f}\")\n",
    "print(f\"abs std: {embed_diff.abs().std():.4f}\")\n",
    "print(f\"Cosine similarity: {cosine_similarity(mask_token_embedding, median_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1442070-cb79-4687-ab6a-1310cb6b391c",
   "metadata": {},
   "source": [
    "# Quickly check the candidate EM-based MLM generates for located indices.\n",
    "\n",
    "## Results\n",
    "- original sentence: `wearing games and holy ****ing shit do I hate horse wearing games.`\n",
    "- masked sentence `(masking locations determined manually): wearing games and<mask><mask><mask><mask> do I hate horse wearing games.`\n",
    "- roberta custom em (mask token added) turned mlm : `wearing games and indoors indoors indoors indoors do I hate horse wearing games.`\n",
    "- roberta em turned mlm : `wearing games and beating you. I do I hate horse wearing games.`\n",
    "- roberta mlm : `wearing games and riding horses. Why do I hate horse wearing games.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d8a7d8-7ad0-44d5-82b3-d10fdbe5c3c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Codes that are common to below test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ebc8f7f-e298-471e-832a-76bd2705c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_sentence = \"wearing games and holy ****ing shit do I hate horse wearing games.\"\n",
    "test_sentence = \"wearing games and<mask><mask><mask><mask> do I hate horse wearing games.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "211195a9-6682-4836-88d8-70166c66d7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/vocab.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/vocab.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "original_mlm_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "primary_tokenizer = AutoTokenizer.from_pretrained(config['model_paths'][0])\n",
    "primary_tokenizer.add_special_tokens({'mask_token':original_mlm_tokenizer.mask_token})\n",
    "primary_mask_token_id = primary_tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "27983b2f-79da-4c23-878d-f1e3c5da3eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={'device': 'cuda',\n",
    "       'k_per_location': 3}\n",
    "config['target_label_ids'] = [0,0]\n",
    "config['losses'] = ['gpt2', 'classification_no_prefix']\n",
    "config['cache_dir'] = 'hf_cache'\n",
    "config['target_type'] = 'embeds'\n",
    "config['min_epsilons'] = [0.75]\n",
    "config['build_loss_dict'] = {\"coeff_steps\": 200, \"coeff_pattern\": \"constant\", \"loss_type\": \"xentropy\", \"length_normalize\": \"false\", \"AR_temperature\": 1.0, \"AR_top_k\": 0, \"AR_top_p\": 0.96, \"max_output_length\": 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa885a-96b1-4622-a73a-b329d8ece940",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparation needed to score hypotheses\n",
    "wandb.init(config={\"closs_weight\": 0.5})\n",
    "source_batch = torch.LongTensor([[mlm_tokenizer.bos_token_id]]).cuda()\n",
    "class dummyArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "build_loss_args=dummyArgs(**config['build_loss_dict'])\n",
    "name2tokenizer = {}\n",
    "name2model = {}\n",
    "name2config = {}\n",
    "loss2modelname = {}\n",
    "loss2tokenizer = {}\n",
    "embed_luts = []\n",
    "embed_scales = []\n",
    "prev_vocab_size = None\n",
    "vocab_size = None\n",
    "primary_vocab_size = None\n",
    "primary_model = None\n",
    "gold_losses = []\n",
    "label_ids = config['target_label_ids'] # target label's ids for each loss\n",
    "keywords = [\"the\" for _ in config['losses']]\n",
    "new_kweight = 5.0\n",
    "use_context = 'false'\n",
    "allsat = True\n",
    "additional_batch = source_batch\n",
    "context_batch = [None]\n",
    "\n",
    "for i, model_path in enumerate(config['model_paths']):\n",
    "    if model_path not in name2model: #making sure we are not loading the model twice in case some constraints use the same model. \n",
    "        \n",
    "        try:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(config['tokenizer_paths'][i], cache_dir=config['cache_dir'],  use_fast=True)\n",
    "        except:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(config['tokenizer_paths'][i], cache_dir=config['cache_dir'],  use_fast=False)\n",
    "            \n",
    "        name2config[model_path] = AutoConfig.from_pretrained(model_path, cache_dir=config['cache_dir'])\n",
    "\n",
    "        if config['model_types'][i] == \"sentence-transformer\":\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(SentenceTransformer(model_path))\n",
    "        elif \"Custom\" in config['model_types'][i]:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(getattr(utils, config['model_types'][i]).from_pretrained(model_path, config=name2config[model_path], cache_dir=config['cache_dir']))\n",
    "        else:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(getattr(transformers, config['model_types'][i]).from_pretrained(model_path, config=name2config[model_path], cache_dir=config['cache_dir']))\n",
    "        name2model[model_path].eval()\n",
    "        name2model[model_path].cuda()\n",
    "        embed_lut_ = name2model[model_path].get_input_embeddings()\n",
    "        if isinstance(embed_lut_, torch.nn.Sequential):\n",
    "            new_vocab_size = embed_lut_[0].num_embeddings\n",
    "        else:\n",
    "            new_vocab_size = embed_lut_.num_embeddings\n",
    "        if prev_vocab_size is None:\n",
    "            vocab_size=new_vocab_size\n",
    "        prev_vocab_size = vocab_size\n",
    "    \n",
    "    input_embeds = name2model[model_path].get_input_embeddings()\n",
    "    if isinstance(input_embeds, torch.nn.Sequential):\n",
    "        input_embeds = input_embeds[0]\n",
    "    embed_luts.append(input_embeds)\n",
    "    \n",
    "    if config['target_type'] == \"embeds\":\n",
    "        embed_luts[-1].requires_grad=False\n",
    "    \n",
    "    if i == 0:\n",
    "        primary_vocab_size = vocab_size\n",
    "        primary_embed_dim = embed_luts[-1].embedding_dim\n",
    "        primary_model = name2model[model_path]\n",
    "    \n",
    "    if getattr(name2model[model_path], \"get_decoder\", None) is None: #this is for MarianMT models which have a weird embedding_scale parameter\n",
    "        embed_scales.append(1.0)\n",
    "    else:\n",
    "        embed_scales.append(getattr(name2model[model_path].get_decoder(), \"embed_scale\", 1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62262d71-36d0-4168-ba66-3e613eae3295",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## RobertaCustom With Mask Token Added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e5ebf-0133-484b-b472-b14d0cbe0f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['model_paths'] = ['gpt2-large', \"models/roberta-base-jigsaw-toxicity-mlm-with-gpt2-large-embeds/checkpoint_best\"]\n",
    "config['tokenizer_paths'] = config['model_paths']\n",
    "config['model_types'] = ['AutoModelForCausalLM', 'RobertaCustomForSequenceClassification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d7b2cbb2-d476-4cc9-b36e-49c01dda4a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 6, 7], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = mlm_tokenizer(test_sentence, return_tensors=\"pt\")\n",
    "lm_outputs = mlm(**inputs.to(config['device']))\n",
    "\n",
    "## get locations (indexes) in test_sentence that is filled with mask token.\n",
    "indices_in_mlm_tokens = (inputs.input_ids == mlm_tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "print(indices_in_mlm_tokens)\n",
    "\n",
    "## get top k tokens for each index\n",
    "logits = lm_outputs.logits\n",
    "predicted_token_ids = torch.topk(logits[0, indices_in_mlm_tokens], k=config['k_per_location'], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6c4602a2-c18c-4a85-b92a-9f7068b92c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wearing games and<mask><mask><mask><mask> do I hate horse wearing games.\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 0: 20\t-----\t5\n",
      "Candidate 1 @ 0: 152\t-----\t�\n",
      "Candidate 2 @ 0: 31797\t-----\t indoors\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 1: 20\t-----\t5\n",
      "Candidate 1 @ 1: 31797\t-----\t indoors\n",
      "Candidate 2 @ 1: 152\t-----\t�\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 2: 20\t-----\t5\n",
      "Candidate 1 @ 2: 31797\t-----\t indoors\n",
      "Candidate 2 @ 2: 152\t-----\t�\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 3: 20\t-----\t5\n",
      "Candidate 1 @ 3: 31797\t-----\t indoors\n",
      "Candidate 2 @ 3: 152\t-----\t�\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Check for candidates\n",
    "## Candidates at each locations are the same.. -> Problematic!\n",
    "print(test_sentence)\n",
    "print(\"-\"*50)\n",
    "for i in range(4):\n",
    "    for j in range(config['k_per_location']):\n",
    "        print(f\"Candidate {j} @ {i}: {predicted_token_ids.indices[i, j]}\\t-----\\t{mlm_tokenizer.decode(predicted_token_ids.indices[i, j])}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ed8c70ce-3620-4243-8221-25abcb826243",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = []\n",
    "num_located_tokens = len(indices_in_mlm_tokens)\n",
    "num_all_cases = config['k_per_location'] ** num_located_tokens\n",
    "tok_cand_combo = [0 for i in range(num_located_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7903849f-b5ac-40a2-a4b0-37d88161ae83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for case_id in range(num_all_cases):\n",
    "    # print(case_id)\n",
    "    for i in range(num_located_tokens):\n",
    "        tok_cand_combo[i] = (case_id // (config['k_per_location']**i)) % config['k_per_location']\n",
    "    \n",
    "    tmp_seq = inputs['input_ids'].clone()\n",
    "    for pos_id, tok_cand_id in enumerate(tok_cand_combo):\n",
    "        tmp_seq[0, indices_in_mlm_tokens[pos_id]] = predicted_token_ids.indices[pos_id, tok_cand_id]\n",
    "\n",
    "    # need to do decode with RobertaTokenizer and encode with GPT2Tokenizer\n",
    "    tmp_dec_seq = primary_tokenizer(mlm_tokenizer.batch_decode(tmp_seq, skip_special_tokens=True), return_tensors=\"pt\").input_ids.cuda()\n",
    "    hypotheses.append(tmp_dec_seq.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f6a145-7136-496e-835f-e31a913bff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code block to score hypotheses using GPT2 and EM.\n",
    "lossfns = []\n",
    "for i, loss in enumerate(config['losses']):\n",
    "    lossfns.append(lossbuilder.build_loss(loss, name2model[config['model_paths'][i]], name2tokenizer[config['model_paths'][i]], build_loss_args))\n",
    "    loss2modelname[loss] = config['model_paths'][i]\n",
    "    loss2tokenizer[loss] = name2tokenizer[config['model_paths'][i]]\n",
    "\n",
    "candidate_total_losses, candidate_primary_losses, candidate_losses_for_loggings = score_hypotheses(source_batch,\n",
    "                                                                                                hypotheses, \n",
    "                                                                                                config, \n",
    "                                                                                                lossfns,\n",
    "                                                                                        additional_batch=additional_batch, \n",
    "                                                                                        context_batch=context_batch,\n",
    "                                                                                        use_context=use_context,\n",
    "                                                                                        label_ids=label_ids,\n",
    "                                                                                        keywords=keywords,\n",
    "                                                                                        kweight=new_kweight)\n",
    "best_ix = np.argmin(np.array(candidate_total_losses))\n",
    "best_prediction = hypotheses[best_ix]\n",
    "best_text = primary_tokenizer.decode(best_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "621c3fa0-9947-4fd7-8ea7-dca5cd615176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: wearing games and<mask><mask><mask><mask> do I hate horse wearing games.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Best hypothesis: wearing games and indoors indoors indoors indoors do I hate horse wearing games.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Random sample of hypothesis: wearing games and��55 do I hate horse wearing games.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original sentence: {test_sentence}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Best hypothesis: {best_text}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Random sample of hypothesis: {primary_tokenizer.decode(hypotheses[random.randint(0, len(hypotheses))])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f25408b-acd5-4b9d-816a-31cb1a8b5e63",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ~RobertaCustom With Mask Token Added + Only use first 11 layers~\n",
    "- Hypothesis: Since the final layer of SeqClassificationModel is trained for SeqClassification task (using CLS token and what not), the candidate generation can improve if we do not use that final layer and only use the first few layers of transformer blocks when converting EM to MLM.\n",
    "- Result: Did not help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c87686-9e3c-42dd-b9ca-e21f5e05433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['model_paths'] = ['gpt2-large', \"models/roberta-base-jigsaw-toxicity-mlm-with-gpt2-large-embeds/checkpoint_best\"]\n",
    "config['tokenizer_paths'] = config['model_paths']\n",
    "config['model_types'] = ['AutoModelForCausalLM', 'RobertaCustomForSequenceClassification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2e8f05df-eedc-4fa7-bf29-4f7632319298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/roberta-base-jigsaw-toxicity-mlm-with-gpt2-large-embeds/checkpoint_best were not used when initializing RobertaCustomForMaskedLM: ['roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.weight']\n",
      "- This IS expected if you are initializing RobertaCustomForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustomForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50258. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/vocab.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/vocab.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_tokenizer = AutoTokenizer.from_pretrained(config['model_paths'][1])\n",
    "mlm_config = AutoConfig.from_pretrained(config['model_paths'][1])\n",
    "mlm_config.update({'num_hidden_layers':11}) # Line that does the magic of only using the first 11 transformer blocks.\n",
    "mlm = RobertaCustomForMaskedLM.from_pretrained(config['model_paths'][1], config=mlm_config)\n",
    "\n",
    "original_mlm_model = AutoModelForMaskedLM.from_pretrained('roberta-base', cache_dir='hf_cache')\n",
    "mlm.lm_head = original_mlm_model.lm_head\n",
    "\n",
    "mlm.add_mask_token_embedding()\n",
    "\n",
    "mlm.to(config['device'])\n",
    "mlm.eval()\n",
    "\n",
    "## Add mask token to tokenizers\n",
    "original_mlm_tokenizer = AutoTokenizer.from_pretrained('roberta-base', cache_dir='hf_cache')\n",
    "mlm_tokenizer.add_special_tokens({'mask_token': original_mlm_tokenizer.mask_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f5f56528-db3f-4bef-af61-3d1e2e8c6873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 6, 7], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = mlm_tokenizer(test_sentence, return_tensors=\"pt\")\n",
    "lm_outputs = mlm(**inputs.to(config['device']))\n",
    "\n",
    "## get locations (indexes) in test_sentence that is filled with mask token.\n",
    "indices_in_mlm_tokens = (inputs.input_ids == mlm_tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "print(indices_in_mlm_tokens)\n",
    "\n",
    "## get top k tokens for each index\n",
    "logits = lm_outputs.logits\n",
    "predicted_token_ids = torch.topk(logits[0, indices_in_mlm_tokens], k=config['k_per_location'], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "950873e1-291f-4843-9e27-1e35d83a385e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wearing games and<mask><mask><mask><mask> do I hate horse wearing games.\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 0: 7351\t-----\t serving\n",
      "Candidate 1 @ 0: 20496\t-----\t Latino\n",
      "Candidate 2 @ 0: 5253\t-----\t distance\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 1: 7351\t-----\t serving\n",
      "Candidate 1 @ 1: 20496\t-----\t Latino\n",
      "Candidate 2 @ 1: 1168\t-----\t Z\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 2: 7351\t-----\t serving\n",
      "Candidate 1 @ 2: 1168\t-----\t Z\n",
      "Candidate 2 @ 2: 20496\t-----\t Latino\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 3: 7351\t-----\t serving\n",
      "Candidate 1 @ 3: 20496\t-----\t Latino\n",
      "Candidate 2 @ 3: 1168\t-----\t Z\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Check for candidates\n",
    "## Candidates at each locations are the same.. -> Problematic!\n",
    "print(test_sentence)\n",
    "print(\"-\"*50)\n",
    "for i in range(4):\n",
    "    for j in range(config['k_per_location']):\n",
    "        print(f\"Candidate {j} @ {i}: {predicted_token_ids.indices[i, j]}\\t-----\\t{mlm_tokenizer.decode(predicted_token_ids.indices[i, j])}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "aaa569b7-f7c4-4e80-8c78-2b9f8b5bafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = []\n",
    "num_located_tokens = len(indices_in_mlm_tokens)\n",
    "num_all_cases = config['k_per_location'] ** num_located_tokens\n",
    "tok_cand_combo = [0 for i in range(num_located_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7ed95dea-afe0-4ae0-bbc5-01409eeb4a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for case_id in range(num_all_cases):\n",
    "    # print(case_id)\n",
    "    for i in range(num_located_tokens):\n",
    "        tok_cand_combo[i] = (case_id // (config['k_per_location']**i)) % config['k_per_location']\n",
    "    \n",
    "    tmp_seq = inputs['input_ids'].clone()\n",
    "    for pos_id, tok_cand_id in enumerate(tok_cand_combo):\n",
    "        tmp_seq[0, indices_in_mlm_tokens[pos_id]] = predicted_token_ids.indices[pos_id, tok_cand_id]\n",
    "\n",
    "    # need to do decode with RobertaTokenizer and encode with GPT2Tokenizer\n",
    "    tmp_dec_seq = primary_tokenizer(mlm_tokenizer.batch_decode(tmp_seq, skip_special_tokens=True), return_tensors=\"pt\").input_ids.cuda()\n",
    "    hypotheses.append(tmp_dec_seq.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "21e82ce9-d4c2-4287-b805-36dbcdaed8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code block to score hypotheses using GPT2 and EM.\n",
    "lossfns = []\n",
    "for i, loss in enumerate(config['losses']):\n",
    "    lossfns.append(lossbuilder.build_loss(loss, name2model[config['model_paths'][i]], name2tokenizer[config['model_paths'][i]], build_loss_args))\n",
    "    loss2modelname[loss] = config['model_paths'][i]\n",
    "    loss2tokenizer[loss] = name2tokenizer[config['model_paths'][i]]\n",
    "\n",
    "candidate_total_losses, candidate_primary_losses, candidate_losses_for_loggings = score_hypotheses(source_batch,\n",
    "                                                                                                hypotheses, \n",
    "                                                                                                config, \n",
    "                                                                                                lossfns,\n",
    "                                                                                        additional_batch=additional_batch, \n",
    "                                                                                        context_batch=context_batch,\n",
    "                                                                                        use_context=use_context,\n",
    "                                                                                        label_ids=label_ids,\n",
    "                                                                                        keywords=keywords,\n",
    "                                                                                        kweight=new_kweight)\n",
    "best_ix = np.argmin(np.array(candidate_total_losses))\n",
    "best_prediction = hypotheses[best_ix]\n",
    "best_text = primary_tokenizer.decode(best_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "585a8dd0-8e29-4049-ae5a-c0c3e3a63803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: wearing games and<mask><mask><mask><mask> do I hate horse wearing games.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Best hypothesis: wearing games and indoors indoors indoors indoors do I hate horse wearing games.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Random sample of hypothesis: wearing games and serving Z Z serving do I hate horse wearing games.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original sentence: {test_sentence}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Best hypothesis: {best_text}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Random sample of hypothesis: {primary_tokenizer.decode(hypotheses[random.randint(0, len(hypotheses))])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98b90e-5bdc-4933-98df-456139a6b5b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Use AutoModelForSequenceClassification for energy model instead\n",
    "What if we not use custom roberta EM and thus remove the need to add mask token? (That is, what if we just use regular roberta classifier for our em and turn it into a mlm?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f85d7a-3d36-42da-a468-3db08728cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['model_paths'] = ['gpt2-large', \"/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint\"]\n",
    "config['tokenizer_paths'] = config['model_paths']\n",
    "config['model_types'] = ['AutoModelForCausalLM', 'AutoModelForSequenceClassification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fd1ce95e-c5ce-4c29-8d75-1d97effd6d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/vocab.json HTTP/1.1\" 200 0\n",
      "Some weights of the model checkpoint at /shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint were not used when initializing RobertaForMaskedLM: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at /shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint and are newly initialized: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/vocab.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primary_tokenizer = AutoTokenizer.from_pretrained(config['model_paths'][0])\n",
    "\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(config['model_paths'][1])\n",
    "mlm_config = AutoConfig.from_pretrained(config['model_paths'][1])\n",
    "# mlm_config.update({'num_hidden_layers':11})\n",
    "\n",
    "mlm = AutoModelForMaskedLM.from_pretrained(config['model_paths'][1], config=mlm_config)\n",
    "\n",
    "model_checkpoint = \"roberta-base\" ## replace the newly initialized lm_head with roberta-base's head\n",
    "original_mlm_model = AutoModelForMaskedLM.from_pretrained('roberta-base', cache_dir='hf_cache')\n",
    "mlm.lm_head = original_mlm_model.lm_head\n",
    "del original_mlm_model\n",
    "\n",
    "original_mlm_tokenizer = AutoTokenizer.from_pretrained('roberta-base', cache_dir='hf_cache')\n",
    "\n",
    "mlm_tokenizer.add_special_tokens({'mask_token': original_mlm_tokenizer.mask_token})\n",
    "primary_tokenizer.add_special_tokens({'mask_token':original_mlm_tokenizer.mask_token})\n",
    "primary_mask_token_id = primary_tokenizer.mask_token_id\n",
    "mlm.to(config['device'])\n",
    "mlm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "41ba8b75-c842-42f3-9a18-1ecc3badc48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = mlm_tokenizer(test_sentence, return_tensors=\"pt\")\n",
    "lm_outputs = mlm(**inputs.to(config['device']))\n",
    "\n",
    "## get locations (indexes) in test_sentence that is filled with mask token.\n",
    "indices_in_mlm_tokens = (inputs.input_ids == mlm_tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "## get top k tokens for each index\n",
    "logits = lm_outputs.logits\n",
    "predicted_token_ids = torch.topk(logits[0, indices_in_mlm_tokens], k=config['k_per_location'], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bc22f37c-6f6c-4e6a-8130-9ab09015d814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wearing games and<mask><mask><mask><mask> do I hate horse wearing games.\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 0: 47\t-----\t you\n",
      "Candidate 1 @ 0: 4108\t-----\t beating\n",
      "Candidate 2 @ 0: 14\t-----\t that\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 1: 4\t-----\t.\n",
      "Candidate 1 @ 1: 154\t-----\ting\n",
      "Candidate 2 @ 1: 47\t-----\t you\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 2: 4\t-----\t.\n",
      "Candidate 1 @ 2: 20\t-----\t The\n",
      "Candidate 2 @ 2: 6\t-----\t,\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 3: 6\t-----\t,\n",
      "Candidate 1 @ 3: 370\t-----\t You\n",
      "Candidate 2 @ 3: 38\t-----\t I\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Check for candidates\n",
    "## Candidates at each locations are the same.. -> Problematic!\n",
    "print(test_sentence)\n",
    "print(\"-\"*50)\n",
    "for i in range(4):\n",
    "    for j in range(config['k_per_location']):\n",
    "        print(f\"Candidate {j} @ {i}: {predicted_token_ids.indices[i, j]}\\t-----\\t{mlm_tokenizer.decode(predicted_token_ids.indices[i, j])}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "03c3bdc8-14c7-45e7-bbe1-d4896fef9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = []\n",
    "num_located_tokens = len(indices_in_mlm_tokens)\n",
    "num_all_cases = config['k_per_location'] ** num_located_tokens\n",
    "tok_cand_combo = [0 for i in range(num_located_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "bd250554-e8bb-4c4d-b42a-21e074717da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for case_id in range(num_all_cases):\n",
    "    # print(case_id)\n",
    "    for i in range(num_located_tokens):\n",
    "        tok_cand_combo[i] = (case_id // (config['k_per_location']**i)) % config['k_per_location']\n",
    "    \n",
    "    tmp_seq = inputs['input_ids'].clone()\n",
    "    for pos_id, tok_cand_id in enumerate(tok_cand_combo):\n",
    "        tmp_seq[0, indices_in_mlm_tokens[pos_id]] = predicted_token_ids.indices[pos_id, tok_cand_id]\n",
    "\n",
    "    # need to do decode with RobertaTokenizer and encode with GPT2Tokenizer\n",
    "    tmp_dec_seq = primary_tokenizer(mlm_tokenizer.batch_decode(tmp_seq, skip_special_tokens=True), return_tensors=\"pt\").input_ids.cuda()\n",
    "    hypotheses.append(tmp_dec_seq.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2bc856cb-6fdf-48f8-be51-58e45bf1d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code block to score hypotheses using GPT2 and EM.\n",
    "lossfns = []\n",
    "for i, loss in enumerate(config['losses']):\n",
    "    lossfns.append(lossbuilder.build_loss(loss, name2model[config['model_paths'][i]], name2tokenizer[config['model_paths'][i]], build_loss_args))\n",
    "    loss2modelname[loss] = config['model_paths'][i]\n",
    "    loss2tokenizer[loss] = name2tokenizer[config['model_paths'][i]]\n",
    "\n",
    "candidate_total_losses, candidate_primary_losses, candidate_losses_for_loggings = score_hypotheses(source_batch,\n",
    "                                                                                                hypotheses, \n",
    "                                                                                                config, \n",
    "                                                                                                lossfns,\n",
    "                                                                                        additional_batch=additional_batch, \n",
    "                                                                                        context_batch=context_batch,\n",
    "                                                                                        use_context=use_context,\n",
    "                                                                                        label_ids=label_ids,\n",
    "                                                                                        keywords=keywords,\n",
    "                                                                                        kweight=new_kweight)\n",
    "best_ix = np.argmin(np.array(candidate_total_losses))\n",
    "best_prediction = hypotheses[best_ix]\n",
    "best_text = primary_tokenizer.decode(best_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "85a187cf-67ff-4bde-b942-a3f561d71a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: wearing games and<mask><mask><mask><mask> do I hate horse wearing games.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Best hypothesis: wearing games and beating you. I do I hate horse wearing games.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Random sample of hypothesis: wearing games and thating,, do I hate horse wearing games.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original sentence: {test_sentence}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Best hypothesis: {best_text}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Random sample of hypothesis: {primary_tokenizer.decode(hypotheses[random.randint(0, len(hypotheses))])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea42b6-8ae4-423b-a57a-8349af149482",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ~Use AutoModelForSequenceClassification for energy model instead + Use only 11 layers~\n",
    "Double checking if the performance can improve by removing the final layer and using the first few layers with AutoModel EM turned MLM.\n",
    "\n",
    "- Hypothesis: Since the final layer of SeqClassificationModel is trained for SeqClassification task (using CLS token and what not), the candidate generation can improve if we do not use that final layer and only use the first few layers of transformer blocks when converting EM to MLM.\n",
    "- Result: Got worse. The hypothesis is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "0f9191f8-389f-4122-a966-8bd518db970b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/vocab.json HTTP/1.1\" 200 0\n",
      "Some weights of the model checkpoint at models/roberta-base-jigsaw-toxicity-mlm-with-gpt2-large-embeds/checkpoint_best were not used when initializing RobertaForMaskedLM: ['roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.embeddings.word_embeddings.1.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.embeddings.word_embeddings.0.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at models/roberta-base-jigsaw-toxicity-mlm-with-gpt2-large-embeds/checkpoint_best and are newly initialized: ['roberta.embeddings.word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/vocab.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primary_tokenizer = AutoTokenizer.from_pretrained(config['model_paths'][0])\n",
    "\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(config['model_paths'][1])\n",
    "mlm_config = AutoConfig.from_pretrained(config['model_paths'][1])\n",
    "mlm_config.update({'num_hidden_layers':11})\n",
    "\n",
    "mlm = AutoModelForMaskedLM.from_pretrained(config['model_paths'][1], config=mlm_config)\n",
    "\n",
    "model_checkpoint = \"roberta-base\" ## replace the newly initialized lm_head with roberta-base's head\n",
    "original_mlm_model = AutoModelForMaskedLM.from_pretrained('roberta-base', cache_dir='hf_cache')\n",
    "mlm.lm_head = original_mlm_model.lm_head\n",
    "del original_mlm_model\n",
    "\n",
    "original_mlm_tokenizer = AutoTokenizer.from_pretrained('roberta-base', cache_dir='hf_cache')\n",
    "\n",
    "mlm_tokenizer.add_special_tokens({'mask_token': original_mlm_tokenizer.mask_token})\n",
    "primary_tokenizer.add_special_tokens({'mask_token':original_mlm_tokenizer.mask_token})\n",
    "primary_mask_token_id = primary_tokenizer.mask_token_id\n",
    "mlm.to(config['device'])\n",
    "mlm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "10c02ccc-19d1-4e12-b4be-24fc8981db7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = mlm_tokenizer(test_sentence, return_tensors=\"pt\")\n",
    "lm_outputs = mlm(**inputs.to(config['device']))\n",
    "\n",
    "## get locations (indexes) in test_sentence that is filled with mask token.\n",
    "indices_in_mlm_tokens = (inputs.input_ids == mlm_tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "## get top k tokens for each index\n",
    "logits = lm_outputs.logits\n",
    "predicted_token_ids = torch.topk(logits[0, indices_in_mlm_tokens], k=config['k_per_location'], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a20f357d-0b95-4a60-a805-ea81bcaf2b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wearing games and<mask><mask><mask><mask> do I hate horse wearing games.\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 0: 4555\t-----\t eth\n",
      "Candidate 1 @ 0: 2183\t-----\t custom\n",
      "Candidate 2 @ 0: 38690\t-----\tstarting\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 1: 12018\t-----\t ox\n",
      "Candidate 1 @ 1: 4555\t-----\t eth\n",
      "Candidate 2 @ 1: 38690\t-----\tstarting\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 2: 1088\t-----\t around\n",
      "Candidate 1 @ 2: 12018\t-----\t ox\n",
      "Candidate 2 @ 2: 2183\t-----\t custom\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 3: 1088\t-----\t around\n",
      "Candidate 1 @ 3: 12018\t-----\t ox\n",
      "Candidate 2 @ 3: 73\t-----\tj\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Check for candidates\n",
    "## Candidates at each locations are the same.. -> Problematic!\n",
    "print(test_sentence)\n",
    "print(\"-\"*50)\n",
    "for i in range(4):\n",
    "    for j in range(config['k_per_location']):\n",
    "        print(f\"Candidate {j} @ {i}: {predicted_token_ids.indices[i, j]}\\t-----\\t{mlm_tokenizer.decode(predicted_token_ids.indices[i, j])}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d08e6b69-aa15-4f78-a917-c32e4d3b3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = []\n",
    "num_located_tokens = len(indices_in_mlm_tokens)\n",
    "num_all_cases = config['k_per_location'] ** num_located_tokens\n",
    "tok_cand_combo = [0 for i in range(num_located_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f5f48f6e-e508-45b1-924c-c4c679eaab5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for case_id in range(num_all_cases):\n",
    "    # print(case_id)\n",
    "    for i in range(num_located_tokens):\n",
    "        tok_cand_combo[i] = (case_id // (config['k_per_location']**i)) % config['k_per_location']\n",
    "    \n",
    "    tmp_seq = inputs['input_ids'].clone()\n",
    "    for pos_id, tok_cand_id in enumerate(tok_cand_combo):\n",
    "        tmp_seq[0, indices_in_mlm_tokens[pos_id]] = predicted_token_ids.indices[pos_id, tok_cand_id]\n",
    "\n",
    "    # need to do decode with RobertaTokenizer and encode with GPT2Tokenizer\n",
    "    tmp_dec_seq = primary_tokenizer(mlm_tokenizer.batch_decode(tmp_seq, skip_special_tokens=True), return_tensors=\"pt\").input_ids.cuda()\n",
    "    hypotheses.append(tmp_dec_seq.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ceaa8f1f-dc6f-4067-8bee-875179ca5110",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code block to score hypotheses using GPT2 and EM.\n",
    "lossfns = []\n",
    "for i, loss in enumerate(config['losses']):\n",
    "    lossfns.append(lossbuilder.build_loss(loss, name2model[config['model_paths'][i]], name2tokenizer[config['model_paths'][i]], build_loss_args))\n",
    "    loss2modelname[loss] = config['model_paths'][i]\n",
    "    loss2tokenizer[loss] = name2tokenizer[config['model_paths'][i]]\n",
    "\n",
    "candidate_total_losses, candidate_primary_losses, candidate_losses_for_loggings = score_hypotheses(source_batch,\n",
    "                                                                                                hypotheses, \n",
    "                                                                                                config, \n",
    "                                                                                                lossfns,\n",
    "                                                                                        additional_batch=additional_batch, \n",
    "                                                                                        context_batch=context_batch,\n",
    "                                                                                        use_context=use_context,\n",
    "                                                                                        label_ids=label_ids,\n",
    "                                                                                        keywords=keywords,\n",
    "                                                                                        kweight=new_kweight)\n",
    "best_ix = np.argmin(np.array(candidate_total_losses))\n",
    "best_prediction = hypotheses[best_ix]\n",
    "best_text = primary_tokenizer.decode(best_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "fdabef01-96de-4dec-b869-f4d25c9b35d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: wearing games and<mask><mask><mask><mask> do I hate horse wearing games.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Best hypothesis: wearing games and custom ox custom ox do I hate horse wearing games.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Random sample of hypothesis: wearing games and custom eth customj do I hate horse wearing games.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original sentence: {test_sentence}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Best hypothesis: {best_text}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Random sample of hypothesis: {primary_tokenizer.decode(hypotheses[random.randint(0, len(hypotheses))])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6501bcd7-7446-4fab-8d2c-c992993c0ee6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Baselne: vanialla MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "734b1f30-71df-49ec-b5aa-30f18936f299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/vocab.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "primary_tokenizer = AutoTokenizer.from_pretrained(config['model_paths'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "fcd45d2c-c96f-4c32-bf24-12c64ce87c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_tokenizer = AutoTokenizer.from_pretrained(config['model_paths'][1])\n",
    "mlm_config = AutoConfig.from_pretrained(config['model_paths'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3707eaed-a811-4b6e-a009-a9ba3d3b8f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"roberta-base\" ## replace the newly initialized lm_head with roberta-base's head\n",
    "mlm = AutoModelForMaskedLM.from_pretrained('roberta-base', cache_dir='hf_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "3d2e3ea2-837b-4637-96d0-07703f620e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primary_tokenizer.add_special_tokens({'mask_token':mlm_tokenizer.mask_token})\n",
    "primary_mask_token_id = primary_tokenizer.mask_token_id\n",
    "mlm.to(config['device'])\n",
    "mlm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "581436d3-b996-4f3d-8172-5e6092cccb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = mlm_tokenizer(test_sentence, return_tensors=\"pt\")\n",
    "lm_outputs = mlm(**inputs.to(config['device']))\n",
    "\n",
    "## get locations (indexes) in test_sentence that is filled with mask token.\n",
    "indices_in_mlm_tokens = (inputs.input_ids == mlm_tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "## get top k tokens for each index\n",
    "logits = lm_outputs.logits\n",
    "predicted_token_ids = torch.topk(logits[0, indices_in_mlm_tokens], k=config['k_per_location'], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a4453d9e-4ade-4321-801b-e6aa810e9f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wearing games and<mask><mask><mask><mask> do I hate horse wearing games.\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 0: 5253\t-----\t horse\n",
      "Candidate 1 @ 0: 8087\t-----\t horses\n",
      "Candidate 2 @ 0: 5793\t-----\t riding\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 1: 8087\t-----\t horses\n",
      "Candidate 1 @ 1: 5253\t-----\t horse\n",
      "Candidate 2 @ 1: 4\t-----\t.\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 2: 2\t-----\t</s>\n",
      "Candidate 1 @ 2: 4\t-----\t.\n",
      "Candidate 2 @ 2: 8087\t-----\t horses\n",
      "--------------------------------------------------\n",
      "Candidate 0 @ 3: 2612\t-----\t Why\n",
      "Candidate 1 @ 3: 596\t-----\t why\n",
      "Candidate 2 @ 3: 7608\t-----\tWhy\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Check for candidates\n",
    "## Candidates at each locations are the same.. -> Problematic!\n",
    "print(test_sentence)\n",
    "print(\"-\"*50)\n",
    "for i in range(4):\n",
    "    for j in range(config['k_per_location']):\n",
    "        print(f\"Candidate {j} @ {i}: {predicted_token_ids.indices[i, j]}\\t-----\\t{mlm_tokenizer.decode(predicted_token_ids.indices[i, j])}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "74f4db52-1f98-467a-8ded-ae950973f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = []\n",
    "num_located_tokens = len(indices_in_mlm_tokens)\n",
    "num_all_cases = config['k_per_location'] ** num_located_tokens\n",
    "tok_cand_combo = [0 for i in range(num_located_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e46d09ba-4a57-43e1-b425-6504fa8261b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for case_id in range(num_all_cases):\n",
    "    # print(case_id)\n",
    "    for i in range(num_located_tokens):\n",
    "        tok_cand_combo[i] = (case_id // (config['k_per_location']**i)) % config['k_per_location']\n",
    "    \n",
    "    tmp_seq = inputs['input_ids'].clone()\n",
    "    for pos_id, tok_cand_id in enumerate(tok_cand_combo):\n",
    "        tmp_seq[0, indices_in_mlm_tokens[pos_id]] = predicted_token_ids.indices[pos_id, tok_cand_id]\n",
    "\n",
    "    # need to do decode with RobertaTokenizer and encode with GPT2Tokenizer\n",
    "    tmp_dec_seq = primary_tokenizer(mlm_tokenizer.batch_decode(tmp_seq, skip_special_tokens=True), return_tensors=\"pt\").input_ids.cuda()\n",
    "    hypotheses.append(tmp_dec_seq.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b30647e6-dff5-475d-a13c-958ae2ebac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code block to score hypotheses using GPT2 and EM.\n",
    "lossfns = []\n",
    "for i, loss in enumerate(config['losses']):\n",
    "    lossfns.append(lossbuilder.build_loss(loss, name2model[config['model_paths'][i]], name2tokenizer[config['model_paths'][i]], build_loss_args))\n",
    "    loss2modelname[loss] = config['model_paths'][i]\n",
    "    loss2tokenizer[loss] = name2tokenizer[config['model_paths'][i]]\n",
    "\n",
    "candidate_total_losses, candidate_primary_losses, candidate_losses_for_loggings = score_hypotheses(source_batch,\n",
    "                                                                                                hypotheses, \n",
    "                                                                                                config, \n",
    "                                                                                                lossfns,\n",
    "                                                                                        additional_batch=additional_batch, \n",
    "                                                                                        context_batch=context_batch,\n",
    "                                                                                        use_context=use_context,\n",
    "                                                                                        label_ids=label_ids,\n",
    "                                                                                        keywords=keywords,\n",
    "                                                                                        kweight=new_kweight)\n",
    "best_ix = np.argmin(np.array(candidate_total_losses))\n",
    "best_prediction = hypotheses[best_ix]\n",
    "best_text = primary_tokenizer.decode(best_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "df8ba9c2-5085-442c-a1bb-9cc44adff1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: wearing games and<mask><mask><mask><mask> do I hate horse wearing games.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Best hypothesis: wearing games and riding horses. Why do I hate horse wearing games.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Random sample of hypothesis: wearing games and horse horse why do I hate horse wearing games.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original sentence: {test_sentence}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Best hypothesis: {best_text}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Random sample of hypothesis: {primary_tokenizer.decode(hypotheses[random.randint(0, len(hypotheses))])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

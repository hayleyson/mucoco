{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_beam_hypotheses' from 'new_module.new_decode_utils' (/data/hyeryung/mucoco/new_module/new_decode_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# from new_module.decode_utils import (\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#     beam_rerank_v0,\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#     beam_rerank_v1,\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#     beam_rerank_v2,\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     combi_rerank,\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnew_module\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnew_decode_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_beam_hypotheses, get_combi_hypotheses, final_reranking\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnew_module\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate_wandb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_main\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnew_module\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnew_locate_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LocateMachine\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_beam_hypotheses' from 'new_module.new_decode_utils' (/data/hyeryung/mucoco/new_module/new_decode_utils.py)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from itertools import chain\n",
    "import math\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "os.chdir('/data/hyeryung/mucoco')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "import new_module.losses as lossbuilder\n",
    "import wandb\n",
    "# from new_module.decode_utils import (\n",
    "#     beam_rerank_v0,\n",
    "#     beam_rerank_v1,\n",
    "#     beam_rerank_v2,\n",
    "#     combi_rerank,\n",
    "# )\n",
    "from new_module.new_decode_utils import get_beam_hypotheses, get_combi_hypotheses, final_reranking\n",
    "from new_module.evaluation.evaluate_wandb import evaluate_main\n",
    "from new_module.locate.new_locate_utils import LocateMachine\n",
    "from new_module.utils.robertacustom import RobertaCustomForSequenceClassification\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(os.environ.get(\"LOGGING_LEVEL\", logging.DEBUG))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import new_module.new_decode_utils\n",
    "importlib.reload(new_module.new_decode_utils)\n",
    "from new_module.new_decode_utils import get_beam_hypotheses, get_combi_hypotheses, final_reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "config = joblib.load('config.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'toxicity',\n",
       " 'source_data': 'new_module/data/toxicity-avoidance/dev_set.jsonl',\n",
       " 'source_style': 'toxic',\n",
       " 'target_style': 'nontoxic',\n",
       " 'target_label_ids': [0, 0],\n",
       " 'model_paths': ['gpt2-large',\n",
       "  '/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint'],\n",
       " 'tokenizer_paths': ['gpt2-large',\n",
       "  '/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint'],\n",
       " 'model_types': ['AutoModelForCausalLM',\n",
       "  'RobertaCustomForSequenceClassification'],\n",
       " 'output_dir_prefix': 'outputs/toxicity/devset',\n",
       " 'early_stopping_patience': 0,\n",
       " 'method': 'mlm-beamsearch-v0',\n",
       " 'locate_unit': 'word',\n",
       " 'min_epsilons': [0.9],\n",
       " 'num_samples': 10,\n",
       " 'device': 'cuda',\n",
       " 'target_type': 'embeds',\n",
       " 'cache_dir': '/data/hyeryung/hf_cache',\n",
       " 'jsonl_primary_key': 'prompt',\n",
       " 'jsonl_secondary_key': 'text',\n",
       " 'losses': ['gpt2', 'classification_no_prefix_logprobloss'],\n",
       " 'build_loss_dict': {'coeff_steps': 200,\n",
       "  'coeff_pattern': 'constant',\n",
       "  'loss_type': 'xentropy',\n",
       "  'length_normalize': False,\n",
       "  'AR_temperature': 1.0,\n",
       "  'AR_top_k': 0,\n",
       "  'AR_top_p': 0.96,\n",
       "  'max_output_length': 20},\n",
       " 'num_edit_token_per_step': 5,\n",
       " 'k_per_location': 10,\n",
       " 'n_iter': 10,\n",
       " 'selection_criteria': 'allsat_primary',\n",
       " 'closs_weight': 0.9,\n",
       " 'beam_size': 5,\n",
       " 'wandb_project': 'toxicity-decoding',\n",
       " 'wandb_entity': 'hayleyson',\n",
       " 'wandb_run_id': None,\n",
       " 'resume': False,\n",
       " 'slurm_job_id': '8657',\n",
       " 'dont_skip_allsat': False,\n",
       " 'locate_method': 'grad_norm',\n",
       " 'server_time_limit': 12.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 1812\n",
      "https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 377\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhayleyson\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Popen(['git', 'cat-file', '--batch-check'], cwd=/data/hyeryung/mucoco, stdin=<valid stream>, shell=False, universal_newlines=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/hyeryung/mucoco/wandb/run-20240406_182129-45d5r7bb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hayleyson/toxicity-decoding/runs/45d5r7bb' target=\"_blank\">vulcan-t-pol-152</a></strong> to <a href='https://wandb.ai/hayleyson/toxicity-decoding' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hayleyson/toxicity-decoding' target=\"_blank\">https://wandb.ai/hayleyson/toxicity-decoding</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hayleyson/toxicity-decoding/runs/45d5r7bb' target=\"_blank\">https://wandb.ai/hayleyson/toxicity-decoding/runs/45d5r7bb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main_start_time = time.time()\n",
    "\n",
    "if not config.get(\"model_tag\", None):\n",
    "    if \"energy-training\" in config[\"model_paths\"][1]:\n",
    "        config[\"model_tag\"] = \"em\"\n",
    "    else:\n",
    "        config[\"model_tag\"] = \"clsf\"\n",
    "\n",
    "    if (config[\"task\"] == \"formality\") and (\"gyafc\" in config[\"model_paths\"][1]):\n",
    "        config[\"model_tag\"] += \"-gyafc\"\n",
    "\n",
    "if config[\"resume\"]:\n",
    "    logger.info(\"resuming from a previous run\")\n",
    "    run = wandb.init(\n",
    "        project=config[\"wandb_project\"],\n",
    "        entity=config[\"wandb_entity\"],\n",
    "        id=config[\"wandb_run_id\"],\n",
    "        resume=\"must\",\n",
    "    )\n",
    "else:\n",
    "    run = wandb.init(\n",
    "        project=config[\"wandb_project\"],\n",
    "        entity=config[\"wandb_entity\"],\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "run_id = run.path.split(\"/\")[-1]\n",
    "display_name = f\"{run_id}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "outdir = os.path.join(config[\"output_dir_prefix\"], display_name)\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "outfile = f\"{outdir}/outputs_epsilon{config['min_epsilons'][0]}.txt\"\n",
    "run.summary[\"outfile_path\"] = outfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class dummyArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "build_loss_args = dummyArgs(**config[\"build_loss_dict\"])\n",
    "\n",
    "## load data\n",
    "if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "    source_dataset = [\n",
    "        json.loads(l)[config[\"jsonl_primary_key\"]][config[\"jsonl_secondary_key\"]]\n",
    "        for l in open(config[\"source_data\"])\n",
    "    ]\n",
    "    generation_dataset = [\n",
    "        json.loads(l)[\"generations\"] for l in open(config[\"source_data\"])\n",
    "    ]\n",
    "elif (config[\"task\"] == \"formality\") or (config[\"task\"] == \"sentiment-lewis-compr\"):\n",
    "    with open(config[\"source_data\"], \"r\") as f:\n",
    "        generation_dataset = [line.rstrip('\\n') for line in f.readlines()]\n",
    "    source_dataset = [\"\" for l in generation_dataset]\n",
    "\n",
    "# check if outfile exists\n",
    "if (config[\"resume\"]) and (os.path.exists(outfile)):\n",
    "\n",
    "    with open(outfile, \"r\") as f:\n",
    "        existing_gens = [x.rstrip(\"\\n\") for x in f.readlines()]\n",
    "    resume_idx = len(existing_gens)\n",
    "    if resume_idx == len(source_dataset):\n",
    "        logger.debug(\"output file is already complete. skipping this run.\")\n",
    "        raise\n",
    "    elif resume_idx < len(source_dataset):\n",
    "        logger.info(\n",
    "            f\"output file already exists but is incomplete. resuming from index: {resume_idx}\"\n",
    "        )\n",
    "        outf = open(outfile, \"a\")\n",
    "        int_outf = open(outfile+\".intermediate\", \"a\")\n",
    "    else:\n",
    "        logger.critical(\n",
    "            f\"output file seems to be corrupted. The file length is {resume_idx}, where the size of source_dataset is {len(source_dataset)}\"\n",
    "        )\n",
    "        raise\n",
    "else:\n",
    "    resume_idx = 0\n",
    "    outf = open(outfile, \"w\")\n",
    "    int_outf = open(outfile+\".intermediate\", \"w\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/generation_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## load tokenizer, models, define losses\n",
    "name2tokenizer = {}\n",
    "name2model = {}\n",
    "name2config = {}\n",
    "loss2tokenizer = {}\n",
    "embed_luts = []\n",
    "\n",
    "for i, model_path in enumerate(config[\"model_paths\"]):\n",
    "    if (\n",
    "        model_path not in name2model\n",
    "    ):  # making sure we are not loading the model twice in case some constraints use the same model.\n",
    "        try:\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=True,\n",
    "            )\n",
    "        except:\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=False,\n",
    "            )\n",
    "\n",
    "        name2config[model_path] = AutoConfig.from_pretrained(\n",
    "            model_path, cache_dir=config[\"cache_dir\"]\n",
    "        )\n",
    "\n",
    "        if config[\"model_types\"][i] == \"RobertaCustomForSequenceClassification\":\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                RobertaCustomForSequenceClassification.from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(transformers, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        name2model[model_path].eval()\n",
    "        name2model[model_path].to(config['device'])\n",
    "\n",
    "    input_embeds = name2model[model_path].get_input_embeddings()\n",
    "    if isinstance(input_embeds, torch.nn.Sequential):\n",
    "        input_embeds = input_embeds[0]\n",
    "    embed_luts.append(input_embeds)\n",
    "\n",
    "    if config[\"target_type\"] == \"embeds\":\n",
    "        embed_luts[-1].requires_grad = False\n",
    "\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "mlm = None if config[\"method\"] == \"mlm-beamsearch-v2\" else AutoModelForMaskedLM.from_pretrained(\"roberta-base\").to(config['device'])\n",
    "\n",
    "lossfns = []\n",
    "for i, loss in enumerate(config[\"losses\"]):\n",
    "    lossfns.append(\n",
    "        lossbuilder.build_loss(\n",
    "            loss,\n",
    "            name2model[config[\"model_paths\"][i]],\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]],\n",
    "            build_loss_args,\n",
    "        )\n",
    "    )\n",
    "    lossfns[i].tokenizer.add_special_tokens({\"mask_token\": mlm_tokenizer.mask_token})\n",
    "    loss2tokenizer[loss] = lossfns[i].tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define an object to locate problematic phrases\n",
    "locator = LocateMachine(lossfns[1].model, lossfns[1].tokenizer)\n",
    "\n",
    "label_ids = config[\"target_label_ids\"]  # target label's ids for each loss\n",
    "\n",
    "run.summary[\"prep_time\"] = time.time() - main_start_time\n",
    "## beginning of main logic\n",
    "decode_start_time = time.time()\n",
    "# text_id = 0\n",
    "if config[\"resume\"]:\n",
    "    num_skipped = run.summary.get(\"num_skipped\", 0)\n",
    "    num_edited = run.summary.get(\"num_edited\", 0)\n",
    "    num_decoded_tokens = run.summary.get(\"num_decoded_tokens\", 0)\n",
    "else:\n",
    "    num_skipped = 0\n",
    "    num_edited = 0\n",
    "    num_decoded_tokens = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_weights = [1 - config['closs_weight, config['closs_weight]\n",
    "interrupted = False\n",
    "# for text_id in range(len(source_dataset))[resume_idx:]:\n",
    "text_id = 0\n",
    "source_text = source_dataset[text_id]\n",
    "if source_text == \"\":\n",
    "    source_text = lossfns[0].tokenizer.bos_token\n",
    "\n",
    "if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "    AR_prediction_all = [x[\"text\"] for x in generation_dataset[text_id]]\n",
    "    # predicted_batches = [x[\"tokens\"] for x in generation_dataset[text_id]]\n",
    "    # predicted_batches = [\n",
    "    #     torch.tensor([x], dtype=torch.long, device=config[\"device\"])\n",
    "    #     for x in predicted_batches\n",
    "    # ]\n",
    "    \n",
    "elif (config[\"task\"] == \"formality\") or (\n",
    "    config[\"task\"] == \"sentiment-lewis-compr\"\n",
    "):\n",
    "    AR_prediction_all = [generation_dataset[text_id]]\n",
    "\n",
    "curr_num_samples = len(AR_prediction_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' dirt. Unfortunately the majority of horses end up in shit that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " ' wearing games and holy ****ing shit do I hate horse wearing games.',\n",
       " \" fetishes: it just makes me want to puke every time I see it on the internet, even though it's not worth a thing because I am a furry.\"]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AR_prediction_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample_idx in range(config[\"num_samples\"])[:]:\n",
    "\n",
    "######### change here! instead of for loop, do a batched operation ########\n",
    "\n",
    "# --------------------------------------------------------------------------------------------- #\n",
    "## check whether initial text satisfies constraint\n",
    "\n",
    "curr_loss = torch.zeros(len(AR_prediction_all)).to(config['device'])\n",
    "logging_loss = torch.zeros((len(AR_prediction_all),2)).to(config['device'])\n",
    "edit_yn = torch.ones(len(AR_prediction_all), dtype=torch.bool).to(config['device'])\n",
    "        \n",
    "for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "    with torch.no_grad():\n",
    "        lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "            source_text, AR_prediction_all,\n",
    "            label_id=config['target_label_ids'][lossid],\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "    curr_loss += loss_weights[lossid] * lossvalue\n",
    "    logging_loss[:, lossid] = lossvalue.clone()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "allsat = logging_loss[:,1] < -math.log(config[\"min_epsilons\"][0])\n",
    "allsat_ix = allsat.nonzero().squeeze(0)\n",
    "edit_yn[allsat_ix] = 0\n",
    "edited_at_all_yn = edit_yn.detach().clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([False, False, False], device='cuda:0'),\n",
       " tensor([], device='cuda:0', size=(0, 1), dtype=torch.int64),\n",
       " tensor([True, True, True], device='cuda:0'),\n",
       " tensor([True, True, True], device='cuda:0'))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allsat, allsat_ix, edit_yn, edited_at_all_yn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (edit_yn.sum().item() == 0) and (not config[\"dont_skip_allsat\"]):\n",
    "    ## save data\n",
    "    num_edited += 0\n",
    "    num_skipped += len(AR_prediction_all)\n",
    "    num_decoded_tokens += 0\n",
    "    print('continue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "num_edited += edit_yn.sum().item()\n",
    "num_skipped += (len(AR_prediction_all) - edit_yn.sum().item())\n",
    "num_decoded_tokens += sum([len(x) for x in name2tokenizer[config[\"tokenizer_paths\"][0]]([x for i, x in enumerate(AR_prediction_all) if edit_yn[i] == 1], add_special_tokens=False).input_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_patience_count = torch.zeros(len(AR_prediction_all),dtype=torch.long).to(config['device'])\n",
    "best_allsat = allsat.detach().clone()\n",
    "best_losses = logging_loss.detach().clone()\n",
    "best_weighted_loss = curr_loss.detach().clone()            \n",
    "running_text = best_text = deepcopy(AR_prediction_all)\n",
    "int_output = [{} for _ in range(len(AR_prediction_all))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "_iter = 0\n",
    "# for _iter in range(config['n_iter):\n",
    "masked_text = locator.locate_main(running_text, \n",
    "                                    method = config['locate_method'], \n",
    "                                    max_num_tokens = config['num_edit_token_per_step, \n",
    "                                    unit = config['locate_unit'], \n",
    "                                    num_layer = -2, #penultimate\n",
    "                                    label_id = config['target_label_ids'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' dirt. Unfortunately the majority of horses end up in shit that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " ' wearing games and holy ****ing shit do I hate horse wearing games.',\n",
       " \" fetishes: it just makes me want to puke every time I see it on the internet, even though it's not worth a thing because I am a furry.\"]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AR_prediction_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = mlm_tokenizer(\n",
    "    masked_text, return_tensors=\"pt\", padding=True, truncation=True\n",
    ")\n",
    "inputs = inputs.to(config['device']) \n",
    "masked_sequence=inputs['input_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## make predictions for the masked indices\n",
    "with torch.no_grad():\n",
    "    logits = mlm(**inputs).logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "special_token_ids = mlm_tokenizer.convert_tokens_to_ids(mlm_tokenizer.all_special_tokens)\n",
    "logits[:, :, special_token_ids] = -float(\"inf\")\n",
    "\n",
    "\n",
    "## 여기에서 repeat이 안되도록 처리할 수 있나? # 아직은 잘 모르겠음\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices_in_mlm_tokens = (\n",
    "    inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    ").nonzero(as_tuple=True)\n",
    "\n",
    "## get top k tokens for each index\n",
    "predicted_token_ids = torch.topk(\n",
    "    logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "    k=config['k_per_location'],\n",
    "    dim=-1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if config[\"method\"] in [\"mlm-beamsearch-v0\",\"mlm-beamsearch-v1\"] :\n",
    "    hypotheses = get_beam_hypotheses(source_text, \n",
    "            masked_sequence, \n",
    "            indices_in_mlm_tokens,\n",
    "            predicted_token_ids.indices,\n",
    "            mlm_tokenizer, \n",
    "            lossfns,\n",
    "            config)\n",
    "elif config[\"method\"] == \"mlm-reranking\":\n",
    "    hypotheses = get_combi_hypotheses(masked_sequence, \n",
    "                indices_in_mlm_tokens,\n",
    "                predicted_token_ids.indices,\n",
    "                mlm_tokenizer,\n",
    "                config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' dirt. Unfortunately the majority of you end up in something that you had to get to. My only recourse is to do it myself. What would be the happy tale of my life then?',\n",
       "  ' dirt. Unfortunately the majority of you end up in something that you had to work in, My only recourse is to make it myself. What would be the happy tale of my life then?',\n",
       "  ' dirt. Unfortunately the majority of you end up in something that you had to go for.\" My only recourse is to face it myself. What would be the happy tale of my life then?',\n",
       "  ' dirt. Unfortunately the majority of you end up in something that you had to help with? My only recourse is to live it myself. What would be the happy tale of my life then?',\n",
       "  ' dirt. Unfortunately the majority of you end up in something that you had to deal from! My only recourse is to buy it myself. What would be the happy tale of my life then?'],\n",
       " [' wearing games and by ****ing god do I hate horse wearing games.',\n",
       "  ' wearing games and no ****ing why do I hate horse wearing games.',\n",
       "  ' wearing games and how ****ing, do I hate horse wearing games.',\n",
       "  ' wearing games and why ****ing how do I hate horse wearing games.',\n",
       "  ' wearing games and God ****ing much do I hate horse wearing games.'],\n",
       " [\" fetishes: it just makes me want to be better every time I see it on the news, even though it's not worth a thing because I am the..\",\n",
       "  \" fetishes: it just makes me want to do something every time I see it on the TV — even though it's not worth a thing because I am a to it\",\n",
       "  \" fetishes: it just makes me want to get, every time I see it on the street - even though it's not worth a thing because I am in it:\",\n",
       "  \" fetishes: it just makes me want to love something every time I see it on the television – even though it's not worth a thing because I am an one)\",\n",
       "  \" fetishes: it just makes me want to buy them every time I see it on the screen -- even though it's not worth a thing because I am not there).\"]]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "final_hypotheses, new_best_weighted_loss, new_best_allsat, new_best_logging_loss = final_reranking(source_text,\n",
    "                                                                                                   hypotheses,\n",
    "                                                                                                    lossfns,\n",
    "                                                                                                    config,\n",
    "                                                                                                     batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' dirt. Unfortunately the majority of you end up in something that you had to get to. My only recourse is to do it myself. What would be the happy tale of my life then?',\n",
       " ' wearing games and no ****ing why do I hate horse wearing games.',\n",
       " \" fetishes: it just makes me want to be better every time I see it on the news, even though it's not worth a thing because I am the..\"]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.857,  5.988, 11.103], device='cuda:0')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_best_weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False,  True], device='cuda:0')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_best_allsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  138.467,     0.011],\n",
       "        [   54.674,     0.579],\n",
       "        [  110.782,     0.027]], device='cuda:0')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_best_logging_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False,  True], device='cuda:0')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_best_allsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "update = torch.Tensor([]).long().to(config['device'])\n",
    "if config['selection_criteria == \"weighted_sum\":\n",
    "    update = best_weighted_loss > new_best_weighted_loss\n",
    "elif config['selection_criteria == \"allsat_primary\":\n",
    "    update = (~best_allsat & new_best_allsat) | \\\n",
    "            (~best_allsat & ~new_best_allsat & (best_weighted_loss > new_best_weighted_loss)) | \\\n",
    "            (best_allsat & new_best_allsat & (best_losses[:, 0] > new_best_logging_loss[:, 0])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## intermediate output for debugging\n",
    "for sample_ix in range(len(AR_prediction_all)):\n",
    "    \n",
    "    int_output[sample_ix].update({f\"iter{_iter}_original_sentence\": running_text[sample_ix],\n",
    "                                f\"iter{_iter}_masked_sentence\": masked_text[sample_ix],\n",
    "                                f\"iter{_iter}_best_text\": final_hypotheses[sample_ix],\n",
    "                                f\"iter{_iter}_update\": update[sample_ix].item()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'iter0_original_sentence': ' dirt. Unfortunately the majority of you end up in something that you had to get to. My only recourse is to do it myself. What would be the happy tale of my life then?',\n",
       "  'iter0_masked_sentence': ' dirt. Unfortunately the majority of<mask> end up in<mask> that you had to<mask><mask><mask> My only recourse is to<mask> it myself. What would be the happy tale of my life then?',\n",
       "  'iter0_best_text': ' dirt. Unfortunately the majority of you end up in something that you had to get to. My only recourse is to do it myself. What would be the happy tale of my life then?',\n",
       "  'iter0_update': True},\n",
       " {'iter0_original_sentence': ' wearing games and no ****ing why do I hate horse wearing games.',\n",
       "  'iter0_masked_sentence': ' wearing games and<mask> ****ing<mask> do I hate horse wearing games.',\n",
       "  'iter0_best_text': ' wearing games and no ****ing why do I hate horse wearing games.',\n",
       "  'iter0_update': True},\n",
       " {'iter0_original_sentence': \" fetishes: it just makes me want to be better every time I see it on the news, even though it's not worth a thing because I am the..\",\n",
       "  'iter0_masked_sentence': \" fetishes: it just makes me want to<mask><mask> every time I see it on the<mask><mask> even though it's not worth a thing because I am<mask><mask><mask>\",\n",
       "  'iter0_best_text': \" fetishes: it just makes me want to be better every time I see it on the news, even though it's not worth a thing because I am the..\",\n",
       "  'iter0_update': True}]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "running_text = deepcopy(final_hypotheses)\n",
    "for update_index in update.nonzero().squeeze().tolist():\n",
    "    best_text[update_index] = final_hypotheses[update_index]\n",
    "best_allsat[update] = new_best_allsat[update]\n",
    "best_losses[update] = new_best_logging_loss[update]\n",
    "best_weighted_loss[update] = new_best_weighted_loss[update]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_patience_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2], device='cuda:0')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(best_allsat & edit_yn).nonzero().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_patience_count[(best_allsat & edit_yn).nonzero().squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "es_patience_count[(best_allsat & edit_yn).nonzero().squeeze()] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1], device='cuda:0')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_patience_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"early_stopping_patience\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (config[\"early_stopping_patience\"] != -1):\n",
    "    edit_yn[es_patience_count > config['early_stopping_patience']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True, False], device='cuda:0')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_yn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_yn.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (config[\"early_stopping_patience\"] != -1):\n",
    "    edit_yn[es_patience_count > config['early_stopping_patience']] = 0\n",
    "if edit_yn.sum() == 0:\n",
    "    print('break')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = {\n",
    "        \"prompt\": {\n",
    "            \"text\": source_text,\n",
    "        },\n",
    "        \"generations\": [\n",
    "            {\n",
    "                \"text\": best_text[i],\n",
    "                \"original_text\": AR_prediction_all[i],\n",
    "                \"allsat\": best_allsat[i].item(),\n",
    "                \"losses\": best_losses[i,:].tolist(),\n",
    "                \"weighted_loss\": best_weighted_loss[i].item(),\n",
    "                \"edited\": edited_at_all_yn[i].tolist(),\n",
    "            } for i in range(len(AR_prediction_all))\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "intermediate_output = {\n",
    "        \"prompt\": {\n",
    "            \"text\": source_text,\n",
    "        },\n",
    "        \"generations\": \n",
    "            int_output\n",
    "        ,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': {'text': 'Let me just start by saying I hate horse'},\n",
       " 'generations': [{'text': ' dirt. Unfortunately the majority of you end up in something that you had to get to. My only recourse is to do it myself. What would be the happy tale of my life then?',\n",
       "   'original_text': ' dirt. Unfortunately the majority of horses end up in shit that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       "   'allsat': True,\n",
       "   'losses': [138.46743774414062, 0.011374153196811676],\n",
       "   'weighted_loss': 13.856980323791504,\n",
       "   'edited': True},\n",
       "  {'text': ' wearing games and no ****ing why do I hate horse wearing games.',\n",
       "   'original_text': ' wearing games and holy ****ing shit do I hate horse wearing games.',\n",
       "   'allsat': False,\n",
       "   'losses': [54.674339294433594, 0.5787469744682312],\n",
       "   'weighted_loss': 5.988306045532227,\n",
       "   'edited': True},\n",
       "  {'text': \" fetishes: it just makes me want to be better every time I see it on the news, even though it's not worth a thing because I am the..\",\n",
       "   'original_text': \" fetishes: it just makes me want to puke every time I see it on the internet, even though it's not worth a thing because I am a furry.\",\n",
       "   'allsat': True,\n",
       "   'losses': [110.78227233886719, 0.02703019417822361],\n",
       "   'weighted_loss': 11.102554321289062,\n",
       "   'edited': True}]}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json.dump(output, outf)\n",
    "outf.write(\"\\n\")\n",
    "outf.flush()\n",
    "\n",
    "json.dump(intermediate_output, int_outf)\n",
    "int_outf.write(\"\\n\")\n",
    "int_outf.flush()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (time.time() - main_start_time) > config['server_time_limit'] * 60 * 60 * 0.9:\n",
    "    interrupted = True\n",
    "    print('break')\n",
    "\n",
    "outf.close()\n",
    "int_outf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf02970fd0fb41e8858db73a2463d244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>decode_time</td><td>1319.89143</td></tr><tr><td>num_decoded_tokens</td><td>172</td></tr><tr><td>num_edited</td><td>6</td></tr><tr><td>num_skipped</td><td>0</td></tr><tr><td>outfile_path</td><td>outputs/toxicity/dev...</td></tr><tr><td>prep_time</td><td>17.00515</td></tr><tr><td>toks_p_sec</td><td>0.13031</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vulcan-t-pol-152</strong> at: <a href='https://wandb.ai/hayleyson/toxicity-decoding/runs/45d5r7bb' target=\"_blank\">https://wandb.ai/hayleyson/toxicity-decoding/runs/45d5r7bb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240406_182129-45d5r7bb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): o151352.ingest.sentry.io:443\n",
      "https://o151352.ingest.sentry.io:443 \"POST /api/4504800232407040/envelope/ HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if config[\"resume\"]:\n",
    "    run.summary[\"decode_time\"] += time.time() - decode_start_time\n",
    "else:\n",
    "    run.summary[\"decode_time\"] = time.time() - decode_start_time\n",
    "run.summary['num_decoded_tokens'] = num_decoded_tokens\n",
    "run.summary['toks_p_sec'] = (num_decoded_tokens/run.summary['decode_time'])\n",
    "run.summary[\"num_skipped\"] = num_skipped\n",
    "run.summary[\"num_edited\"] = num_edited\n",
    "\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Locally Editing Text Generation\")\n",
    "    parser.add_argument(\n",
    "        \"--task\",\n",
    "        type=str,\n",
    "        help=\"task name\",\n",
    "        choices=[\"toxicity\", \"formality\", \"sentiment\", \"sentiment-lewis-compr\"],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--source_data\",\n",
    "        type=str,\n",
    "        default=\"data/formality/GYAFC_Corpus/Entertainment_Music/test/informal\",\n",
    "        help=\"source data path\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--source_style\", type=str, default=\"informal\", help=\"source style\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--target_style\", type=str, default=\"formal\", help=\"target style\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--target_label_ids\",\n",
    "        nargs=\"+\",\n",
    "        type=int,\n",
    "        default=[1, 1],\n",
    "        help=\"a list of indices of target label used in each of models. e.g. [1,1]\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_paths\",\n",
    "        nargs=\"+\",\n",
    "        type=str,\n",
    "        default=[\n",
    "            \"gpt2-large\",\n",
    "            \"/home/s3/hyeryung/data/loc_edit/roberta-base-pt16-formality-regressor-with-gpt2-large-embeds-rescale/epoch_17\",\n",
    "        ],\n",
    "        help=\"model paths\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tokenizer_paths\",\n",
    "        nargs=\"+\",\n",
    "        type=str,\n",
    "        default=[\n",
    "            \"gpt2-large\",\n",
    "            \"/home/s3/hyeryung/data/loc_edit/roberta-base-pt16-formality-regressor-with-gpt2-large-embeds-rescale/epoch_17\",\n",
    "        ],\n",
    "        help=\"tokenizer paths\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_types\",\n",
    "        nargs=\"+\",\n",
    "        type=str,\n",
    "        default=[\"AutoModelForCausalLM\", \"RobertaCustomForSequenceClassification\"],\n",
    "        help=\"model types\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir_prefix\",\n",
    "        type=str,\n",
    "        help=\"output directory prefix. e.g. outputs/formality/mlm-reranking\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--early_stopping_patience\",\n",
    "        type=int,\n",
    "        default=-1,\n",
    "        help=\"early stopping patience\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--method\",\n",
    "        type=str,\n",
    "        default=\"mlm-beamsearch-v0\",\n",
    "        help=\"method name\",\n",
    "        choices=[\n",
    "            \"mlm-beamsearch-v0\",\n",
    "            \"mlm-beamsearch-v1\",\n",
    "            \"mlm-beamsearch-v2\",\n",
    "            \"mlm-reranking\",\n",
    "        ],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--locate_unit\", type=str, default=\"token\", help=\"unit to locate\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_epsilons\", nargs=\"+\", type=float, default=[0.75], help=\"min epsilons\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_samples\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"number of samples to edit per prompt\",\n",
    "    )\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda\", help=\"device\")\n",
    "    parser.add_argument(\n",
    "        \"--target_type\",\n",
    "        type=str,\n",
    "        default=\"embeds\",\n",
    "        help=\"target type (embeds, simplex, probability) from prior work's code\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cache_dir\", type=str, default=\"hf_cache\", help=\"cache directory\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--jsonl_primary_key\", type=str, default=\"prompt\", help=\"jsonl primary key\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--jsonl_secondary_key\", type=str, default=\"text\", help=\"jsonl secondary key\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--losses\",\n",
    "        nargs=\"+\",\n",
    "        type=str,\n",
    "        default=[\"gpt2\", \"classification_no_prefix_logprobloss\"],\n",
    "        help=\"losses\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--build_loss_dict\",\n",
    "        type=json.loads,\n",
    "        default='{\"coeff_steps\": 200, \"coeff_pattern\": \"constant\", \"loss_type\": \"xentropy\", \"length_normalize\": false, \"AR_temperature\": 1.0, \"AR_top_k\": 0, \"AR_top_p\": 0.96, \"max_output_length\": 20}',\n",
    "        help=\"build loss dict\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_edit_token_per_step\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "        help=\"number of edit tokens per step\",\n",
    "    )\n",
    "    parser.add_argument(\"--k_per_location\", type=int, default=15, help=\"k per location\")\n",
    "    parser.add_argument(\"--n_iter\", type=int, default=3, help=\"number of iterations\")\n",
    "    parser.add_argument(\n",
    "        \"--selection_criteria\",\n",
    "        type=str,\n",
    "        default=\"weighted_sum\",\n",
    "        help=\"selection criteria\",\n",
    "    )\n",
    "    parser.add_argument(\"--closs_weight\", type=float, default=0.32, help=\"closs weight\")\n",
    "    parser.add_argument(\"--beam_size\", type=int, default=5, help=\"beam size\")\n",
    "    parser.add_argument(\n",
    "        \"--wandb_project\", type=str, default=\"mlm_reranking\", help=\"wandb project name\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--wandb_entity\", type=str, default=\"hayleyson\", help=\"wandb entity name\"\n",
    "    )\n",
    "    parser.add_argument(\"--wandb_run_id\", type=str, help=\"wandb run name\")\n",
    "    parser.add_argument(\n",
    "        \"--resume\", action=\"store_true\", help=\"whether to resume from a previous run\"\n",
    "    )\n",
    "    parser.add_argument(\"--slurm_job_id\", type=str, help=\"slurm job id (for debugging)\")\n",
    "    parser.add_argument(\n",
    "        \"--dont_skip_allsat\",\n",
    "        action=\"store_true\",\n",
    "        help=\"if this argument is passed, the module will conduct decoding on all samples even if they already satisfy constraints\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--locate_method\",\n",
    "        type=str,\n",
    "        help=\"method to use for locating tokens\",\n",
    "        choices=[\"attention\", \"grad_norm\"],\n",
    "        default=\"attention\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--server_time_limit\",\n",
    "        type=float,\n",
    "        help=\"Number of maximum hours to run the script for. Can be fractions e.g. 7.5.\",\n",
    "        default=10000\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    config = vars(args)\n",
    "\n",
    "    main(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "import math\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "os.chdir('/data/hyeryung/mucoco')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "import new_module.losses as lossbuilder\n",
    "import wandb\n",
    "# from new_module.decode_utils import (\n",
    "#     beam_rerank_v0,\n",
    "#     beam_rerank_v1,\n",
    "#     beam_rerank_v2,\n",
    "#     combi_rerank,\n",
    "# )\n",
    "from new_module.new_decode_utils import get_beam_hypotheses_v0, get_beam_hypotheses_v1, get_combi_hypotheses, final_reranking\n",
    "from new_module.evaluation.evaluate_wandb import evaluate_main\n",
    "from new_module.locate.new_locate_utils import LocateMachine\n",
    "from new_module.utils.robertacustom import RobertaCustomForSequenceClassification\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(os.environ.get(\"LOGGING_LEVEL\", logging.DEBUG))\n",
    "\n",
    "import joblib\n",
    "config = joblib.load('config.pkl')\n",
    "config['device'] = 'cuda:7'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "int_outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/generation_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "class dummyArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "build_loss_args = dummyArgs(**config[\"build_loss_dict\"])\n",
    "\n",
    "## load data\n",
    "if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "    source_dataset = [\n",
    "        json.loads(l)[config[\"jsonl_primary_key\"]][config[\"jsonl_secondary_key\"]]\n",
    "        for l in open(config[\"source_data\"])\n",
    "    ]\n",
    "    generation_dataset = [\n",
    "        json.loads(l)[\"generations\"] for l in open(config[\"source_data\"])\n",
    "    ]\n",
    "elif (config[\"task\"] == \"formality\") or (config[\"task\"] == \"sentiment-lewis-compr\"):\n",
    "    with open(config[\"source_data\"], \"r\") as f:\n",
    "        generation_dataset = [line.rstrip('\\n') for line in f.readlines()]\n",
    "    source_dataset = [\"\" for l in generation_dataset]\n",
    "\n",
    "## load tokenizer, models, define losses\n",
    "name2tokenizer = {}\n",
    "name2model = {}\n",
    "name2config = {}\n",
    "loss2tokenizer = {}\n",
    "embed_luts = []\n",
    "\n",
    "for i, model_path in enumerate(config[\"model_paths\"]):\n",
    "    if (\n",
    "        model_path not in name2model\n",
    "    ):  # making sure we are not loading the model twice in case some constraints use the same model.\n",
    "        try:\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=True,\n",
    "            )\n",
    "        except:\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=False,\n",
    "            )\n",
    "\n",
    "        name2config[model_path] = AutoConfig.from_pretrained(\n",
    "            model_path, cache_dir=config[\"cache_dir\"]\n",
    "        )\n",
    "\n",
    "        if config[\"model_types\"][i] == \"RobertaCustomForSequenceClassification\":\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                RobertaCustomForSequenceClassification.from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(transformers, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        name2model[model_path].eval()\n",
    "        name2model[model_path].to(config['device'])\n",
    "\n",
    "    input_embeds = name2model[model_path].get_input_embeddings()\n",
    "    if isinstance(input_embeds, torch.nn.Sequential):\n",
    "        input_embeds = input_embeds[0]\n",
    "    embed_luts.append(input_embeds)\n",
    "\n",
    "    if config[\"target_type\"] == \"embeds\":\n",
    "        embed_luts[-1].requires_grad = False\n",
    "\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "mlm = None if config[\"method\"] == \"mlm-beamsearch-v2\" else AutoModelForMaskedLM.from_pretrained(\"roberta-base\").to(config['device'])\n",
    "\n",
    "lossfns = []\n",
    "for i, loss in enumerate(config[\"losses\"]):\n",
    "    lossfns.append(\n",
    "        lossbuilder.build_loss(\n",
    "            loss,\n",
    "            name2model[config[\"model_paths\"][i]],\n",
    "            name2tokenizer[config[\"tokenizer_paths\"][i]],\n",
    "            build_loss_args,\n",
    "        )\n",
    "    )\n",
    "    lossfns[i].tokenizer.add_special_tokens({\"mask_token\": mlm_tokenizer.mask_token})\n",
    "    loss2tokenizer[loss] = lossfns[i].tokenizer\n",
    "\n",
    "# define an object to locate problematic phrases\n",
    "locator = LocateMachine(lossfns[1].model, lossfns[1].tokenizer)\n",
    "\n",
    "label_ids = config[\"target_label_ids\"]  # target label's ids for each loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume_idx = 0\n",
    "loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "interrupted = False\n",
    "# for text_id in range(len(source_dataset))[resume_idx:]:\n",
    "text_id = 34\n",
    "source_text = source_dataset[text_id]\n",
    "if source_text == \"\":\n",
    "    source_text = lossfns[0].tokenizer.bos_token\n",
    "if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "    AR_prediction_all = [x[\"text\"] for x in generation_dataset[text_id]]\n",
    "elif (config[\"task\"] == \"formality\") or (\n",
    "    config[\"task\"] == \"sentiment-lewis-compr\"\n",
    "):\n",
    "    AR_prediction_all = [generation_dataset[text_id]]\n",
    "\n",
    "curr_num_samples = len(AR_prediction_all)\n",
    "\n",
    "curr_loss = torch.zeros(len(AR_prediction_all)).to(config['device'])\n",
    "logging_loss = torch.zeros((len(AR_prediction_all),len(config[\"losses\"]))).to(config['device'])\n",
    "edit_yn = torch.ones(len(AR_prediction_all), dtype=torch.bool).to(config['device'])\n",
    "        \n",
    "for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "    with torch.no_grad():\n",
    "        lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "            source_text, AR_prediction_all,\n",
    "            label_id=config['target_label_ids'][lossid],\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "    curr_loss += loss_weights[lossid] * lossvalue\n",
    "    logging_loss[:, lossid] = lossvalue.clone()\n",
    "\n",
    "\n",
    "allsat = logging_loss[:,1] < -math.log(config[\"min_epsilons\"][0])\n",
    "allsat_ix = allsat.nonzero().squeeze(0)\n",
    "if (not config[\"dont_skip_allsat\"]):\n",
    "    edit_yn[allsat_ix] = False\n",
    "edited_at_all_yn = edit_yn.detach().clone()\n",
    "\n",
    "es_patience_count = torch.zeros(len(AR_prediction_all),dtype=torch.long).to(config['device'])\n",
    "best_allsat = allsat.detach().clone()\n",
    "best_losses = logging_loss.detach().clone()\n",
    "best_weighted_loss = curr_loss.detach().clone()            \n",
    "best_text = deepcopy(AR_prediction_all)\n",
    "running_text = [x for i, x in enumerate(AR_prediction_all) if edit_yn[i]] ## 실제 고쳐야 할 sample만 가지고 있음\n",
    "int_output = [{} for _ in range(len(AR_prediction_all))]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# for _iter in range(config['n_iter']):\n",
    "_iter = 0\n",
    "## masked_text : N (num samples to edit)\n",
    "masked_text = locator.locate_main(running_text, \n",
    "                        method = config['locate_method'], \n",
    "                        max_num_tokens = config['num_edit_token_per_step'], \n",
    "                        unit = config['locate_unit'], \n",
    "                        num_layer = 10,#-2, #penultimate\n",
    "                        label_id = config['target_label_ids'][1])\n",
    "\n",
    "## replace tokens at the indices with mask tokens\n",
    "\n",
    "inputs = mlm_tokenizer(\n",
    "    masked_text, return_tensors=\"pt\", padding=True, truncation=True\n",
    ")\n",
    "inputs = inputs.to(config['device']) \n",
    "masked_sequence=inputs['input_ids']\n",
    "\n",
    "## make predictions for the masked indices\n",
    "with torch.no_grad():\n",
    "    logits = mlm(**inputs).logits\n",
    "\n",
    "special_token_ids = mlm_tokenizer.convert_tokens_to_ids(mlm_tokenizer.all_special_tokens)\n",
    "logits[:, :, special_token_ids] = -float(\"inf\")\n",
    "\n",
    "indices_in_mlm_tokens = (\n",
    "    inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    ").nonzero(as_tuple=True)\n",
    "\n",
    "## get top k tokens for each index\n",
    "predicted_token_ids = torch.topk(\n",
    "    logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "    k=config['k_per_location'],\n",
    "    dim=-1,\n",
    ")\n",
    "\n",
    "if config[\"method\"] == \"mlm-beamsearch-v0\":\n",
    "    hypotheses = get_beam_hypotheses_v0(source_text, \n",
    "            masked_sequence, \n",
    "            indices_in_mlm_tokens,\n",
    "            predicted_token_ids.indices,\n",
    "            mlm_tokenizer, \n",
    "            lossfns,\n",
    "            config)\n",
    "elif config[\"method\"] == \"mlm-beamsearch-v1\":\n",
    "    hypotheses = get_beam_hypotheses_v1(source_text, \n",
    "            masked_sequence, \n",
    "            indices_in_mlm_tokens,\n",
    "            predicted_token_ids.indices,\n",
    "            mlm_tokenizer, \n",
    "            lossfns,\n",
    "            config)\n",
    "elif config[\"method\"] == \"mlm-reranking\":\n",
    "    hypotheses = get_combi_hypotheses(masked_sequence, \n",
    "                indices_in_mlm_tokens,\n",
    "                predicted_token_ids.indices,\n",
    "                mlm_tokenizer,\n",
    "                config)\n",
    "    \n",
    "final_hypotheses_, new_best_weighted_loss_, new_best_allsat_, new_best_logging_loss_ = final_reranking(source_text,\n",
    "                                                                                                    hypotheses,\n",
    "                                                                                                    lossfns,\n",
    "                                                                                                    config,\n",
    "                                                                                                    batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\", I think you\\'ve got to prove it to everybody in the media.', \" sexist, I want to see me like I'm a man.\", \" racist, or a sexist, or a racist, or a conspiracy theorist or says you're in it\", '\", well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many of the parties or candidates in the party.', '\", you are wrong.\" He was, he, James Durden for not paying attention when he came up with his own name.', '… or I call you out of your privilege, you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for our culture.', \"It's not, then you should take a step back. Because here's a little fact.\"]\n",
      "tensor([ 5.447,  4.272,  5.125, 10.781, 11.166, 10.932,  6.684],\n",
      "       device='cuda:7')\n",
      "tensor([ True, False, False,  True,  True,  True,  True], device='cuda:7')\n",
      "tensor([[   54.399,     0.008],\n",
      "        [   40.785,     0.216],\n",
      "        [   43.044,     0.911],\n",
      "        [  107.677,     0.015],\n",
      "        [  111.426,     0.026],\n",
      "        [  108.389,     0.104],\n",
      "        [   66.649,     0.022]], device='cuda:7')\n"
     ]
    }
   ],
   "source": [
    "print(final_hypotheses_)\n",
    "print(new_best_weighted_loss_)\n",
    "print(new_best_allsat_)\n",
    "print(new_best_logging_loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.391, 7.559, 5.073, 8.513, 9.152, 9.713, 5.863], device='cuda:7')\n",
      "tensor([False, False, False, False, False, False, False], device='cuda:7')\n",
      "tensor([[33.326,  1.177],\n",
      "        [32.255,  4.815],\n",
      "        [44.340,  0.710],\n",
      "        [77.809,  0.813],\n",
      "        [87.941,  0.397],\n",
      "        [90.010,  0.791],\n",
      "        [55.694,  0.326]], device='cuda:7')\n"
     ]
    }
   ],
   "source": [
    "print(best_weighted_loss)\n",
    "print(best_allsat)\n",
    "print(best_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True], device='cuda:7')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_yn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_best_weighted_loss = torch.empty((len(AR_prediction_all),)).fill_(float(\"inf\")).to(config['device'])\n",
    "new_best_weighted_loss[edit_yn] = new_best_weighted_loss_\n",
    "\n",
    "new_best_logging_loss = torch.empty((len(AR_prediction_all), len(config['losses']))).fill_(float(\"inf\")).to(config['device'])\n",
    "new_best_logging_loss[edit_yn, :] = new_best_logging_loss_\n",
    "\n",
    "new_best_allsat = torch.zeros((len(AR_prediction_all),)).bool().to(config['device'])\n",
    "new_best_allsat[edit_yn] = new_best_allsat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 5.447,  4.272,  5.125, 10.781, 11.166, 10.932,  6.684],\n",
       "        device='cuda:7'),\n",
       " tensor([[   54.399,     0.008],\n",
       "         [   40.785,     0.216],\n",
       "         [   43.044,     0.911],\n",
       "         [  107.677,     0.015],\n",
       "         [  111.426,     0.026],\n",
       "         [  108.389,     0.104],\n",
       "         [   66.649,     0.022]], device='cuda:7'),\n",
       " tensor([ True, False, False,  True,  True,  True,  True], device='cuda:7'))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_best_weighted_loss, new_best_logging_loss, new_best_allsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_ixes = edit_yn.nonzero().squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6], device='cuda:7')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hypotheses = [final_hypotheses_[torch.where(edit_ixes==i)[0].item()] if edit_yn[i] else '' for i in range(len(AR_prediction_all))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\", I think you\\'ve got to prove it to everybody in the media.',\n",
       " \" sexist, I want to see me like I'm a man.\",\n",
       " \" racist, or a sexist, or a racist, or a conspiracy theorist or says you're in it\",\n",
       " '\", well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many of the parties or candidates in the party.',\n",
       " '\", you are wrong.\" He was, he, James Durden for not paying attention when he came up with his own name.',\n",
       " '… or I call you out of your privilege, you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for our culture.',\n",
       " \"It's not, then you should take a step back. Because here's a little fact.\"]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hypotheses_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\", I think you\\'ve got to prove it to everybody in the media.',\n",
       " \" sexist, I want to see me like I'm a man.\",\n",
       " \" racist, or a sexist, or a racist, or a conspiracy theorist or says you're in it\",\n",
       " '\", well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many of the parties or candidates in the party.',\n",
       " '\", you are wrong.\" He was, he, James Durden for not paying attention when he came up with his own name.',\n",
       " '… or I call you out of your privilege, you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for our culture.',\n",
       " \"It's not, then you should take a step back. Because here's a little fact.\"]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "update = torch.Tensor([]).bool().to(config['device'])\n",
    "if config['selection_criteria'] == \"weighted_sum\":\n",
    "    update = best_weighted_loss > new_best_weighted_loss ## edit_yn이 false 였던 곳은 무조건 false\n",
    "elif config['selection_criteria'] == \"allsat_primary\":\n",
    "    update = (~best_allsat & new_best_allsat) | \\\n",
    "            (~best_allsat & ~new_best_allsat & (best_weighted_loss > new_best_weighted_loss)) | \\\n",
    "            (best_allsat & new_best_allsat & (best_losses[:, 0] > new_best_logging_loss[:, 0])) \n",
    "            ## (~best_allsat & new_best_allsat) : edit_yn이 false였던 곳은 무조건 false\n",
    "            ## (~best_allsat & ~new_best_allsat & (best_weighted_loss > new_best_weighted_loss)) : edit_yn이 false 였던 곳은 무조건 false\n",
    "            ## (best_allsat & new_best_allsat & (best_losses[:, 0] > new_best_logging_loss[:, 0])) : edit_yn이 false였던 곳은 무조건 false\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True, False,  True,  True,  True,  True], device='cuda:7')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "update = (update & edit_yn) # edit 대상인 것들만 update하기 위해서 update 조건에 edit_yn을 sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True, False,  True,  True,  True,  True], device='cuda:7')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{}, {}, {}, {}, {}, {}, {}]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(0, device='cuda:7')\n",
      "1\n",
      "tensor(1, device='cuda:7')\n",
      "2\n",
      "tensor(2, device='cuda:7')\n",
      "3\n",
      "tensor(3, device='cuda:7')\n",
      "4\n",
      "tensor(4, device='cuda:7')\n",
      "5\n",
      "tensor(5, device='cuda:7')\n",
      "6\n",
      "tensor(6, device='cuda:7')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## intermediate output for debugging\n",
    "# for sample_ix in edit_yn.nonzero().squeeze(-1).tolist(): # edit 대상인 것들만 update.\n",
    "\n",
    "for sample_ix in range(len(running_text)): # edit 대상인 것들만 update.\n",
    "    print(sample_ix)\n",
    "    print(edit_ixes[sample_ix])\n",
    "    int_output[edit_ixes[sample_ix]].update({f\"iter{_iter}_original_sentence\": running_text[sample_ix],\n",
    "                                            f\"iter{_iter}_masked_sentence\": masked_text[sample_ix],\n",
    "                                            f\"iter{_iter}_best_text\": final_hypotheses[edit_ixes[sample_ix]],\n",
    "                                            f\"iter{_iter}_update\": update[sample_ix].item()})    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'iter0_original_sentence': \" homophobe, you've got to prove it to everybody in the media.\",\n",
       "  'iter0_masked_sentence': \"<mask><mask><mask><mask> you've got to prove it to everybody in the media.\",\n",
       "  'iter0_best_text': '\", I think you\\'ve got to prove it to everybody in the media.',\n",
       "  'iter0_update': True},\n",
       " {'iter0_original_sentence': \" sexist ’don't treat me like I'm a bitch.\",\n",
       "  'iter0_masked_sentence': \" sexist<mask><mask><mask><mask><mask> me like I'm a<mask><mask>\",\n",
       "  'iter0_best_text': \" sexist, I want to see me like I'm a man.\",\n",
       "  'iter0_update': True},\n",
       " {'iter0_original_sentence': \" racist, or a pro-whistleblower or a conspiracy theorist or says you're in it\",\n",
       "  'iter0_masked_sentence': \" racist, or a<mask><mask><mask><mask><mask><mask> or a<mask> theorist or says you're in it\",\n",
       "  'iter0_best_text': \" racist, or a sexist, or a racist, or a conspiracy theorist or says you're in it\",\n",
       "  'iter0_update': False},\n",
       " {'iter0_original_sentence': ' racist, well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many \"racist\" or \"sexist\" Republicans',\n",
       "  'iter0_masked_sentence': '<mask><mask> well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many<mask><mask><mask> or<mask><mask><mask><mask><mask>',\n",
       "  'iter0_best_text': '\", well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many of the parties or candidates in the party.',\n",
       "  'iter0_update': True},\n",
       " {'iter0_original_sentence': ' racist, you are wrong.\"\\n\\nWollmann criticized Durden for not paying attention when he came up with his family joke.',\n",
       "  'iter0_masked_sentence': '<mask><mask> you are wrong.\"<mask><mask><mask><mask><mask><mask> Durden for not paying attention when he came up with his<mask><mask><mask>',\n",
       "  'iter0_best_text': '\", you are wrong.\" He was, he, James Durden for not paying attention when he came up with his own name.',\n",
       "  'iter0_update': True},\n",
       " {'iter0_original_sentence': ' Nazi or I hate you because of your race, you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for Jews’',\n",
       "  'iter0_masked_sentence': '<mask> or I<mask> you<mask> of your<mask><mask> you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for<mask><mask><mask>',\n",
       "  'iter0_best_text': '… or I call you out of your privilege, you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for our culture.',\n",
       "  'iter0_update': True},\n",
       " {'iter0_original_sentence': \" bigot’, then you should take your hate test. Because here's a little fact.\",\n",
       "  'iter0_masked_sentence': \"<mask><mask><mask><mask> then you should take<mask><mask><mask><mask> Because here's a little fact.\",\n",
       "  'iter0_best_text': \"It's not, then you should take a step back. Because here's a little fact.\",\n",
       "  'iter0_update': True}]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update.nonzero().squeeze(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# update running_text, best_text, best_allsat, best_losses, best_weighted_loss\n",
    "for update_index in update.nonzero().squeeze(-1).tolist():\n",
    "    print(update_index)\n",
    "    best_text[update_index] = final_hypotheses[update_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\", I think you\\'ve got to prove it to everybody in the media.',\n",
       " \" sexist, I want to see me like I'm a man.\",\n",
       " \" racist, or a pro-whistleblower or a conspiracy theorist or says you're in it\",\n",
       " '\", well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many of the parties or candidates in the party.',\n",
       " '\", you are wrong.\" He was, he, James Durden for not paying attention when he came up with his own name.',\n",
       " '… or I call you out of your privilege, you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for our culture.',\n",
       " \"It's not, then you should take a step back. Because here's a little fact.\"]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False], device='cuda:7')"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_allsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_allsat[update] = new_best_allsat[update]\n",
    "best_losses[update] = new_best_logging_loss[update]\n",
    "best_weighted_loss[update] = new_best_weighted_loss[update]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False,  True,  True,  True,  True], device='cuda:7')"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_allsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   54.399,     0.008],\n",
       "        [   40.785,     0.216],\n",
       "        [   44.340,     0.710],\n",
       "        [  107.677,     0.015],\n",
       "        [  111.426,     0.026],\n",
       "        [  108.389,     0.104],\n",
       "        [   66.649,     0.022]], device='cuda:7')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.447,  4.272,  5.073, 10.781, 11.166, 10.932,  6.684],\n",
       "       device='cuda:7')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "es_patience_count[(best_allsat & edit_yn).nonzero().squeeze(-1)] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 1, 1, 1], device='cuda:7')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_patience_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (config[\"early_stopping_patience\"] != -1):\n",
    "    edit_yn[es_patience_count > config['early_stopping_patience']] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True,  True, False, False, False, False], device='cuda:7')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_yn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "running_text = [x for i, x in enumerate(final_hypotheses) if edit_yn[i]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\", I think you\\'ve got to prove it to everybody in the media.',\n",
       " \" sexist, I want to see me like I'm a man.\",\n",
       " \" racist, or a sexist, or a racist, or a conspiracy theorist or says you're in it\",\n",
       " '\", well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many of the parties or candidates in the party.',\n",
       " '\", you are wrong.\" He was, he, James Durden for not paying attention when he came up with his own name.',\n",
       " '… or I call you out of your privilege, you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for our culture.',\n",
       " \"It's not, then you should take a step back. Because here's a little fact.\"]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" sexist, I want to see me like I'm a man.\",\n",
       " \" racist, or a sexist, or a racist, or a conspiracy theorist or says you're in it\"]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# output = {\n",
    "#             \"prompt\": {\n",
    "#                 \"text\": source_text,\n",
    "#             },\n",
    "#             \"generations\": [\n",
    "#                 {\n",
    "#                     \"text\": best_text[i],\n",
    "#                     \"original_text\": AR_prediction_all[i],\n",
    "#                     \"allsat\": best_allsat[i].item(),\n",
    "#                     \"losses\": best_losses[i,:].tolist(),\n",
    "#                     \"weighted_loss\": best_weighted_loss[i].item(),\n",
    "#                     \"edited\": edited_at_all_yn[i].tolist(),\n",
    "#                 } for i in range(len(AR_prediction_all))\n",
    "#             ],\n",
    "#         }\n",
    "    \n",
    "# intermediate_output = {\n",
    "#         \"prompt\": {\n",
    "#             \"text\": source_text,\n",
    "#         },\n",
    "#         \"generations\": \n",
    "#             int_output\n",
    "#         ,\n",
    "#     }\n",
    "\n",
    "# outputs.append(output)\n",
    "# int_outputs.append(intermediate_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs=outputs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_outputs=int_outputs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" sexist, I want to see me like I'm a man.\",\n",
       " \" racist, or a sexist, or a racist, or a conspiracy theorist or says you're in it\"]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# for _iter in range(config['n_iter']):\n",
    "_iter = 1\n",
    "## masked_text : N (num samples to edit)\n",
    "masked_text = locator.locate_main(running_text, \n",
    "                        method = config['locate_method'], \n",
    "                        max_num_tokens = config['num_edit_token_per_step'], \n",
    "                        unit = config['locate_unit'], \n",
    "                        num_layer = 10,#-2, #penultimate\n",
    "                        label_id = config['target_label_ids'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<mask><mask> I want to see me<mask> I'm a<mask><mask>\",\n",
       " \"<mask><mask> or a<mask><mask> or a<mask><mask> or a conspiracy theorist or says you're in it\"]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## replace tokens at the indices with mask tokens\n",
    "\n",
    "inputs = mlm_tokenizer(\n",
    "    masked_text, return_tensors=\"pt\", padding=True, truncation=True\n",
    ")\n",
    "inputs = inputs.to(config['device']) \n",
    "masked_sequence=inputs['input_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 50264, 50264,    38,   236,     7,   192,   162, 50264,    38,\n",
       "           437,    10, 50264, 50264,     2,     1,     1,     1,     1,     1,\n",
       "             1,     1],\n",
       "        [    0, 50264, 50264,    50,    10, 50264, 50264,    50,    10, 50264,\n",
       "         50264,    50,    10,  6556, 40646,    50,   161,    47,   214,    11,\n",
       "            24,     2]], device='cuda:7'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:7')}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm=mlm.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## make predictions for the masked indices\n",
    "with torch.no_grad():\n",
    "    logits = mlm(**inputs).logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 50265])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "special_token_ids = mlm_tokenizer.convert_tokens_to_ids(mlm_tokenizer.all_special_tokens)\n",
    "logits[:, :, special_token_ids] = -float(\"inf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  -inf,   -inf,   -inf,  ..., -1.631,  1.295,   -inf],\n",
       "         [  -inf,   -inf,   -inf,  ..., -5.229, -3.584,   -inf],\n",
       "         [  -inf,   -inf,   -inf,  ..., -6.065, -5.619,   -inf],\n",
       "         ...,\n",
       "         [  -inf,   -inf,   -inf,  ..., -6.791, -5.273,   -inf],\n",
       "         [  -inf,   -inf,   -inf,  ..., -6.791, -5.273,   -inf],\n",
       "         [  -inf,   -inf,   -inf,  ..., -6.791, -5.273,   -inf]],\n",
       "\n",
       "        [[  -inf,   -inf,   -inf,  ..., -0.751,  1.547,   -inf],\n",
       "         [  -inf,   -inf,   -inf,  ..., -7.387, -5.483,   -inf],\n",
       "         [  -inf,   -inf,   -inf,  ..., -8.430, -5.503,   -inf],\n",
       "         ...,\n",
       "         [  -inf,   -inf,   -inf,  ...,  0.904,  0.146,   -inf],\n",
       "         [  -inf,   -inf,   -inf,  ..., -4.306, -2.841,   -inf],\n",
       "         [  -inf,   -inf,   -inf,  ..., -4.439, -2.868,   -inf]]],\n",
       "       device='cuda:7')"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices_in_mlm_tokens = (\n",
    "    inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    ").nonzero(as_tuple=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:7'),\n",
       " tensor([ 1,  2,  8, 12, 13,  1,  2,  5,  6,  9, 10], device='cuda:7'))"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_in_mlm_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## get top k tokens for each index\n",
    "predicted_token_ids = torch.topk(\n",
    "    logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "    k=config['k_per_location'],\n",
    "    dim=-1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[15.585, 15.555, 14.680, 14.383, 14.204, 13.978, 13.797, 13.788, 13.612,\n",
       "         13.604],\n",
       "        [12.254, 12.003, 11.934, 11.026, 10.672, 10.525, 10.516, 10.505, 10.485,\n",
       "         10.480],\n",
       "        [15.644, 15.021, 14.282, 14.254, 14.228, 13.991, 13.822, 13.389, 13.369,\n",
       "         13.181],\n",
       "        [12.463, 11.636, 11.441, 11.148, 11.039, 10.726, 10.711, 10.701, 10.618,\n",
       "         10.541],\n",
       "        [13.247, 12.326, 11.850, 10.051,  9.906,  9.486,  9.482,  9.342,  9.223,\n",
       "          8.698],\n",
       "        [16.050, 15.052, 14.724, 14.512, 14.164, 14.158, 13.915, 13.170, 13.001,\n",
       "         12.885],\n",
       "        [11.483, 10.134,  9.806,  9.596,  9.556,  9.454,  9.377,  9.352,  9.261,\n",
       "          9.240],\n",
       "        [11.707, 11.003, 10.931, 10.454, 10.337, 10.335, 10.254, 10.195,  9.802,\n",
       "          9.676],\n",
       "        [12.549, 12.110, 11.796, 10.401, 10.378, 10.309, 10.118, 10.091, 10.073,\n",
       "         10.070],\n",
       "        [12.079, 11.187, 10.946, 10.704, 10.443, 10.422, 10.318, 10.238,  9.940,\n",
       "          9.759],\n",
       "        [12.575, 11.974, 11.942, 10.447, 10.446, 10.383, 10.371, 10.137, 10.108,\n",
       "         10.090]], device='cuda:7'),\n",
       "indices=tensor([[  113,  7608,  2264,   100,  6179,  5975,   894,   133,  2409, 12375],\n",
       "        [    6,     4,   109,    35,  1708,   222,   122,  8275,   116,  2409],\n",
       "        [   77,     4,   142,   137,   101,    25,   206,   224,   114,   116],\n",
       "        [  313, 13317,  9916, 21068,   693, 28731,  6132,  7251,  4607,  1050],\n",
       "        [    4,    72,   116,  1917,   113,   955,   313,   328,    35,  1174],\n",
       "        [  250,   133,  6323,   102, 11094, 29375,  4688, 21518, 40100,   713],\n",
       "        [    6, 28587,  2173,   621,  8676, 33747, 30603,   313, 26457,  6176],\n",
       "        [ 6556, 31150, 17145,   168, 32310,   588,   559, 42196,  1099, 11728],\n",
       "        [    6,   661, 40646,  2936,  3121,   397, 22860,   621,   254,   368],\n",
       "        [ 6556, 31150, 17145, 32310,   168,   559,   588, 42196,  1104, 11728],\n",
       "        [  661,     6, 40646,  2936,  3121,   397,   254, 22860,   405,   621]],\n",
       "       device='cuda:7'))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_token_ids_old' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredicted_token_ids_old\u001b[49m\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m==\u001b[39m predicted_token_ids\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predicted_token_ids_old' is not defined"
     ]
    }
   ],
   "source": [
    "predicted_token_ids_old.values == predicted_token_ids.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]],\n",
       "       device='cuda:7')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_ids_old.indices == predicted_token_ids.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sequence = inputs.input_ids\n",
    "if config[\"method\"] == \"mlm-beamsearch-v0\":\n",
    "    hypotheses = get_beam_hypotheses_v0(source_text, \n",
    "            masked_sequence, \n",
    "            indices_in_mlm_tokens,\n",
    "            predicted_token_ids.indices,\n",
    "            mlm_tokenizer, \n",
    "            lossfns,\n",
    "            config)\n",
    "elif config[\"method\"] == \"mlm-beamsearch-v1\":\n",
    "    hypotheses = get_beam_hypotheses_v1(source_text, \n",
    "            masked_sequence, \n",
    "            indices_in_mlm_tokens,\n",
    "            predicted_token_ids.indices,\n",
    "            mlm_tokenizer, \n",
    "            lossfns,\n",
    "            config)\n",
    "elif config[\"method\"] == \"mlm-reranking\":\n",
    "    hypotheses = get_combi_hypotheses(masked_sequence, \n",
    "                indices_in_mlm_tokens,\n",
    "                predicted_token_ids.indices,\n",
    "                mlm_tokenizer,\n",
    "                config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\". I want to see me. I\\'m a man.',\n",
       "  '\". I want to see me. I\\'m a woman.',\n",
       "  '\"? I want to see me. I\\'m a man.',\n",
       "  '\"? I want to see me. I\\'m a woman.',\n",
       "  '\". I want to see me. I\\'m a kid.'],\n",
       " [\"a, or a conspiracy theorist or a conspiracy theorist or a conspiracy theorist or says you're in it\",\n",
       "  \"a, or a conspiracy theorist or a realist or a conspiracy theorist or says you're in it\",\n",
       "  \"a, or a conspiracy theorist or a conspiracy, or a conspiracy theorist or says you're in it\",\n",
       "  \"a, or a bad person or a conspiracy theorist or a conspiracy theorist or says you're in it\",\n",
       "  \"a, or a conspiracy theorist or a conspiracy person or a conspiracy theorist or says you're in it\"]]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "final_hypotheses_, new_best_weighted_loss_, new_best_allsat_, new_best_logging_loss_ = final_reranking(source_text,\n",
    "                                                                                                    hypotheses,\n",
    "                                                                                                    lossfns,\n",
    "                                                                                                    config,\n",
    "                                                                                                    batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\". I want to see me. I\\'m a man.', \"a, or a conspiracy theorist or a conspiracy theorist or a conspiracy theorist or says you're in it\"]\n",
      "[4.857304096221924, 6.436732769012451]\n",
      "[True, True]\n",
      "[[48.0064697265625, 0.06295201927423477], [63.938995361328125, 0.0475926548242569]]\n"
     ]
    }
   ],
   "source": [
    "print(final_hypotheses_)\n",
    "print(new_best_weighted_loss_.tolist())\n",
    "print(new_best_allsat_.tolist())\n",
    "print(new_best_logging_loss_.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hypotheses_old' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhypotheses_old\u001b[49m[best_ix_old]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hypotheses_old' is not defined"
     ]
    }
   ],
   "source": [
    "hypotheses_old[best_ix_old]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6.6754633940756305, [66.51388549804688, 0.026749826967716217], True)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ix_old, candidate_total_losses_old[best_ix_old], candidate_losses_for_loggings_old[best_ix_old], candidate_allsats_old[best_ix_old]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.447,  4.272,  5.073, 10.781, 11.166, 10.932,  6.684],\n",
      "       device='cuda:7')\n",
      "tensor([ True, False, False,  True,  True,  True,  True], device='cuda:7')\n",
      "tensor([[   54.399,     0.008],\n",
      "        [   40.785,     0.216],\n",
      "        [   44.340,     0.710],\n",
      "        [  107.677,     0.015],\n",
      "        [  111.426,     0.026],\n",
      "        [  108.389,     0.104],\n",
      "        [   66.649,     0.022]], device='cuda:7')\n"
     ]
    }
   ],
   "source": [
    "print(best_weighted_loss)\n",
    "print(best_allsat)\n",
    "print(best_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True,  True, False, False, False, False], device='cuda:7')"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_yn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_best_weighted_loss = torch.empty((len(AR_prediction_all),)).fill_(float(\"inf\")).to(config['device'])\n",
    "new_best_weighted_loss[edit_yn] = new_best_weighted_loss_\n",
    "\n",
    "new_best_logging_loss = torch.empty((len(AR_prediction_all), len(config['losses']))).fill_(float(\"inf\")).to(config['device'])\n",
    "new_best_logging_loss[edit_yn, :] = new_best_logging_loss_\n",
    "\n",
    "new_best_allsat = torch.zeros((len(AR_prediction_all),)).bool().to(config['device'])\n",
    "new_best_allsat[edit_yn] = new_best_allsat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  inf, 4.857, 6.437,   inf,   inf,   inf,   inf], device='cuda:7'),\n",
       " tensor([[      inf,       inf],\n",
       "         [   48.006,     0.063],\n",
       "         [   63.939,     0.048],\n",
       "         [      inf,       inf],\n",
       "         [      inf,       inf],\n",
       "         [      inf,       inf],\n",
       "         [      inf,       inf]], device='cuda:7'),\n",
       " tensor([False,  True,  True, False, False, False, False], device='cuda:7'))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_best_weighted_loss, new_best_logging_loss, new_best_allsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  inf, 4.857, 6.437,   inf,   inf,   inf,   inf], device='cuda:7'),\n",
       " tensor([[      inf,       inf],\n",
       "         [   48.006,     0.063],\n",
       "         [   63.939,     0.048],\n",
       "         [      inf,       inf],\n",
       "         [      inf,       inf],\n",
       "         [      inf,       inf],\n",
       "         [      inf,       inf]], device='cuda:7'),\n",
       " tensor([False,  True,  True, False, False, False, False], device='cuda:7'))"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_best_weighted_loss, new_best_logging_loss, new_best_allsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_ixes = edit_yn.nonzero().squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2], device='cuda:7')"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\". I want to see me. I\\'m a man.',\n",
       " \"a, or a conspiracy theorist or a conspiracy theorist or a conspiracy theorist or says you're in it\"]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hypotheses_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hypotheses = [final_hypotheses_[torch.where(edit_ixes==i)[0].item()] if edit_yn[i] else '' for i in range(len(AR_prediction_all))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\". I want to see me. I\\'m a man.',\n",
       " \"a, or a conspiracy theorist or a conspiracy theorist or a conspiracy theorist or says you're in it\"]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hypotheses_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '\". I want to see me. I\\'m a man.',\n",
       " \"a, or a conspiracy theorist or a conspiracy theorist or a conspiracy theorist or says you're in it\",\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "update = torch.Tensor([]).bool().to(config['device'])\n",
    "if config['selection_criteria'] == \"weighted_sum\":\n",
    "    update = best_weighted_loss > new_best_weighted_loss ## edit_yn이 false 였던 곳은 무조건 false\n",
    "elif config['selection_criteria'] == \"allsat_primary\":\n",
    "    update = (~best_allsat & new_best_allsat) | \\\n",
    "            (~best_allsat & ~new_best_allsat & (best_weighted_loss > new_best_weighted_loss)) | \\\n",
    "            (best_allsat & new_best_allsat & (best_losses[:, 0] > new_best_logging_loss[:, 0])) \n",
    "            ## (~best_allsat & new_best_allsat) : edit_yn이 false였던 곳은 무조건 false\n",
    "            ## (~best_allsat & ~new_best_allsat & (best_weighted_loss > new_best_weighted_loss)) : edit_yn이 false 였던 곳은 무조건 false\n",
    "            ## (best_allsat & new_best_allsat & (best_losses[:, 0] > new_best_logging_loss[:, 0])) : edit_yn이 false였던 곳은 무조건 false\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True,  True, False, False, False, False], device='cuda:7')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "update = (update & edit_yn) # edit 대상인 것들만 update하기 위해서 update 조건에 edit_yn을 sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True,  True, False, False, False, False], device='cuda:7')"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'iter0_original_sentence': \" homophobe, you've got to prove it to everybody in the media.\",\n",
       "  'iter0_masked_sentence': \"<mask><mask><mask><mask> you've got to prove it to everybody in the media.\",\n",
       "  'iter0_best_text': '\", I think you\\'ve got to prove it to everybody in the media.',\n",
       "  'iter0_update': True},\n",
       " {'iter0_original_sentence': \" sexist ’don't treat me like I'm a bitch.\",\n",
       "  'iter0_masked_sentence': \" sexist<mask><mask><mask><mask><mask> me like I'm a<mask><mask>\",\n",
       "  'iter0_best_text': \" sexist, I want to see me like I'm a man.\",\n",
       "  'iter0_update': True},\n",
       " {'iter0_original_sentence': \" racist, or a pro-whistleblower or a conspiracy theorist or says you're in it\",\n",
       "  'iter0_masked_sentence': \" racist, or a<mask><mask><mask><mask><mask><mask> or a<mask> theorist or says you're in it\",\n",
       "  'iter0_best_text': \" racist, or a sexist, or a racist, or a conspiracy theorist or says you're in it\",\n",
       "  'iter0_update': False},\n",
       " {'iter0_original_sentence': ' racist, well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many \"racist\" or \"sexist\" Republicans',\n",
       "  'iter0_masked_sentence': '<mask><mask> well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many<mask><mask><mask> or<mask><mask><mask><mask><mask>',\n",
       "  'iter0_best_text': '\", well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many of the parties or candidates in the party.',\n",
       "  'iter0_update': True},\n",
       " {'iter0_original_sentence': ' racist, you are wrong.\"\\n\\nWollmann criticized Durden for not paying attention when he came up with his family joke.',\n",
       "  'iter0_masked_sentence': '<mask><mask> you are wrong.\"<mask><mask><mask><mask><mask><mask> Durden for not paying attention when he came up with his<mask><mask><mask>',\n",
       "  'iter0_best_text': '\", you are wrong.\" He was, he, James Durden for not paying attention when he came up with his own name.',\n",
       "  'iter0_update': True},\n",
       " {'iter0_original_sentence': ' Nazi or I hate you because of your race, you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for Jews’',\n",
       "  'iter0_masked_sentence': '<mask> or I<mask> you<mask> of your<mask><mask> you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for<mask><mask><mask>',\n",
       "  'iter0_best_text': '… or I call you out of your privilege, you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for our culture.',\n",
       "  'iter0_update': True},\n",
       " {'iter0_original_sentence': \" bigot’, then you should take your hate test. Because here's a little fact.\",\n",
       "  'iter0_masked_sentence': \"<mask><mask><mask><mask> then you should take<mask><mask><mask><mask> Because here's a little fact.\",\n",
       "  'iter0_best_text': \"It's not, then you should take a step back. Because here's a little fact.\",\n",
       "  'iter0_update': True}]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(1, device='cuda:7')\n",
      "1\n",
      "tensor(2, device='cuda:7')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## intermediate output for debugging\n",
    "# for sample_ix in edit_yn.nonzero().squeeze(-1).tolist(): # edit 대상인 것들만 update.\n",
    "\n",
    "for sample_ix in range(len(running_text)): # edit 대상인 것들만 update.\n",
    "    print(sample_ix)\n",
    "    print(edit_ixes[sample_ix])\n",
    "    int_output[edit_ixes[sample_ix]].update({f\"iter{_iter}_original_sentence\": running_text[sample_ix],\n",
    "                                            f\"iter{_iter}_masked_sentence\": masked_text[sample_ix],\n",
    "                                            f\"iter{_iter}_best_text\": final_hypotheses[edit_ixes[sample_ix]],\n",
    "                                            f\"iter{_iter}_update\": update[edit_ixes[sample_ix]].item()})    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'iter0_original_sentence': \" homophobe, you've got to prove it to everybody in the media.\",\n",
       "  'iter0_masked_sentence': \"<mask><mask><mask><mask> you've got to prove it to everybody in the media.\",\n",
       "  'iter0_best_text': '\", I think you\\'ve got to prove it to everybody in the media.',\n",
       "  'iter0_update': True},\n",
       " {'iter0_original_sentence': \" sexist ’don't treat me like I'm a bitch.\",\n",
       "  'iter0_masked_sentence': \" sexist<mask><mask><mask><mask><mask> me like I'm a<mask><mask>\",\n",
       "  'iter0_best_text': \" sexist, I want to see me like I'm a man.\",\n",
       "  'iter0_update': True,\n",
       "  'iter1_original_sentence': \" sexist, I want to see me like I'm a man.\",\n",
       "  'iter1_masked_sentence': \"<mask><mask> I want to see me<mask> I'm a<mask><mask>\",\n",
       "  'iter1_best_text': '\". I want to see me. I\\'m a man.',\n",
       "  'iter1_update': True},\n",
       " {'iter0_original_sentence': \" racist, or a pro-whistleblower or a conspiracy theorist or says you're in it\",\n",
       "  'iter0_masked_sentence': \" racist, or a<mask><mask><mask><mask><mask><mask> or a<mask> theorist or says you're in it\",\n",
       "  'iter0_best_text': \" racist, or a sexist, or a racist, or a conspiracy theorist or says you're in it\",\n",
       "  'iter0_update': False,\n",
       "  'iter1_original_sentence': \" racist, or a sexist, or a racist, or a conspiracy theorist or says you're in it\",\n",
       "  'iter1_masked_sentence': \"<mask><mask> or a<mask><mask> or a<mask><mask> or a conspiracy theorist or says you're in it\",\n",
       "  'iter1_best_text': \"a, or a conspiracy theorist or a conspiracy theorist or a conspiracy theorist or says you're in it\",\n",
       "  'iter1_update': True},\n",
       " {'iter0_original_sentence': ' racist, well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many \"racist\" or \"sexist\" Republicans',\n",
       "  'iter0_masked_sentence': '<mask><mask> well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many<mask><mask><mask> or<mask><mask><mask><mask><mask>',\n",
       "  'iter0_best_text': '\", well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many of the parties or candidates in the party.',\n",
       "  'iter0_update': True},\n",
       " {'iter0_original_sentence': ' racist, you are wrong.\"\\n\\nWollmann criticized Durden for not paying attention when he came up with his family joke.',\n",
       "  'iter0_masked_sentence': '<mask><mask> you are wrong.\"<mask><mask><mask><mask><mask><mask> Durden for not paying attention when he came up with his<mask><mask><mask>',\n",
       "  'iter0_best_text': '\", you are wrong.\" He was, he, James Durden for not paying attention when he came up with his own name.',\n",
       "  'iter0_update': True},\n",
       " {'iter0_original_sentence': ' Nazi or I hate you because of your race, you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for Jews’',\n",
       "  'iter0_masked_sentence': '<mask> or I<mask> you<mask> of your<mask><mask> you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for<mask><mask><mask>',\n",
       "  'iter0_best_text': '… or I call you out of your privilege, you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for our culture.',\n",
       "  'iter0_update': True},\n",
       " {'iter0_original_sentence': \" bigot’, then you should take your hate test. Because here's a little fact.\",\n",
       "  'iter0_masked_sentence': \"<mask><mask><mask><mask> then you should take<mask><mask><mask><mask> Because here's a little fact.\",\n",
       "  'iter0_best_text': \"It's not, then you should take a step back. Because here's a little fact.\",\n",
       "  'iter0_update': True}]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update.nonzero().squeeze(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# update running_text, best_text, best_allsat, best_losses, best_weighted_loss\n",
    "for update_index in update.nonzero().squeeze(-1).tolist():\n",
    "    print(update_index)\n",
    "    best_text[update_index] = final_hypotheses[update_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\", I think you\\'ve got to prove it to everybody in the media.',\n",
       " '\". I want to see me. I\\'m a man.',\n",
       " \"a, or a conspiracy theorist or a conspiracy theorist or a conspiracy theorist or says you're in it\",\n",
       " '\", well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many of the parties or candidates in the party.',\n",
       " '\", you are wrong.\" He was, he, James Durden for not paying attention when he came up with his own name.',\n",
       " '… or I call you out of your privilege, you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for our culture.',\n",
       " \"It's not, then you should take a step back. Because here's a little fact.\"]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False,  True,  True,  True,  True], device='cuda:7')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_allsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_allsat[update] = new_best_allsat[update]\n",
    "best_losses[update] = new_best_logging_loss[update]\n",
    "best_weighted_loss[update] = new_best_weighted_loss[update]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True], device='cuda:7')"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_allsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[      inf,       inf],\n",
       "        [   48.006,     0.063],\n",
       "        [   63.939,     0.048],\n",
       "        [      inf,       inf],\n",
       "        [      inf,       inf],\n",
       "        [      inf,       inf],\n",
       "        [      inf,       inf]], device='cuda:7')"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_best_logging_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   54.399,     0.008],\n",
       "        [   48.006,     0.063],\n",
       "        [   63.939,     0.048],\n",
       "        [  107.677,     0.015],\n",
       "        [  111.426,     0.026],\n",
       "        [  108.389,     0.104],\n",
       "        [   66.649,     0.022]], device='cuda:7')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.447,  4.857,  6.437, 10.781, 11.166, 10.932,  6.684],\n",
       "       device='cuda:7')"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.447,  4.857,  6.437, 10.781, 11.166, 10.932,  6.684],\n",
       "       device='cuda:7')"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "es_patience_count[(best_allsat & edit_yn).nonzero().squeeze(-1)] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:7')"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_patience_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:7')"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_patience_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (config[\"early_stopping_patience\"] != -1):\n",
    "    edit_yn[es_patience_count > config['early_stopping_patience']] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False], device='cuda:7')"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_yn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False], device='cuda:7')"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_yn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "running_text = [x for i, x in enumerate(final_hypotheses) if edit_yn[i]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = {\n",
    "            \"prompt\": {\n",
    "                \"text\": source_text,\n",
    "            },\n",
    "            \"generations\": [\n",
    "                {\n",
    "                    \"text\": best_text[i],\n",
    "                    \"original_text\": AR_prediction_all[i],\n",
    "                    \"allsat\": best_allsat[i].item(),\n",
    "                    \"losses\": best_losses[i,:].tolist(),\n",
    "                    \"weighted_loss\": best_weighted_loss[i].item(),\n",
    "                    \"edited\": edited_at_all_yn[i].tolist(),\n",
    "                } for i in range(len(AR_prediction_all))\n",
    "            ],\n",
    "        }\n",
    "    \n",
    "intermediate_output = {\n",
    "        \"prompt\": {\n",
    "            \"text\": source_text,\n",
    "        },\n",
    "        \"generations\": \n",
    "            int_output\n",
    "        ,\n",
    "    }\n",
    "\n",
    "outputs.append(output)\n",
    "int_outputs.append(intermediate_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': {'text': 'And if you’re going to be shying away because someone calls you a bigot or a'},\n",
       "  'generations': [{'text': '\", I think you\\'ve got to prove it to everybody in the media.',\n",
       "    'original_text': \" homophobe, you've got to prove it to everybody in the media.\",\n",
       "    'allsat': True,\n",
       "    'losses': [54.398582458496094, 0.007761795073747635],\n",
       "    'weighted_loss': 5.446844100952148,\n",
       "    'edited': True},\n",
       "   {'text': '\". I want to see me. I\\'m a man.',\n",
       "    'original_text': \" sexist ’don't treat me like I'm a bitch.\",\n",
       "    'allsat': True,\n",
       "    'losses': [48.0064697265625, 0.06295201927423477],\n",
       "    'weighted_loss': 4.857304096221924,\n",
       "    'edited': True},\n",
       "   {'text': \"a, or a conspiracy theorist or a conspiracy theorist or a conspiracy theorist or says you're in it\",\n",
       "    'original_text': \" racist, or a pro-whistleblower or a conspiracy theorist or says you're in it\",\n",
       "    'allsat': True,\n",
       "    'losses': [63.938995361328125, 0.0475926548242569],\n",
       "    'weighted_loss': 6.436732769012451,\n",
       "    'edited': True},\n",
       "   {'text': '\", well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many of the parties or candidates in the party.',\n",
       "    'original_text': ' racist, well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many \"racist\" or \"sexist\" Republicans',\n",
       "    'allsat': True,\n",
       "    'losses': [107.67715454101562, 0.015053833834826946],\n",
       "    'weighted_loss': 10.781264305114746,\n",
       "    'edited': True},\n",
       "   {'text': '\", you are wrong.\" He was, he, James Durden for not paying attention when he came up with his own name.',\n",
       "    'original_text': ' racist, you are wrong.\"\\n\\nWollmann criticized Durden for not paying attention when he came up with his family joke.',\n",
       "    'allsat': True,\n",
       "    'losses': [111.42560577392578, 0.02626398578286171],\n",
       "    'weighted_loss': 11.16619873046875,\n",
       "    'edited': True},\n",
       "   {'text': '… or I call you out of your privilege, you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for our culture.',\n",
       "    'original_text': ' Nazi or I hate you because of your race, you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for Jews’',\n",
       "    'allsat': True,\n",
       "    'losses': [108.38932800292969, 0.1037822887301445],\n",
       "    'weighted_loss': 10.932336807250977,\n",
       "    'edited': True},\n",
       "   {'text': \"It's not, then you should take a step back. Because here's a little fact.\",\n",
       "    'original_text': \" bigot’, then you should take your hate test. Because here's a little fact.\",\n",
       "    'allsat': True,\n",
       "    'losses': [66.64909362792969, 0.02162473276257515],\n",
       "    'weighted_loss': 6.684371471405029,\n",
       "    'edited': True}]}]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_old=[]\n",
    "int_outputs_old=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/generation_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "os.chdir('/data/hyeryung/mucoco')\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "import new_module.losses_old as lossbuilder\n",
    "import wandb\n",
    "from new_module.decode_utils import (\n",
    "    beam_rerank_v0,\n",
    "    beam_rerank_v1,\n",
    "    beam_rerank_v2,\n",
    "    combi_rerank,\n",
    ")\n",
    "from new_module.evaluation.evaluate_wandb import evaluate_main\n",
    "from new_module.locate.locate_utils import locate_main\n",
    "from new_module.utils.robertacustom import RobertaCustomForSequenceClassification\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(os.environ.get(\"LOGGING_LEVEL\", logging.DEBUG))\n",
    "\n",
    "import joblib\n",
    "config = joblib.load('config.pkl')\n",
    "config['device'] = 'cuda:7'\n",
    "\n",
    "class dummyArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "build_loss_args = dummyArgs(**config[\"build_loss_dict\"])\n",
    "\n",
    "## load data\n",
    "if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "    source_dataset = [\n",
    "        json.loads(l)[config[\"jsonl_primary_key\"]][config[\"jsonl_secondary_key\"]]\n",
    "        for l in open(config[\"source_data\"])\n",
    "    ]\n",
    "    generation_dataset = [\n",
    "        json.loads(l)[\"generations\"] for l in open(config[\"source_data\"])\n",
    "    ]\n",
    "elif (config[\"task\"] == \"formality\") or (config[\"task\"] == \"sentiment-lewis-compr\"):\n",
    "    with open(config[\"source_data\"], \"r\") as f:\n",
    "        generation_dataset = [line.rstrip('\\n') for line in f.readlines()]\n",
    "    source_dataset = [\"\" for l in generation_dataset]\n",
    "\n",
    "\n",
    "## load tokenizer, models, define losses\n",
    "name2tokenizer = {}\n",
    "name2model = {}\n",
    "name2config = {}\n",
    "loss2tokenizer = {}\n",
    "embed_luts = []\n",
    "\n",
    "for i, model_path in enumerate(config[\"model_paths\"]):\n",
    "    if (\n",
    "        model_path not in name2model\n",
    "    ):  # making sure we are not loading the model twice in case some constraints use the same model.\n",
    "        try:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=True,\n",
    "            )\n",
    "        except:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=False,\n",
    "            )\n",
    "\n",
    "        name2config[model_path] = AutoConfig.from_pretrained(\n",
    "            model_path, cache_dir=config[\"cache_dir\"]\n",
    "        )\n",
    "\n",
    "        if config[\"model_types\"][i] == \"RobertaCustomForSequenceClassification\":\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                RobertaCustomForSequenceClassification.from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(transformers, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        name2model[model_path].eval()\n",
    "        name2model[model_path].to(config['device'])\n",
    "\n",
    "    input_embeds = name2model[model_path].get_input_embeddings()\n",
    "    if isinstance(input_embeds, torch.nn.Sequential):\n",
    "        input_embeds = input_embeds[0]\n",
    "    embed_luts.append(input_embeds)\n",
    "\n",
    "    if config[\"target_type\"] == \"embeds\":\n",
    "        embed_luts[-1].requires_grad = False\n",
    "\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "mlm = None if config[\"method\"] == \"mlm-beamsearch-v2\" else AutoModelForMaskedLM.from_pretrained(\"roberta-base\")  \n",
    "\n",
    "lossfns = []\n",
    "for i, loss in enumerate(config[\"losses\"]):\n",
    "    lossfns.append(\n",
    "        lossbuilder.build_loss(\n",
    "            loss,\n",
    "            name2model[config[\"model_paths\"][i]],\n",
    "            name2tokenizer[config[\"model_paths\"][i]],\n",
    "            build_loss_args,\n",
    "        )\n",
    "    )\n",
    "    lossfns[i].tokenizer.add_special_tokens({\"mask_token\": mlm_tokenizer.mask_token})\n",
    "    loss2tokenizer[loss] = lossfns[i].tokenizer\n",
    "# lossfns[0].tokenizer = loss2tokenizer[config[\"losses\"][0]]\n",
    "# lossfns[1].tokenizer = loss2tokenizer[config[\"losses\"][1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def beam_rerank_v0(source_text, ## text (too arbitrary?)\n",
    "                    masked_sequence, ## in mlm tokenizer's tokens\n",
    "                    indices_in_mlm_tokens,\n",
    "                    predicted_token_ids,\n",
    "                    mlm_tokenizer, \n",
    "                    lossfns,\n",
    "                    config, \n",
    "                    beam_size = 5):\n",
    "    \n",
    "    hypotheses = [torch.LongTensor([]).to(config['device'])]\n",
    "    L = masked_sequence.size(-1)\n",
    "\n",
    "    for i in range(L):\n",
    "        if masked_sequence[0, i] != mlm_tokenizer.mask_token_id:\n",
    "            hypotheses = list(torch.cat([torch.stack(hypotheses,dim=0), \n",
    "                                        masked_sequence[:, i].unsqueeze(0).repeat((len(hypotheses),1)).to(config['device'])],dim=-1))\n",
    "        else:\n",
    "            num_hypotheses = len(hypotheses)\n",
    "            hypotheses = torch.stack(hypotheses,dim=0).unsqueeze(0)\n",
    "            hypotheses = hypotheses.repeat(config['k_per_location'], 1, 1)\n",
    "            candidates = predicted_token_ids.indices[torch.where(indices_in_mlm_tokens == i)[0], :].to(config['device']).T.unsqueeze(1)\n",
    "            candidates = candidates.repeat(1, num_hypotheses, 1)\n",
    "            hypotheses_exp = torch.cat([hypotheses, candidates], dim=-1)\n",
    "            hypotheses_exp = hypotheses_exp.view(-1, hypotheses_exp.shape[-1])\n",
    "            hypotheses_exp = list(hypotheses_exp)\n",
    "\n",
    "            losses = []\n",
    "            loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "            for hyp in hypotheses_exp:\n",
    "                curr_loss = 0.0\n",
    "                for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "                    with torch.no_grad():\n",
    "                        lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                            source_text, mlm_tokenizer.decode(hyp, skip_special_tokens=True),\n",
    "                            label_id=config['target_label_ids'][lossid],\n",
    "                        )\n",
    "                    curr_loss += loss_weights[lossid] * lossvalue.item()\n",
    "                losses.append(curr_loss)\n",
    "\n",
    "            hypotheses = sorted(zip(hypotheses_exp, losses), key=lambda x: x[1])[:beam_size]\n",
    "            hypotheses = [x[0] for x in hypotheses]\n",
    "            \n",
    "    return [mlm_tokenizer.decode(x, skip_special_tokens=True) for x in hypotheses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "text_id 34 sample_id 0 \n",
      "[prompt] And if you’re going to be shying away because someone calls you a bigot or a [text]  homophobe, you've got to prove it to everybody in the media.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter 0, sample_idx: 0\n",
      "locate result: [\"<mask><mask><mask><mask> you've got to prove it to everybody in the media.\"]\n",
      "iter 0. Update best prediction\n",
      "best_text: \", I think you've got to prove it to everybody in the media.\n",
      "early stopping at iter 0\n",
      "text_id 34 sample_id 1 \n",
      "[prompt] And if you’re going to be shying away because someone calls you a bigot or a [text]  sexist ’don't treat me like I'm a bitch.\n",
      "iter 0, sample_idx: 1\n",
      "locate result: [\" sexist<mask><mask><mask><mask><mask> me like I'm a<mask><mask>\"]\n",
      "iter 0. Update best prediction\n",
      "best_text:  sexist, I want to see me like I'm a man.\n",
      "iter 1, sample_idx: 1\n",
      "locate result: [\"<mask><mask> I want to see me<mask> I'm a<mask><mask>\"]\n",
      "iter 1. Update best prediction\n",
      "best_text: \". I want to see me. I'm a man.\n",
      "early stopping at iter 1\n",
      "text_id 34 sample_id 2 \n",
      "[prompt] And if you’re going to be shying away because someone calls you a bigot or a [text]  racist, or a pro-whistleblower or a conspiracy theorist or says you're in it\n",
      "iter 0, sample_idx: 2\n",
      "locate result: [\" racist, or a<mask><mask><mask><mask><mask><mask> or a<mask> theorist or says you're in it\"]\n",
      "iter 1, sample_idx: 2\n",
      "locate result: [\"<mask><mask> or a<mask><mask> or a<mask><mask> or a conspiracy theorist or says you're in it\"]\n",
      "iter 1. Update best prediction\n",
      "best_text: a, or a conspiracy theorist or a conspiracy theorist or a conspiracy theorist or says you're in it\n",
      "early stopping at iter 1\n",
      "text_id 34 sample_id 3 \n",
      "[prompt] And if you’re going to be shying away because someone calls you a bigot or a [text]  racist, well you won't get any seats in this party.\"\n",
      "\n",
      "But, Burgess said, there aren't many \"racist\" or \"sexist\" Republicans\n",
      "iter 0, sample_idx: 3\n",
      "locate result: ['<mask><mask> well you won\\'t get any seats in this party.\"\\n\\nBut, Burgess said, there aren\\'t many<mask><mask><mask> or<mask><mask><mask><mask><mask>']\n",
      "iter 0. Update best prediction\n",
      "best_text: \", well you won't get any seats in this party.\"\n",
      "\n",
      "But, Burgess said, there aren't many of the parties or candidates in the party.\n",
      "early stopping at iter 0\n",
      "text_id 34 sample_id 4 \n",
      "[prompt] And if you’re going to be shying away because someone calls you a bigot or a [text]  racist, you are wrong.\"\n",
      "\n",
      "Wollmann criticized Durden for not paying attention when he came up with his family joke.\n",
      "iter 0, sample_idx: 4\n",
      "locate result: ['<mask><mask> you are wrong.\"<mask><mask><mask><mask><mask><mask> Durden for not paying attention when he came up with his<mask><mask><mask>']\n",
      "iter 0. Update best prediction\n",
      "best_text: \", you are wrong.\" He was, he, James Durden for not paying attention when he came up with his own name.\n",
      "early stopping at iter 0\n",
      "text_id 34 sample_id 5 \n",
      "[prompt] And if you’re going to be shying away because someone calls you a bigot or a [text]  Nazi or I hate you because of your race, you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for Jews’\n",
      "iter 0, sample_idx: 5\n",
      "locate result: ['<mask> or I<mask> you<mask> of your<mask><mask> you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for<mask><mask><mask>']\n",
      "iter 0. Update best prediction\n",
      "best_text: … or I call you out of your privilege, you are not showing the proper respect for ourselves as a people of all backgrounds, and you are not showing the respect for our culture.\n",
      "early stopping at iter 0\n",
      "text_id 34 sample_id 6 \n",
      "[prompt] And if you’re going to be shying away because someone calls you a bigot or a [text]  bigot’, then you should take your hate test. Because here's a little fact.\n",
      "iter 0, sample_idx: 6\n",
      "locate result: [\"<mask><mask><mask><mask> then you should take<mask><mask><mask><mask> Because here's a little fact.\"]\n",
      "iter 0. Update best prediction\n",
      "best_text: It's not, then you should take a step back. Because here's a little fact.\n",
      "early stopping at iter 0\n"
     ]
    }
   ],
   "source": [
    "resume_idx = 34\n",
    "label_ids = config[\"target_label_ids\"]  # target label's ids for each loss\n",
    "\n",
    "## beginning of main logic\n",
    "# text_id = 0\n",
    "\n",
    "interrupted = False\n",
    "for text_id in range(len(source_dataset))[resume_idx:]:\n",
    "    source_text = source_dataset[text_id]\n",
    "    if source_text == \"\":\n",
    "        source_text = lossfns[0].tokenizer.bos_token\n",
    "\n",
    "    if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "        AR_prediction_all = [x[\"text\"] for x in generation_dataset[text_id]]\n",
    "        # predicted_batches = [x[\"tokens\"] for x in generation_dataset[text_id]]\n",
    "        # predicted_batches = [\n",
    "        #     torch.tensor([x], dtype=torch.long, device=config[\"device\"])\n",
    "        #     for x in predicted_batches\n",
    "        # ]\n",
    "        \n",
    "    elif (config[\"task\"] == \"formality\") or (\n",
    "        config[\"task\"] == \"sentiment-lewis-compr\"\n",
    "    ):\n",
    "        AR_prediction_all = [generation_dataset[text_id]]\n",
    "\n",
    "    sample_idx = 0\n",
    "    curr_num_samples = len(AR_prediction_all)\n",
    "    # for sample_idx in range(config[\"num_samples\"])[:]:\n",
    "    for sample_idx in range(curr_num_samples): ## updated (3/15)\n",
    "        \n",
    "        ## commented out (3/15) : dev set doesn't have the space problem.\n",
    "        # if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "        #     predicted_batch = predicted_batches[sample_idx].cuda()\n",
    "        #     AR_prediction = lossfns[0].tokenizer.batch_decode(predicted_batch)[0]\n",
    "        # else:\n",
    "        AR_prediction = AR_prediction_all[sample_idx]\n",
    "\n",
    "        logger.debug(\n",
    "            f\"text_id {text_id} sample_id {sample_idx} \\n[prompt] {source_text} [text] {AR_prediction}\"\n",
    "        )\n",
    "\n",
    "        # --------------------------------------------------------------------------------------------- #\n",
    "        ## check whether initial text satisfies constraint\n",
    "        allsat = True\n",
    "        gold_losses = []\n",
    "        curr_loss = 0.0\n",
    "        loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "        for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "            with torch.no_grad():\n",
    "                lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                    source_text, AR_prediction,\n",
    "                    label_id=label_ids[lossid],\n",
    "                )\n",
    "                \n",
    "            gold_losses.append(lossvalue.squeeze().item())\n",
    "            curr_loss += loss_weights[lossid] * lossvalue.squeeze().item()\n",
    "            if (lossid >= 1) and (gold_losses[lossid] > -np.log(\n",
    "                config[\"min_epsilons\"][lossid - 1]\n",
    "            )):\n",
    "                allsat = False\n",
    "\n",
    "        if (allsat) and (not config[\"dont_skip_allsat\"]):\n",
    "            logger.info(\n",
    "                f\"skipping this sample since it already satisfies constraint. {gold_losses}\"\n",
    "            )\n",
    "            if sample_idx == 0:\n",
    "                output = {\n",
    "                    \"prompt\": {\n",
    "                        \"text\": source_text,\n",
    "                    },\n",
    "                    \"generations\": [\n",
    "                        {\n",
    "                            \"text\": AR_prediction,\n",
    "                            \"indices\": [[]],\n",
    "                            \"allsat\": allsat,\n",
    "                            \"losses\": gold_losses,\n",
    "                            \"weighted_loss\": curr_loss,\n",
    "                            \"edited\": False,\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "                intermediate_output = {\n",
    "                    \"prompt\": {\n",
    "                        \"text\": source_text,\n",
    "                    },\n",
    "                    \"generations\": [\n",
    "                        {}\n",
    "                    ],\n",
    "                }\n",
    "            else:\n",
    "                output[\"generations\"].append(\n",
    "                    {\n",
    "                        \"text\": AR_prediction,\n",
    "                        \"indices\": [[]],\n",
    "                        \"allsat\": allsat,\n",
    "                        \"losses\": gold_losses,\n",
    "                        \"weighted_loss\": curr_loss,\n",
    "                        \"edited\": False,\n",
    "                    }       \n",
    "                )\n",
    "                intermediate_output['generations'].append({})\n",
    "\n",
    "            # if sample_idx + 1 == config[\"num_samples\"]:\n",
    "            if sample_idx + 1 == curr_num_samples:\n",
    "                outputs_old.append(output)\n",
    "                int_outputs_old.append(intermediate_output)\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            es_patience_count = 0\n",
    "            best_ix = None\n",
    "            best_allsat = allsat\n",
    "            best_losses = gold_losses\n",
    "            best_weighted_loss = curr_loss                \n",
    "            running_text = best_text = AR_prediction\n",
    "            int_output = {}\n",
    "\n",
    "            _iter = 0\n",
    "            for _iter in range(config['n_iter']):\n",
    "                ## locate tokens to edit\n",
    "                masked_text  = locate_main(running_text, \n",
    "                                        config[\"locate_method\"], \n",
    "                                        name2model[config[\"model_paths\"][1]], \n",
    "                                        name2tokenizer[config[\"tokenizer_paths\"][1]], \n",
    "                                        max_num_tokens = config['num_edit_token_per_step'], \n",
    "                                        unit=config[\"locate_unit\"], \n",
    "                                        device=config['device'], \n",
    "                                        label_id=config[\"target_label_ids\"][1],\n",
    "                                        num_layer=10)\n",
    "                logger.debug(f\"iter {_iter}, sample_idx: {sample_idx}\")\n",
    "                logger.debug(f\"locate result: {masked_text}\")\n",
    "                \n",
    "                if config[\"method\"] == \"mlm-beamsearch-v2\":\n",
    "                    pass\n",
    "                else:\n",
    "                    ## replace tokens at the indices with mask tokens\n",
    "                    inputs = mlm_tokenizer(\n",
    "                        masked_text, return_tensors=\"pt\"\n",
    "                    )\n",
    "                    # inputs = mlm_tokenizer(\n",
    "                    #     source_text + ' ' + masked_text[0], return_tensors=\"pt\", add_special_tokens=False\n",
    "                    # )\n",
    "                    \n",
    "                    ## make predictions for the masked indices\n",
    "                    with torch.no_grad():\n",
    "                        logits = mlm(**inputs).logits\n",
    "                    indices_in_mlm_tokens = (\n",
    "                        inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "                    )[0].nonzero(as_tuple=True)[0]\n",
    "                    # print(f\"indices_in_mlm_tokens: {indices_in_mlm_tokens}\")\n",
    "                    ## get top k tokens for each index\n",
    "                    \n",
    "                    ## make logits for special tokens -inf.\n",
    "                    special_token_ids = mlm_tokenizer.convert_tokens_to_ids(mlm_tokenizer.all_special_tokens)\n",
    "                    logits[:, :, special_token_ids] = -np.inf\n",
    "                    \n",
    "                    predicted_token_ids = torch.topk(\n",
    "                        logits[0, indices_in_mlm_tokens],\n",
    "                        k=config['k_per_location'],\n",
    "                        dim=-1,\n",
    "                    )\n",
    "                    # print(f\"predicted_token_ids: {predicted_token_ids}\")\n",
    "                    # print(f\"mlm_tokenizer.batch_decode(predicted_token_ids.indices): {mlm_tokenizer.batch_decode(predicted_token_ids.indices)}\")\n",
    "                    \n",
    "                if config[\"method\"] == \"mlm-beamsearch-v0\":\n",
    "                    # print(config[\"method\"])\n",
    "                    hypotheses = beam_rerank_v0(source_text,\n",
    "                                                inputs.input_ids,\n",
    "                                                indices_in_mlm_tokens,\n",
    "                                                predicted_token_ids,\n",
    "                                                mlm_tokenizer, \n",
    "                                                lossfns,\n",
    "                                                config, \n",
    "                                                beam_size = config['beam_size'])\n",
    "                elif config[\"method\"] == \"mlm-beamsearch-v1\":\n",
    "                    hypotheses = beam_rerank_v1(source_text,\n",
    "                                                inputs.input_ids,\n",
    "                                                indices_in_mlm_tokens,\n",
    "                                                predicted_token_ids,\n",
    "                                                mlm_tokenizer, \n",
    "                                                lossfns,\n",
    "                                                config, \n",
    "                                                beam_size = config['beam_size'])\n",
    "                elif config[\"method\"] == \"mlm-beamsearch-v2\":\n",
    "                    source_batch = lossfns[0].tokenizer(source_text, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(config['device'])\n",
    "                    masked_sequence = lossfns[0].tokenizer(masked_text, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(config['device'])\n",
    "                    hypotheses = beam_rerank_v2(\n",
    "                        source_batch,\n",
    "                        masked_sequence,\n",
    "                        lossfns[0].model,\n",
    "                        lossfns[0].tokenizer,\n",
    "                        config,\n",
    "                        beam_size=config['beam_size'],\n",
    "                    )\n",
    "                elif config[\"method\"] == \"mlm-reranking\":\n",
    "                    hypotheses = combi_rerank(inputs.input_ids, ## in mlm tokenizer's tokens\n",
    "                        indices_in_mlm_tokens,\n",
    "                        predicted_token_ids,\n",
    "                        mlm_tokenizer,\n",
    "                        config)\n",
    "\n",
    "                candidate_total_losses = []\n",
    "                candidate_primary_losses = []\n",
    "                candidate_losses_for_loggings = []\n",
    "                candidate_allsats = []\n",
    "                \n",
    "                for hyp in hypotheses:\n",
    "                    curr_loss = 0.0\n",
    "                    logging_loss = []\n",
    "                    allsat = True\n",
    "                    for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "                        with torch.no_grad():\n",
    "                            lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                                source_text, hyp,\n",
    "                                label_id=config['target_label_ids'][lossid],\n",
    "                            )\n",
    "                        curr_loss += loss_weights[lossid] * lossvalue.item()\n",
    "                        logging_loss.append(lossvalue.item())\n",
    "                        if lossid==0:\n",
    "                            candidate_primary_losses.append(lossvalue.item())\n",
    "                        elif (lossid >= 1) and (\n",
    "                            lossvalue.item()\n",
    "                            > -np.log(config[\"min_epsilons\"][lossid - 1])\n",
    "                        ):\n",
    "                            allsat = False\n",
    "                    candidate_total_losses.append(curr_loss)\n",
    "                    candidate_losses_for_loggings.append(logging_loss)\n",
    "                    candidate_allsats.append(allsat)\n",
    "\n",
    "\n",
    "                if config['selection_criteria'] == \"weighted_sum\":\n",
    "                    best_ix = np.argmin(np.array(candidate_total_losses))\n",
    "                elif config['selection_criteria'] == \"allsat_primary\":\n",
    "                    allsat_ix = np.where(np.array(candidate_allsats) == True)[0]\n",
    "                    if len(allsat_ix) > 0:\n",
    "                        best_ix = np.argmin(\n",
    "                            np.array(candidate_primary_losses)[allsat_ix]\n",
    "                        )  # select min primary loss among allsats\n",
    "                        best_ix = allsat_ix[best_ix]\n",
    "                    else:  # if no candidate satisfying constraints, default to weighted_sum\n",
    "                        best_ix = np.argmin(np.array(candidate_total_losses))\n",
    "                    \n",
    "                update = False\n",
    "                if config['selection_criteria'] == \"weighted_sum\":\n",
    "                    if best_weighted_loss > candidate_total_losses[best_ix]:\n",
    "                        update = True\n",
    "                elif config['selection_criteria'] == \"allsat_primary\":\n",
    "                    if (\n",
    "                        best_allsat is False\n",
    "                        and candidate_allsats[best_ix] is True\n",
    "                    ):\n",
    "                        update = True\n",
    "                    elif (\n",
    "                        best_allsat is False\n",
    "                        and candidate_allsats[best_ix] is False\n",
    "                    ):\n",
    "                        if best_weighted_loss > candidate_total_losses[best_ix]:\n",
    "                            update = True\n",
    "                    elif (\n",
    "                        best_allsat is True\n",
    "                        and candidate_allsats[best_ix] is True\n",
    "                    ):\n",
    "                        if (\n",
    "                            best_losses[0]\n",
    "                            > candidate_losses_for_loggings[best_ix][0]\n",
    "                        ):\n",
    "                            update = True\n",
    "\n",
    "\n",
    "                ## intermediate output for debugging\n",
    "                int_output.update({f\"iter{_iter}_original_sentence\": running_text,\n",
    "                                f\"iter{_iter}_masked_sentence\": masked_text,\n",
    "                                f\"iter{_iter}_best_text\": hypotheses[best_ix],\n",
    "                                f\"iter{_iter}_update\": update})    \n",
    "                \n",
    "                running_text = hypotheses[best_ix]\n",
    "                if update:\n",
    "                    ## save the best prediction in a format compatible with mucola outputs\n",
    "                    best_text = hypotheses[best_ix]\n",
    "                    best_allsat = candidate_allsats[best_ix]\n",
    "                    best_losses = candidate_losses_for_loggings[best_ix]\n",
    "                    best_weighted_loss = candidate_total_losses[best_ix]\n",
    "\n",
    "                    logger.debug(f\"iter {_iter}. Update best prediction\")\n",
    "                    logger.debug(f\"best_text: {best_text}\")\n",
    "                \n",
    "                if best_allsat:\n",
    "                    es_patience_count += 1\n",
    "                    if (config[\"early_stopping_patience\"] != -1) and (es_patience_count > config[\"early_stopping_patience\"]):\n",
    "                        logger.info(f\"early stopping at iter {_iter}\")\n",
    "                        break\n",
    "\n",
    "            if sample_idx == 0:\n",
    "                output = {\n",
    "                    \"prompt\": {\n",
    "                        \"text\": source_text,\n",
    "                    },\n",
    "                    \"generations\": [\n",
    "                        {\n",
    "                            \"text\": best_text,\n",
    "                            \"original_text\": AR_prediction,\n",
    "                            \"allsat\": best_allsat,\n",
    "                            \"losses\": best_losses,\n",
    "                            \"weighted_loss\": best_weighted_loss,\n",
    "                            \"edited\": True,\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "                \n",
    "                intermediate_output = {\n",
    "                    \"prompt\": {\n",
    "                        \"text\": source_text,\n",
    "                    },\n",
    "                    \"generations\": [\n",
    "                        int_output\n",
    "                    ],\n",
    "                }\n",
    "            else:\n",
    "                output[\"generations\"].append(\n",
    "                    {\n",
    "                            \"text\": best_text,\n",
    "                            \"original_text\": AR_prediction,\n",
    "                            \"allsat\": best_allsat,\n",
    "                            \"losses\": best_losses,\n",
    "                            \"weighted_loss\": best_weighted_loss,\n",
    "                            \"edited\": True,\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                intermediate_output[\"generations\"].append(int_output)\n",
    "\n",
    "            # if sample_idx + 1 == config[\"num_samples\"]:\n",
    "            if sample_idx + 1 == curr_num_samples:\n",
    "                outputs_old.append(output)\n",
    "                int_outputs_old.append(intermediate_output)\n",
    "                break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_old=outputs_old[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_old[0]['generations'][0]['text'] == outputs[0]['generations'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_old[0]['generations'][1]['text'] == outputs[0]['generations'][1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_old[0]['generations'][2]['text'] == outputs[0]['generations'][2]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i in range(3,len(outputs_old[0]['generations'])):\n",
    "    print(outputs_old[0]['generations'][i]['text'] == outputs[0]['generations'][i]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iter0_original_sentence': ' wearing games and holy ****ing shit do I hate horse wearing games.',\n",
       " 'iter0_masked_sentence': [' wearing games and<mask> ****ing<mask> do I hate horse wearing games.'],\n",
       " 'iter0_best_text': ' wearing games and how ****ing much do I hate horse wearing games.',\n",
       " 'iter0_update': True,\n",
       " 'iter1_original_sentence': ' wearing games and how ****ing much do I hate horse wearing games.',\n",
       " 'iter1_masked_sentence': [' wearing games and how<mask><mask> much do I hate<mask> wearing games.'],\n",
       " 'iter1_best_text': ' wearing games and how and how much do I hate wearing wearing games.',\n",
       " 'iter1_update': True}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_outputs_old[0]['generations'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' wearing, and how, very much do I hate not wearing games.',\n",
       " 'original_text': ' wearing games and holy ****ing shit do I hate horse wearing games.',\n",
       " 'allsat': True,\n",
       " 'losses': [62.027462005615234, 0.02681632898747921],\n",
       " 'weighted_loss': 6.22688102722168,\n",
       " 'edited': True}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]['generations'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iter0_original_sentence': ' wearing games and holy ****ing shit do I hate horse wearing games.',\n",
       " 'iter0_masked_sentence': ' wearing games and<mask> ****ing<mask> do I hate horse wearing games.',\n",
       " 'iter0_best_text': ' wearing games and how ****ing much do I hate horse wearing games.',\n",
       " 'iter0_update': True,\n",
       " 'iter1_original_sentence': ' wearing games and how ****ing much do I hate horse wearing games.',\n",
       " 'iter1_masked_sentence': ' wearing<mask> and how<mask><mask> much do I hate<mask> wearing games.',\n",
       " 'iter1_best_text': ' wearing, and how, very much do I hate not wearing games.',\n",
       " 'iter1_update': False}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_outputs[0]['generations'][1] ## 2번째 iteration으로 넘어가면서 꼬인다. (1. best text 결과가 다르고 2. iter1_update: False 인데도 final best text에 업데이트가 되었다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/generation_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 47.54 GiB of which 4.94 MiB is free. Process 3095476 has 40.40 GiB memory in use. Including non-PyTorch memory, this process has 7.12 GiB memory in use. Of the allocated memory 6.70 GiB is allocated by PyTorch, and 164.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 71\u001b[0m\n\u001b[1;32m     63\u001b[0m         name2model[model_path] \u001b[38;5;241m=\u001b[39m lossbuilder\u001b[38;5;241m.\u001b[39mModelWrapper(\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;28mgetattr\u001b[39m(transformers, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_types\u001b[39m\u001b[38;5;124m\"\u001b[39m][i])\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     65\u001b[0m                 model_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m             )\n\u001b[1;32m     69\u001b[0m         )\n\u001b[1;32m     70\u001b[0m     name2model[model_path]\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 71\u001b[0m     \u001b[43mname2model\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m input_embeds \u001b[38;5;241m=\u001b[39m name2model[model_path]\u001b[38;5;241m.\u001b[39mget_input_embeddings()\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_embeds, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential):\n",
      "File \u001b[0;32m/data/hyeryung/.conda/envs/loc-edit/lib/python3.8/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/hyeryung/.conda/envs/loc-edit/lib/python3.8/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/data/hyeryung/.conda/envs/loc-edit/lib/python3.8/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m/data/hyeryung/.conda/envs/loc-edit/lib/python3.8/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/data/hyeryung/.conda/envs/loc-edit/lib/python3.8/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/data/hyeryung/.conda/envs/loc-edit/lib/python3.8/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 47.54 GiB of which 4.94 MiB is free. Process 3095476 has 40.40 GiB memory in use. Including non-PyTorch memory, this process has 7.12 GiB memory in use. Of the allocated memory 6.70 GiB is allocated by PyTorch, and 164.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "class dummyArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "build_loss_args = dummyArgs(**config[\"build_loss_dict\"])\n",
    "\n",
    "## load data\n",
    "if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "    source_dataset = [\n",
    "        json.loads(l)[config[\"jsonl_primary_key\"]][config[\"jsonl_secondary_key\"]]\n",
    "        for l in open(config[\"source_data\"])\n",
    "    ]\n",
    "    generation_dataset = [\n",
    "        json.loads(l)[\"generations\"] for l in open(config[\"source_data\"])\n",
    "    ]\n",
    "elif (config[\"task\"] == \"formality\") or (config[\"task\"] == \"sentiment-lewis-compr\"):\n",
    "    with open(config[\"source_data\"], \"r\") as f:\n",
    "        generation_dataset = [line.rstrip('\\n') for line in f.readlines()]\n",
    "    source_dataset = [\"\" for l in generation_dataset]\n",
    "\n",
    "\n",
    "## load tokenizer, models, define losses\n",
    "name2tokenizer = {}\n",
    "name2model = {}\n",
    "name2config = {}\n",
    "loss2tokenizer = {}\n",
    "embed_luts = []\n",
    "\n",
    "for i, model_path in enumerate(config[\"model_paths\"]):\n",
    "    if (\n",
    "        model_path not in name2model\n",
    "    ):  # making sure we are not loading the model twice in case some constraints use the same model.\n",
    "        try:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=True,\n",
    "            )\n",
    "        except:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=False,\n",
    "            )\n",
    "\n",
    "        name2config[model_path] = AutoConfig.from_pretrained(\n",
    "            model_path, cache_dir=config[\"cache_dir\"]\n",
    "        )\n",
    "\n",
    "        if config[\"model_types\"][i] == \"RobertaCustomForSequenceClassification\":\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                RobertaCustomForSequenceClassification.from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(transformers, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        name2model[model_path].eval()\n",
    "        name2model[model_path].cuda()\n",
    "\n",
    "    input_embeds = name2model[model_path].get_input_embeddings()\n",
    "    if isinstance(input_embeds, torch.nn.Sequential):\n",
    "        input_embeds = input_embeds[0]\n",
    "    embed_luts.append(input_embeds)\n",
    "\n",
    "    if config[\"target_type\"] == \"embeds\":\n",
    "        embed_luts[-1].requires_grad = False\n",
    "\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "mlm = None if config[\"method\"] == \"mlm-beamsearch-v2\" else AutoModelForMaskedLM.from_pretrained(\"roberta-base\")  \n",
    "\n",
    "lossfns_old = []\n",
    "for i, loss in enumerate(config[\"losses\"]):\n",
    "    lossfns_old.append(\n",
    "        lossbuilder.build_loss(\n",
    "            loss,\n",
    "            name2model[config[\"model_paths\"][i]],\n",
    "            name2tokenizer[config[\"model_paths\"][i]],\n",
    "            build_loss_args,\n",
    "        )\n",
    "    )\n",
    "    lossfns_old[i].tokenizer.add_special_tokens({\"mask_token\": mlm_tokenizer.mask_token})\n",
    "    loss2tokenizer[loss] = lossfns_old[i].tokenizer\n",
    "# lossfns_old[0].tokenizer = loss2tokenizer[config[\"losses\"][0]]\n",
    "# lossfns_old[1].tokenizer = loss2tokenizer[config[\"losses\"][1]]\n",
    "\n",
    "text_id=0\n",
    "source_text = source_dataset[text_id]\n",
    "if source_text == \"\":\n",
    "    source_text = lossfns_old[0].tokenizer.bos_token\n",
    "\n",
    "if (config[\"task\"] == \"toxicity\") or (config[\"task\"] == \"sentiment\"):\n",
    "    AR_prediction_all = [x[\"text\"] for x in generation_dataset[text_id]]    \n",
    "elif (config[\"task\"] == \"formality\") or (\n",
    "    config[\"task\"] == \"sentiment-lewis-compr\"\n",
    "):\n",
    "    AR_prediction_all = [generation_dataset[text_id]]\n",
    "\n",
    "sample_idx = 1\n",
    "curr_num_samples = len(AR_prediction_all)\n",
    "AR_prediction = AR_prediction_all[sample_idx]\n",
    "\n",
    "allsat = True\n",
    "gold_losses = []\n",
    "curr_loss = 0.0\n",
    "loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "    with torch.no_grad():\n",
    "        lossvalue = lossfns_old[lossid].compute_gold_loss(\n",
    "            source_text, AR_prediction,\n",
    "            label_id=label_ids[lossid],\n",
    "        )\n",
    "        \n",
    "    gold_losses.append(lossvalue.squeeze().item())\n",
    "    curr_loss += loss_weights[lossid] * lossvalue.squeeze().item()\n",
    "    if (lossid >= 1) and (gold_losses[lossid] > -np.log(\n",
    "        config[\"min_epsilons\"][lossid - 1]\n",
    "    )):\n",
    "        allsat = False\n",
    "\n",
    "es_patience_count_old = 0\n",
    "best_ix_old = None\n",
    "best_allsat_old = allsat\n",
    "best_losses_old = gold_losses\n",
    "best_weighted_loss_old = curr_loss                \n",
    "running_text_old = best_text_old = AR_prediction\n",
    "int_output_old = {}\n",
    "\n",
    "_iter = 0\n",
    "## locate tokens to edit\n",
    "masked_text_old  = locate_main(running_text_old, \n",
    "                        config[\"locate_method\"], \n",
    "                        name2model[config[\"model_paths\"][1]], \n",
    "                        name2tokenizer[config[\"tokenizer_paths\"][1]], \n",
    "                        max_num_tokens = config['num_edit_token_per_step'], \n",
    "                        unit=config[\"locate_unit\"], \n",
    "                        device=\"cuda\", \n",
    "                        label_id=config[\"target_label_ids\"][1],\n",
    "                        num_layer=10)\n",
    "\n",
    "## replace tokens at the indices with mask tokens\n",
    "inputs_old = mlm_tokenizer(\n",
    "    masked_text_old, return_tensors=\"pt\"\n",
    ")\n",
    "# inputs_old = mlm_tokenizer(\n",
    "#     source_text + ' ' + masked_text_old[0], return_tensors=\"pt\", add_special_tokens=False\n",
    "# )\n",
    "\n",
    "## make predictions for the masked indices\n",
    "with torch.no_grad():\n",
    "    logits_old = mlm(**inputs_old).logits\n",
    "indices_in_mlm_tokens_old = (\n",
    "    inputs_old.input_ids == mlm_tokenizer.mask_token_id\n",
    ")[0].nonzero(as_tuple=True)[0]\n",
    "# print(f\"indices_in_mlm_tokens_old: {indices_in_mlm_tokens_old}\")\n",
    "## get top k tokens for each index\n",
    "\n",
    "## make logits for special tokens -inf.\n",
    "special_token_ids = mlm_tokenizer.convert_tokens_to_ids(mlm_tokenizer.all_special_tokens)\n",
    "logits_old[:, :, special_token_ids] = -np.inf\n",
    "\n",
    "predicted_token_ids_old = torch.topk(\n",
    "    logits_old[0, indices_in_mlm_tokens_old],\n",
    "    k=config['k_per_location'],\n",
    "    dim=-1,\n",
    ")\n",
    "# print(f\"predicted_token_ids_old: {predicted_token_ids_old}\")\n",
    "# print(f\"mlm_tokenizer.batch_decode(predicted_token_ids_old.indices): {mlm_tokenizer.batch_decode(predicted_token_ids_old.indices)}\")\n",
    "    \n",
    "if config[\"method\"] == \"mlm-beamsearch-v0\":\n",
    "    # print(config[\"method\"])\n",
    "    hypotheses_old = beam_rerank_v0(source_text,\n",
    "                                inputs_old.input_ids,\n",
    "                                indices_in_mlm_tokens_old,\n",
    "                                predicted_token_ids_old,\n",
    "                                mlm_tokenizer, \n",
    "                                lossfns_old,\n",
    "                                config, \n",
    "                                beam_size = config['beam_size'])\n",
    "elif config[\"method\"] == \"mlm-beamsearch-v1\":\n",
    "    hypotheses_old = beam_rerank_v1(source_text,\n",
    "                                inputs_old.input_ids,\n",
    "                                indices_in_mlm_tokens_old,\n",
    "                                predicted_token_ids_old,\n",
    "                                mlm_tokenizer, \n",
    "                                lossfns_old,\n",
    "                                config, \n",
    "                                beam_size = config['beam_size'])\n",
    "\n",
    "elif config[\"method\"] == \"mlm-reranking\":\n",
    "    hypotheses_old = combi_rerank(inputs_old.input_ids, ## in mlm tokenizer's tokens\n",
    "        indices_in_mlm_tokens_old,\n",
    "        predicted_token_ids_old,\n",
    "        mlm_tokenizer,\n",
    "        config)\n",
    "\n",
    "candidate_total_losses_old = []\n",
    "candidate_primary_losses_old = []\n",
    "candidate_losses_for_loggings_old = []\n",
    "candidate_allsats_old = []\n",
    "\n",
    "for hyp in hypotheses_old:\n",
    "    curr_loss = 0.0\n",
    "    logging_loss = []\n",
    "    allsat = True\n",
    "    for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "        with torch.no_grad():\n",
    "            lossvalue = lossfns_old[lossid].compute_gold_loss(\n",
    "                source_text, hyp,\n",
    "                label_id=config['target_label_ids'][lossid],\n",
    "            )\n",
    "        curr_loss += loss_weights[lossid] * lossvalue.item()\n",
    "        logging_loss.append(lossvalue.item())\n",
    "        if lossid==0:\n",
    "            candidate_primary_losses_old.append(lossvalue.item())\n",
    "        elif (lossid >= 1) and (\n",
    "            lossvalue.item()\n",
    "            > -np.log(config[\"min_epsilons\"][lossid - 1])\n",
    "        ):\n",
    "            allsat = False\n",
    "    candidate_total_losses_old.append(curr_loss)\n",
    "    candidate_losses_for_loggings_old.append(logging_loss)\n",
    "    candidate_allsats_old.append(allsat)\n",
    "\n",
    "\n",
    "if config['selection_criteria'] == \"weighted_sum\":\n",
    "    best_ix_old = np.argmin(np.array(candidate_total_losses_old))\n",
    "elif config['selection_criteria'] == \"allsat_primary\":\n",
    "    allsat_ix = np.where(np.array(candidate_allsats_old) == True)[0]\n",
    "    if len(allsat_ix) > 0:\n",
    "        best_ix_old = np.argmin(\n",
    "            np.array(candidate_primary_losses_old)[allsat_ix]\n",
    "        )  # select min primary loss among allsats\n",
    "        best_ix_old = allsat_ix[best_ix_old]\n",
    "    else:  # if no candidate satisfying constraints, default to weighted_sum\n",
    "        best_ix_old = np.argmin(np.array(candidate_total_losses_old))\n",
    "    \n",
    "update = False\n",
    "if config['selection_criteria'] == \"weighted_sum\":\n",
    "    if best_weighted_loss_old > candidate_total_losses_old[best_ix_old]:\n",
    "        update = True\n",
    "elif config['selection_criteria'] == \"allsat_primary\":\n",
    "    if (\n",
    "        best_allsat_old is False\n",
    "        and candidate_allsats_old[best_ix_old] is True\n",
    "    ):\n",
    "        update = True\n",
    "    elif (\n",
    "        best_allsat_old is False\n",
    "        and candidate_allsats_old[best_ix_old] is False\n",
    "    ):\n",
    "        if best_weighted_loss_old > candidate_total_losses_old[best_ix_old]:\n",
    "            update = True\n",
    "    elif (\n",
    "        best_allsat_old is True\n",
    "        and candidate_allsats_old[best_ix_old] is True\n",
    "    ):\n",
    "        if (\n",
    "            best_losses_old[0]\n",
    "            > candidate_losses_for_loggings_old[best_ix_old][0]\n",
    "        ):\n",
    "            update = True\n",
    "\n",
    "\n",
    "## intermediate output for debugging\n",
    "int_output_old.update({f\"iter{_iter}_original_sentence\": running_text_old,\n",
    "                f\"iter{_iter}_masked_sentence\": masked_text_old,\n",
    "                f\"iter{_iter}_best_text\": hypotheses_old[best_ix_old],\n",
    "                f\"iter{_iter}_update\": update})    \n",
    "\n",
    "running_text_old = hypotheses_old[best_ix_old]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' wearing games and how ****ing much do I hate horse wearing games.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_text_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_iter = 1\n",
    "## locate tokens to edit\n",
    "masked_text_old  = locate_main(running_text_old, \n",
    "                        config[\"locate_method\"], \n",
    "                        name2model[config[\"model_paths\"][1]], \n",
    "                        name2tokenizer[config[\"tokenizer_paths\"][1]], \n",
    "                        max_num_tokens = config['num_edit_token_per_step'], \n",
    "                        unit=config[\"locate_unit\"], \n",
    "                        device=\"cuda\", \n",
    "                        label_id=config[\"target_label_ids\"][1],\n",
    "                        num_layer=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' wearing games and how ****ing much do I hate horse wearing games.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_text_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' wearing games and how ****ing much do I hate horse wearing games.']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' wearing games and how<mask><mask> much do I hate<mask> wearing games.']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_text_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' wearing<mask> and how<mask><mask> much do I hate<mask> wearing games.']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grad_norm'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['locate_method']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_text = locator.locate_main(running_text, \n",
    "                        method = config['locate_method'], \n",
    "                        max_num_tokens = config['num_edit_token_per_step'], \n",
    "                        unit = config['locate_unit'], \n",
    "                        num_layer = 10, #penultimate\n",
    "                        label_id = config['target_label_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' wearing games and how<mask><mask> much do I hate<mask> wearing games.']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_text_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "진짜 의외로 locate이 culprit 인것 같다.. 그렇다면 locate을 정확하게 했다고 치면 뒷부분은 동일한가? (이에 대한 확인은 위쪽 - new implementation - 섹션에서 진행하였다. -> 결론 : locate 이후는 동일하다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## replace tokens at the indices with mask tokens\n",
    "inputs_old = mlm_tokenizer(\n",
    "    masked_text_old, return_tensors=\"pt\"\n",
    ")\n",
    "# inputs_old = mlm_tokenizer(\n",
    "#     source_text + ' ' + masked_text_old[0], return_tensors=\"pt\", add_special_tokens=False\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_old=inputs_old.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True]], device='cuda:7')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids==inputs_old.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  2498,   426,     8,   141, 50264, 50264,   203,   109,    38,\n",
       "          4157, 50264,  2498,   426,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## make predictions for the masked indices\n",
    "with torch.no_grad():\n",
    "    logits_old = mlm(**inputs_old).logits\n",
    "indices_in_mlm_tokens_old = (\n",
    "    inputs_old.input_ids == mlm_tokenizer.mask_token_id\n",
    ")[0].nonzero(as_tuple=True)[0]\n",
    "# print(f\"indices_in_mlm_tokens_old: {indices_in_mlm_tokens_old}\")\n",
    "## get top k tokens for each index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5,  6, 11], device='cuda:7')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_in_mlm_tokens_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## make logits for special tokens -inf.\n",
    "special_token_ids = mlm_tokenizer.convert_tokens_to_ids(mlm_tokenizer.all_special_tokens)\n",
    "logits_old[:, :, special_token_ids] = -np.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted_token_ids_old = torch.topk(\n",
    "    logits_old[0, indices_in_mlm_tokens_old],\n",
    "    k=config['k_per_location'],\n",
    "    dim=-1,\n",
    ")\n",
    "# print(f\"predicted_token_ids_old: {predicted_token_ids_old}\")\n",
    "# print(f\"mlm_tokenizer.batch_decode(predicted_token_ids_old.indices): {mlm_tokenizer.batch_decode(predicted_token_ids_old.indices)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[16.869, 16.116, 13.910, 13.801, 13.715, 12.183, 12.001, 11.853, 11.661,\n",
       "         11.642],\n",
       "        [14.058, 11.541, 11.299, 10.800, 10.641, 10.358, 10.337, 10.216, 10.194,\n",
       "         10.193],\n",
       "        [15.107, 13.711, 13.133, 12.689, 12.413, 12.123, 12.028, 11.980, 11.967,\n",
       "         11.663]], device='cuda:7'),\n",
       "indices=tensor([[    6,     8,    73,   116,   203,    12, 14223,  2230,     4,    50],\n",
       "        [  141,  1336,  7105,  6179, 23523, 26536,   352,     6,  9178,   182],\n",
       "        [   45,   604,  2498,    82,  2185,   390,   888,    59,   127,    47]],\n",
       "       device='cuda:7'))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_ids_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"method\"] == \"mlm-beamsearch-v0\":\n",
    "    # print(config[\"method\"])\n",
    "    hypotheses_old = beam_rerank_v0(source_text,\n",
    "                                inputs_old.input_ids,\n",
    "                                indices_in_mlm_tokens_old,\n",
    "                                predicted_token_ids_old,\n",
    "                                mlm_tokenizer, \n",
    "                                lossfns_old,\n",
    "                                config, \n",
    "                                beam_size = config['beam_size'])\n",
    "elif config[\"method\"] == \"mlm-beamsearch-v1\":\n",
    "    hypotheses_old = beam_rerank_v1(source_text,\n",
    "                                inputs_old.input_ids,\n",
    "                                indices_in_mlm_tokens_old,\n",
    "                                predicted_token_ids_old,\n",
    "                                mlm_tokenizer, \n",
    "                                lossfns_old,\n",
    "                                config, \n",
    "                                beam_size = config['beam_size'])\n",
    "\n",
    "elif config[\"method\"] == \"mlm-reranking\":\n",
    "    hypotheses_old = combi_rerank(inputs_old.input_ids, ## in mlm tokenizer's tokens\n",
    "        indices_in_mlm_tokens_old,\n",
    "        predicted_token_ids_old,\n",
    "        mlm_tokenizer,\n",
    "        config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x==y for x,y in zip(hypotheses[0],hypotheses_old)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' wearing games and how and how much do I hate my wearing games.',\n",
       " ' wearing games and how and how much do I hate wearing wearing games.',\n",
       " ' wearing games and how and how much do I hate about wearing games.',\n",
       " ' wearing games and how and how much do I hate myself wearing games.',\n",
       " ' wearing games and how much very much do I hate my wearing games.']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "candidate_total_losses_old = []\n",
    "candidate_primary_losses_old = []\n",
    "candidate_losses_for_loggings_old = []\n",
    "candidate_allsats_old = []\n",
    "\n",
    "for hyp in hypotheses_old:\n",
    "    curr_loss = 0.0\n",
    "    logging_loss = []\n",
    "    allsat = True\n",
    "    for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "        with torch.no_grad():\n",
    "            lossvalue = lossfns_old[lossid].compute_gold_loss(\n",
    "                source_text, hyp,\n",
    "                label_id=config['target_label_ids'][lossid],\n",
    "            )\n",
    "        curr_loss += loss_weights[lossid] * lossvalue.item()\n",
    "        logging_loss.append(lossvalue.item())\n",
    "        if lossid==0:\n",
    "            candidate_primary_losses_old.append(lossvalue.item())\n",
    "        elif (lossid >= 1) and (\n",
    "            lossvalue.item()\n",
    "            > -np.log(config[\"min_epsilons\"][lossid - 1])\n",
    "        ):\n",
    "            allsat = False\n",
    "    candidate_total_losses_old.append(curr_loss)\n",
    "    candidate_losses_for_loggings_old.append(logging_loss)\n",
    "    candidate_allsats_old.append(allsat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if config['selection_criteria'] == \"weighted_sum\":\n",
    "    best_ix_old = np.argmin(np.array(candidate_total_losses_old))\n",
    "elif config['selection_criteria'] == \"allsat_primary\":\n",
    "    allsat_ix = np.where(np.array(candidate_allsats_old) == True)[0]\n",
    "    if len(allsat_ix) > 0:\n",
    "        best_ix_old = np.argmin(\n",
    "            np.array(candidate_primary_losses_old)[allsat_ix]\n",
    "        )  # select min primary loss among allsats\n",
    "        best_ix_old = allsat_ix[best_ix_old]\n",
    "    else:  # if no candidate satisfying constraints, default to weighted_sum\n",
    "        best_ix_old = np.argmin(np.array(candidate_total_losses_old))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6.6754633940756305, [66.51388549804688, 0.026749826967716217], True)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ix_old, candidate_total_losses_old[best_ix_old], candidate_losses_for_loggings_old[best_ix_old], candidate_allsats_old[best_ix_old]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' wearing games and how and how much do I hate wearing wearing games.'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses_old[best_ix_old]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update = False\n",
    "if config['selection_criteria'] == \"weighted_sum\":\n",
    "    if best_weighted_loss_old > candidate_total_losses_old[best_ix_old]:\n",
    "        update = True\n",
    "elif config['selection_criteria'] == \"allsat_primary\":\n",
    "    if (\n",
    "        best_allsat_old is False\n",
    "        and candidate_allsats_old[best_ix_old] is True\n",
    "    ):\n",
    "        update = True\n",
    "    elif (\n",
    "        best_allsat_old is False\n",
    "        and candidate_allsats_old[best_ix_old] is False\n",
    "    ):\n",
    "        if best_weighted_loss_old > candidate_total_losses_old[best_ix_old]:\n",
    "            update = True\n",
    "    elif (\n",
    "        best_allsat_old is True\n",
    "        and candidate_allsats_old[best_ix_old] is True\n",
    "    ):\n",
    "        if (\n",
    "            best_losses_old[0]\n",
    "            > candidate_losses_for_loggings_old[best_ix_old][0]\n",
    "        ):\n",
    "            update = True\n",
    "\n",
    "\n",
    "## intermediate output for debugging\n",
    "int_output_old.update({f\"iter{_iter}_original_sentence\": running_text_old,\n",
    "                f\"iter{_iter}_masked_sentence\": masked_text_old,\n",
    "                f\"iter{_iter}_best_text\": hypotheses_old[best_ix_old],\n",
    "                f\"iter{_iter}_update\": update})    \n",
    "\n",
    "running_text_old = hypotheses_old[best_ix_old]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc-edit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

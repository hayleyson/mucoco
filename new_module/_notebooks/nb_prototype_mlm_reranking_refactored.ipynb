{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38b40a4-9262-4d88-afb9-3ee8d79fa3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.1.2 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/s3/hyeryung/mucoco')\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "import mucoco.utils as utils\n",
    "import new_module.losses as lossbuilder\n",
    "import wandb\n",
    "from new_module.decode_utils import beam_rerank_v0, beam_rerank_v1, beam_rerank_v2, combi_rerank\n",
    "from new_module.evaluate_wandb import evaluate\n",
    "from new_module.locate.locate_utils import locate_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e80d7c-08b7-4ade-a95b-3c90ad98d3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import new_module.locate.locate_utils\n",
    "# importlib.reload(new_module.locate.locate_utils)\n",
    "# from new_module.locate.locate_utils import locate_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4a0813-e9f3-4596-96c0-ac71d0964386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import new_module.losses.gpt2\n",
    "# import new_module.losses.classification_no_prefix\n",
    "# import new_module.losses as lossbuilder\n",
    "# importlib.reload(new_module.losses)\n",
    "# importlib.reload(new_module.losses.classification_no_prefix)\n",
    "# importlib.reload(new_module.losses.gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "060c3a04-8e47-4288-8d04-29c06a617d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(os.environ.get(\"LOGGING_LEVEL\", logging.DEBUG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3ba687b-3a13-4314-92d4-8cc0650ccf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={#'model_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint'],\n",
    "        # 'tokenizer_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint'],\n",
    "        'model_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/step_2800_best_checkpoint/'],\n",
    "        'tokenizer_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/step_2800_best_checkpoint/'],\n",
    "        'model_types': [\"AutoModelForCausalLM\", \"AutoModelForSequenceClassification\"],\n",
    "        'cache_dir': \"hf_cache\",\n",
    "        'target_type': \"embeds\",\n",
    "        'method': \"mlm-beamsearch-v0\",\n",
    "       'losses': [\"gpt2\", \"classification_no_prefix_logprobloss\"],\n",
    "       'target_label_ids': [0,0] ,\n",
    "       'build_loss_dict': {\"coeff_steps\": 200, \"coeff_pattern\": \"constant\", \"loss_type\": \"xentropy\", \"length_normalize\": False, \"AR_temperature\": 1.0, \"AR_top_k\": 0, \"AR_top_p\": 0.96, \"max_output_length\": 20},\n",
    "       'min_epsilons': [0.75],\n",
    "       'source_data': 'new_module/data/toxicity-avoidance/testset_gpt2_2500.jsonl',\n",
    "       'locate_unit': 'word',\n",
    "       'locate_method': 'grad_norm',\n",
    "       'device': 'cuda',\n",
    "       'k_per_location': 3,\n",
    "       'closs_weight': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "162fbf50-f9be-46c7-8d2f-1e7a0c912159",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dummyArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "build_loss_args = dummyArgs(**config[\"build_loss_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3bcf7dff-3442-41e4-baf1-a0ea59929f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "name2tokenizer = {}\n",
    "name2model = {}\n",
    "name2config = {}\n",
    "loss2tokenizer = {}\n",
    "embed_luts = []\n",
    "primary_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "59367e36-ce2d-405f-bc9d-af29c40a4579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resetting dropped connection: huggingface.co\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "Some weights of the model checkpoint at /shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/step_2800_best_checkpoint/ were not used when initializing RobertaForSequenceClassification: ['roberta.embeddings.word_embeddings.0.weight', 'roberta.embeddings.word_embeddings.1.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/step_2800_best_checkpoint/ and are newly initialized: ['roberta.embeddings.word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "for i, model_path in enumerate(config[\"model_paths\"]):\n",
    "    if (\n",
    "        model_path not in name2model\n",
    "    ):  # making sure we are not loading the model twice in case some constraints use the same model.\n",
    "        try:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=True,\n",
    "            )\n",
    "        except:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=False,\n",
    "            )\n",
    "\n",
    "        name2config[model_path] = AutoConfig.from_pretrained(\n",
    "            model_path, cache_dir=config[\"cache_dir\"]\n",
    "        )\n",
    "\n",
    "        if \"Custom\" in config[\"model_types\"][i]:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(utils, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(transformers, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        name2model[model_path].eval()\n",
    "        name2model[model_path].cuda()\n",
    "\n",
    "    input_embeds = name2model[model_path].get_input_embeddings()\n",
    "    if isinstance(input_embeds, torch.nn.Sequential):\n",
    "        input_embeds = input_embeds[0]\n",
    "    embed_luts.append(input_embeds)\n",
    "\n",
    "    if config[\"target_type\"] == \"embeds\":\n",
    "        embed_luts[-1].requires_grad = False\n",
    "\n",
    "    if i == 0:\n",
    "        primary_model = name2model[model_path]\n",
    "\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "mlm = None if config[\"method\"] == \"mlm-beamsearch-v2\" else AutoModelForMaskedLM.from_pretrained(\"roberta-base\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c115177d-1a28-46ea-b8fe-0b3d350299ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfns = []\n",
    "for i, loss in enumerate(config[\"losses\"]):\n",
    "    lossfns.append(\n",
    "        lossbuilder.build_loss(\n",
    "            loss,\n",
    "            name2model[config[\"model_paths\"][i]],\n",
    "            name2tokenizer[config[\"model_paths\"][i]],\n",
    "            build_loss_args,\n",
    "        )\n",
    "    )\n",
    "    lossfns[i].tokenizer.add_special_tokens({\"mask_token\": mlm_tokenizer.mask_token})\n",
    "    loss2tokenizer[loss] = lossfns[i].tokenizer\n",
    "\n",
    "primary_tokenizer = name2tokenizer['gpt2-large']\n",
    "secondary_tokenizer = list(name2tokenizer.values())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55858af3-ce1f-4ea7-8b0f-f3f89916443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = config[\"target_label_ids\"]  # target label's ids for each loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "021c9739-ce48-4dfa-a6b3-623c680b66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"jsonl_primary_key\"]=\"prompt\"\n",
    "config[\"jsonl_secondary_key\"]=\"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "70b47582-da01-4851-b8d1-caaa745882c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(config[\"source_data\"], \"r\") as f:\n",
    "#     generation_dataset = [line.rstrip('\\n') for line in f.readlines()]\n",
    "# source_dataset = [\"\" for l in generation_dataset]\n",
    "source_dataset = [\n",
    "    json.loads(l)[config[\"jsonl_primary_key\"]][config[\"jsonl_secondary_key\"]]\n",
    "    for l in open(config[\"source_data\"])\n",
    "]\n",
    "generation_dataset = [\n",
    "    json.loads(l)[\"generations\"] for l in open(config[\"source_data\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19de82ce-16c5-480a-8244-412d62a52d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id=0\n",
    "source_text = source_dataset[text_id]\n",
    "if source_text == \"\":\n",
    "    source_text = lossfns[0].tokenizer.bos_token\n",
    "AR_prediction_all = [x[\"text\"] for x in generation_dataset[text_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "174d90bf-52c5-47ac-a4ea-5ad3387b1c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx=0\n",
    "predicted_batches = [x[\"tokens\"] for x in generation_dataset[text_id]]\n",
    "predicted_batches = [\n",
    "    torch.tensor([x], dtype=torch.long, device=config[\"device\"])\n",
    "    for x in predicted_batches\n",
    "]\n",
    "predicted_batch = predicted_batches[sample_idx].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ecd10d21-17e6-48b4-a336-a13134d0e5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_text: Let me just start by saying I hate horse\n",
      "AR_prediction_all: ['dirt. Unfortunately the majority of horses end up in shit that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?', 'wearing games and holy ****ing shit do I hate horse wearing games.', \"fetishes: it just makes me want to puke every time I see it on the internet, even though it's not worth a thing because I am a furry.\", 'head gelatin and see it as a sort of travelogues of American life, so it was good for me to be in Louisville as long', '-trading (talk).', 'riding or vegas\\' version of live riding, we have fine horse riding movies like \"Tancer\", \"Stoner Horse\", \"Ride', '-drawn cars.', \"racing. But we are a young country and if we can't beat those guys then that means we're not fit to do anything good in this world.\", 'racing as well as people who are into it.', \"play so you may just have to live with me when I say that this moment was glorious.'\\n\\nFinishing with a pious smile, he leapt upon the back\"]\n"
     ]
    }
   ],
   "source": [
    "print(f\"source_text: {source_text}\")\n",
    "print(f\"AR_prediction_all: {AR_prediction_all}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1506015f-7517-4ea2-b17a-7d6220aeb015",
   "metadata": {},
   "source": [
    "## Initial evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "66611ff3-41ab-4819-be2d-584a53225796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' dirt. Unfortunately the majority of horses end up in shit that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?<|endoftext|><|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "AR_prediction = primary_tokenizer.batch_decode(predicted_batch)[0]\n",
    "\n",
    "# --------------------------------------------------------------------------------------------- #\n",
    "## check whether initial text satisfies constraint\n",
    "allsat = True\n",
    "gold_losses = []\n",
    "for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "    with torch.no_grad():\n",
    "        lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "            source_text, AR_prediction,\n",
    "            label_id=label_ids[lossid],\n",
    "        )\n",
    "        \n",
    "    gold_losses.append(lossvalue.squeeze().item())\n",
    "    if (lossid >= 1) and (gold_losses[lossid] > -np.log(\n",
    "        config[\"min_epsilons\"][lossid - 1]\n",
    "    )):\n",
    "        allsat = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a63e3-187d-49fd-9b0e-0abfd244d83c",
   "metadata": {},
   "source": [
    "## Locate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b612d77c-3a0b-41d0-90dc-4e0de751427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"locate_unit\"]='token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21c974ab-1d2c-44db-ba19-3fec8d9f55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_text  = locate_main(AR_prediction, \n",
    "            config[\"locate_method\"], \n",
    "            name2model[config[\"model_paths\"][1]], \n",
    "            name2tokenizer[config[\"tokenizer_paths\"][1]], \n",
    "            max_num_tokens = 6, \n",
    "            unit=config[\"locate_unit\"], \n",
    "            device=\"cuda\", \n",
    "            label_id=config[\"target_label_ids\"][1],\n",
    "            num_layer=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c07cd5-6d00-4e1a-8c08-9e9aeaaafbb4",
   "metadata": {},
   "source": [
    "## Generate candidate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "546863ef-bcc5-4f7d-a762-f20bfc02865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = mlm_tokenizer(\n",
    "    source_text + ' ' + masked_text[0], return_tensors=\"pt\", add_special_tokens=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "788f604e-34b8-430b-8c48-d171ce32177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = mlm(**inputs).logits\n",
    "indices_in_mlm_tokens = (\n",
    "    inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    ")[0].nonzero(as_tuple=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08991e18-3f27-4454-a2ee-21503769cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get top k tokens for each index\n",
    "predicted_token_ids = torch.topk(\n",
    "    logits[0, indices_in_mlm_tokens],\n",
    "    k=config['k_per_location'],\n",
    "    dim=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a6384-2d5c-4364-98f4-e8ff446b3a79",
   "metadata": {},
   "source": [
    "## MLM reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c19dd9fe-1bc3-4554-be39-2f260a9cd452",
   "metadata": {},
   "outputs": [],
   "source": [
    "### \"mlm-reranking\"\n",
    "hypotheses = []\n",
    "num_located_tokens = len(indices_in_mlm_tokens)\n",
    "num_all_cases = config[\"k_per_location\"] ** num_located_tokens\n",
    "tok_cand_combo = [0 for i in range(num_located_tokens)]\n",
    "\n",
    "for case_id in range(num_all_cases):\n",
    "    for i in range(num_located_tokens):\n",
    "        tok_cand_combo[i] = (\n",
    "            case_id // (config[\"k_per_location\"] ** i)\n",
    "        ) % config[\"k_per_location\"]\n",
    "\n",
    "    tmp_seq = inputs[\"input_ids\"].clone()\n",
    "    for pos_id, tok_cand_id in enumerate(tok_cand_combo):\n",
    "        tmp_seq[\n",
    "            0, indices_in_mlm_tokens[pos_id]\n",
    "        ] = predicted_token_ids.indices[pos_id, tok_cand_id]\n",
    "\n",
    "    # need to do decode with RobertaTokenizer and encode with GPT2Tokenizer\n",
    "    # logger.debug(mlm_tokenizer.batch_decode(tmp_seq[:, indices_in_mlm_tokens], skip_special_tokens=True))\n",
    "    tmp_dec_seq = mlm_tokenizer.batch_decode(\n",
    "            tmp_seq, skip_special_tokens=True\n",
    "        )\n",
    "    hypotheses.append(tmp_dec_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b9b05e-ce61-41e1-a889-7517fa665d62",
   "metadata": {},
   "source": [
    "## BV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3741426e-79ad-44a3-af10-900c4560bf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.57 s ± 10.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "hypotheses = constrained_beam_search(source_text,\n",
    "                               inputs.input_ids,\n",
    "                               indices_in_mlm_tokens,\n",
    "                               predicted_token_ids,\n",
    "                               mlm_tokenizer, \n",
    "                               lossfns,\n",
    "                               config, \n",
    "                               beam_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b73165a1-38c2-4eb5-b33e-f5825802314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fn():\n",
    "    beam_size= 5\n",
    "    hypotheses = [torch.LongTensor([]).to(config['device'])]\n",
    "    \n",
    "    masked_sequence = inputs[\"input_ids\"].clone()\n",
    "    L = masked_sequence.size(-1)\n",
    "    \n",
    "    for i in range(L):\n",
    "\n",
    "        if masked_sequence[0, i] != mlm_tokenizer.mask_token_id:\n",
    "            for j in range(len(hypotheses)):\n",
    "                hypotheses[j] = torch.cat([hypotheses[j], masked_sequence[0, i].unsqueeze(0).to(config['device'])], dim = -1)\n",
    "\n",
    "        else:\n",
    "            hypotheses_exp = []\n",
    "            losses = []\n",
    "            for hyp in hypotheses:\n",
    "                # logger.debug(f\"hyp: {hyp}\")\n",
    "                for j in range(config['k_per_location']):\n",
    "                    candidate = predicted_token_ids.indices[torch.where(indices_in_mlm_tokens == i)[0], j].to(config['device'])\n",
    "                    hypotheses_exp.append(torch.cat([hyp, candidate], dim=-1))\n",
    "    \n",
    "                    # logger.debug(f\"hypotheses_exp at {i}: {hypotheses_exp}\")\n",
    "                    with torch.no_grad():\n",
    "                        lossvalue = lossfns[0].compute_gold_loss(\n",
    "                            source_text, mlm_tokenizer.decode(hypotheses_exp[-1])\n",
    "                        )\n",
    "                    losses.append(lossvalue)\n",
    "    \n",
    "            hypotheses = sorted(zip(hypotheses_exp, losses), key=lambda x: x[1])[:beam_size]\n",
    "            hypotheses = [x[0] for x in hypotheses]\n",
    "            \n",
    "    return [mlm_tokenizer.decode(x) for x in hypotheses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "396a75e3-8b78-461a-9469-9fd956c792cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.61 s ± 61.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dummy_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf52573-ab61-4471-9cb3-7212dacf5e44",
   "metadata": {},
   "source": [
    "## BV0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2b5f2a9-6e0f-4e1b-ad84-db765b01682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_rerank_v0(source_text,\n",
    "                    masked_sequence,\n",
    "                    indices_in_mlm_tokens,\n",
    "                    predicted_token_ids,\n",
    "                    mlm_tokenizer, \n",
    "                    lossfns,\n",
    "                    config, \n",
    "                    beam_size = 5):\n",
    "    \n",
    "    hypotheses = [torch.LongTensor([]).to(config['device'])]\n",
    "    L = masked_sequence.size(-1)\n",
    "\n",
    "    for i in range(L):\n",
    "        if masked_sequence[0, i] != mlm_tokenizer.mask_token_id:\n",
    "            hypotheses = list(torch.cat([torch.stack(hypotheses,dim=0), \n",
    "                                        masked_sequence[:, i].unsqueeze(0).repeat((len(hypotheses),1)).to(config['device'])],dim=-1))\n",
    "        else:\n",
    "            num_hypotheses = len(hypotheses)\n",
    "            hypotheses = torch.stack(hypotheses,dim=0).unsqueeze(0)\n",
    "            hypotheses = hypotheses.repeat(config['k_per_location'], 1, 1)\n",
    "            candidates = predicted_token_ids.indices[torch.where(indices_in_mlm_tokens == i)[0], :].to(config['device']).T.unsqueeze(1)\n",
    "            candidates = candidates.repeat(1, num_hypotheses, 1)\n",
    "            hypotheses_exp = torch.cat([hypotheses, candidates], dim=-1)\n",
    "            hypotheses_exp = hypotheses_exp.view(-1, hypotheses_exp.shape[-1])\n",
    "            hypotheses_exp = list(hypotheses_exp)\n",
    "\n",
    "            losses = []\n",
    "            loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "            for hyp in hypotheses_exp:\n",
    "                curr_loss = 0.0\n",
    "                for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "                    with torch.no_grad():\n",
    "                        lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                            source_text, mlm_tokenizer.decode(hyp),\n",
    "                            label_id=config['target_label_ids'][lossid],\n",
    "                        )\n",
    "                    curr_loss += loss_weights[lossid] * lossvalue.item()\n",
    "                losses.append(curr_loss)\n",
    "\n",
    "            hypotheses = sorted(zip(hypotheses_exp, losses), key=lambda x: x[1])[:beam_size]\n",
    "            hypotheses = [x[0] for x in hypotheses]\n",
    "            \n",
    "    return [mlm_tokenizer.decode(x) for x in hypotheses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ac4f8b5-b88e-495f-b734-1b156f02b9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let me just start by saying I hate horse dong. But the majority of us grew up the horse that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'Let me just start by saying I hate horse dong. But the majority of people grow up the horse that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'Let me just start by saying I hate horse dong. But the majority of people grew up the horse that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'Let me just start by saying I hate horse dong. But the majority of us grow up the horse that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'Let me just start by saying I hate horse dong. But the majority of us grew up the fact that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_rerank_v0(source_text,\n",
    "                    inputs.input_ids,\n",
    "                    indices_in_mlm_tokens,\n",
    "                    predicted_token_ids,\n",
    "                    mlm_tokenizer, \n",
    "                    lossfns,\n",
    "                    config, \n",
    "                    beam_size = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa9eb34-c068-416e-aa5c-ddea491a2d42",
   "metadata": {},
   "source": [
    "## BV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66f5802c-4e68-423f-870d-14aee45b91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -> 오래걸려서 버린 버전\n",
    "\n",
    "hypotheses = [torch.LongTensor([]).to(config['device'])]\n",
    "L = masked_sequence.size(-1)\n",
    "\n",
    "for i in range(L):\n",
    "    if masked_sequence[0, i] != primary_tokenizer.mask_token_id:\n",
    "        # print('!')\n",
    "        # print(masked_sequence[:, i])\n",
    "        hypotheses = list(torch.cat([torch.stack(hypotheses,dim=0), \n",
    "                                    masked_sequence[:, i].unsqueeze(0).repeat((len(hypotheses),1)).to(config['device'])],dim=-1))\n",
    "        # print(hypotheses)\n",
    "    else:\n",
    "        prefix_added_hypotheses = torch.cat([source_batch.expand(len(hypotheses), -1), torch.stack(hypotheses,dim=0)], dim=-1)\n",
    "        with torch.no_grad():\n",
    "            model_output = primary_model(input_ids = prefix_added_hypotheses)\n",
    "\n",
    "        logits_t = model_output.logits[:, -1, :] # get logits for the last timestep\n",
    "        logp_t = F.log_softmax(logits_t, dim=-1) # (num_hypotheses, |V|)\n",
    "        # print(logp_t.shape)\n",
    "        top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(-logp_t, k=beam_size, largest=True, dim=-1)\n",
    "\n",
    "        candidates = top_cand_hyp_pos.T.unsqueeze(1).repeat(1, beam_size, 1)\n",
    "        hypotheses_ = torch.stack(hypotheses).unsqueeze(1).repeat(1, beam_size, 1).view(len(hypotheses)*beam_size,-1)\n",
    "        hypotheses_exp = list(torch.cat([hypotheses_, top_cand_hyp_pos.view(-1,1)], dim=-1))\n",
    "        # print(len(hypotheses_exp))\n",
    "\n",
    "        losses = []\n",
    "        for hyp in hypotheses_exp:\n",
    "            with torch.no_grad():\n",
    "                lossvalue = lossfns[0].compute_gold_loss(\n",
    "                    source_text, mlm_tokenizer.decode(hyp),\n",
    "                )\n",
    "            losses.append(lossvalue.item())\n",
    "        hypotheses = sorted(zip(hypotheses_exp, losses), key=lambda x: x[1])[:beam_size]\n",
    "        hypotheses = [x[0] for x in hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "daacccca-c78e-42ae-a4f1-be1e2b8d8930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   67,   185,    13,   212,   262,  3741,   286,   211,   208,   510,\n",
       "           208,   208,   326,   345,   550,   284,  3708,  3511,    13,  2011,\n",
       "           691, 38424,   318,   284,  3745,   340,  3589,    13,  1867,   561,\n",
       "           307,   262,  3772, 12838,   286,   616,  1204,   788,    30],\n",
       "        device='cuda:0'),\n",
       " tensor([   67,   185,    13,   212,   262,  3741,   286,   211,   208,   510,\n",
       "           208,   181,   326,   345,   550,   284,  3708,  3511,    13,  2011,\n",
       "           691, 38424,   318,   284,  3745,   340,  3589,    13,  1867,   561,\n",
       "           307,   262,  3772, 12838,   286,   616,  1204,   788,    30],\n",
       "        device='cuda:0'),\n",
       " tensor([   67,   185,    13,   212,   262,  3741,   286,   211,   208,   510,\n",
       "           208,   212,   326,   345,   550,   284,  3708,  3511,    13,  2011,\n",
       "           691, 38424,   318,   284,  3745,   340,  3589,    13,  1867,   561,\n",
       "           307,   262,  3772, 12838,   286,   616,  1204,   788,    30],\n",
       "        device='cuda:0'),\n",
       " tensor([   67,   185,    13,   212,   262,  3741,   286,   211,   208,   510,\n",
       "           216,   208,   326,   345,   550,   284,  3708,  3511,    13,  2011,\n",
       "           691, 38424,   318,   284,  3745,   340,  3589,    13,  1867,   561,\n",
       "           307,   262,  3772, 12838,   286,   616,  1204,   788,    30],\n",
       "        device='cuda:0'),\n",
       " tensor([   67,   185,    13,   212,   262,  3741,   286,   211,   208,   510,\n",
       "           203,   208,   326,   345,   550,   284,  3708,  3511,    13,  2011,\n",
       "           691, 38424,   318,   284,  3745,   340,  3589,    13,  1867,   561,\n",
       "           307,   262,  3772, 12838,   286,   616,  1204,   788,    30],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3640c1a-2035-4325-9009-59b61b23410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = [primary_tokenizer.decode(x) for x in list(hypotheses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fe63f89-81b8-418e-aa5d-e8dda40d9199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.69 s ± 9.44 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "predicted_batch = masked_sequence\n",
    "hypotheses = torch.LongTensor([[]]).to(config['device'])\n",
    "hyp_scores = torch.zeros(len(hypotheses), dtype = torch.float, device = config['device'])\n",
    "L = masked_sequence.size(-1)\n",
    "\n",
    "for t in range(L):\n",
    "    prefix_added_hypotheses = torch.cat([source_batch.expand(hypotheses.size(0), -1), hypotheses], dim=-1)\n",
    "    # print(prefix_added_hypotheses)\n",
    "    with torch.no_grad():\n",
    "        model_output = primary_model(input_ids = prefix_added_hypotheses)\n",
    "\n",
    "    logits_t = model_output.logits[:, -1, :] # get logits for the last timestep\n",
    "    logp_t = F.log_softmax(logits_t, dim=-1) # (num_hypotheses, |V|)\n",
    "    \n",
    "    if predicted_batch[:,t] != primary_tokenizer.mask_token_id:\n",
    "        \n",
    "        curr_nll = F.nll_loss(logp_t, predicted_batch[:, t].expand(logp_t.size(0)), reduction=\"none\") # returns (num_hypotheses)\n",
    "        hyp_scores = hyp_scores.expand_as(curr_nll) + curr_nll # (num_hypotheses)\n",
    "        hypotheses = torch.cat([hypotheses, predicted_batch[:, t].expand(hypotheses.size(0), -1)], dim=-1)\n",
    "        \n",
    "    else:\n",
    "        contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(logp_t) + (-logp_t)).view(-1) # (num_hypotheses x |V|)\n",
    "        top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=beam_size, largest=True)\n",
    "        \n",
    "        prev_hyp_ids = torch.div(top_cand_hyp_pos, len(primary_tokenizer), rounding_mode='floor') # prev_hyp_id for each of top_cand_hyp. (beam_size)\n",
    "        hyp_word_ids = top_cand_hyp_pos % len(primary_tokenizer) # hyp_word_id for each of top_cand_hyp. (beam_size)\n",
    "        \n",
    "        hypotheses = torch.cat([hypotheses[prev_hyp_ids], hyp_word_ids.unsqueeze(1)], dim=-1)\n",
    "        hyp_scores = top_cand_hyp_scores\n",
    "\n",
    "    # torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb3a43e7-867c-4f59-9a85-28ffe8338193",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = [primary_tokenizer.decode(x) for x in list(hypotheses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50b10949-2734-4e9d-87b4-7eba2d91389c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d�.� the majority of\\x17� up\\x12\\x14 that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'd�.� the majority of\\x17� up\\x12� that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'd�.� the majority of\\x17� up\\x12� that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'd�.� the majority of\\x17� up\\x12\\x18 that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'd�.� the majority of\\x17� up\\x12� that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b062b3a-abfe-49b4-b72c-a5f97e920123",
   "metadata": {},
   "source": [
    "## Final hypothesis evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b3cb0b1-15ac-40ef-847b-952eda34b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "closs_weight = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06f9cb59-be4d-4005-a61a-064fd38fd581",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_total_losses = []\n",
    "candidate_primary_losses = []\n",
    "candidate_losses_for_loggings = []\n",
    "candidate_allsats = []\n",
    "loss_weights = [1 - closs_weight, closs_weight]\n",
    "for hyp in hypotheses:\n",
    "    curr_loss = 0.0\n",
    "    logging_loss = []\n",
    "    allsat = True\n",
    "    for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "        with torch.no_grad():\n",
    "            lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                source_text, hyp,\n",
    "                label_id=config['target_label_ids'][lossid],\n",
    "            )\n",
    "        curr_loss += loss_weights[lossid] * lossvalue.item()\n",
    "        logging_loss.append(lossvalue.item())\n",
    "        if lossid==0:\n",
    "            candidate_primary_losses.append(lossvalue.item())\n",
    "        elif (lossid >= 1) and (\n",
    "            lossvalue.item()\n",
    "            > -np.log(config[\"min_epsilons\"][lossid - 1])\n",
    "        ):\n",
    "            allsat = False\n",
    "    candidate_total_losses.append(curr_loss)\n",
    "    candidate_losses_for_loggings.append(logging_loss)\n",
    "    candidate_allsats.append(allsat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "070ad447-be07-4c0f-980c-60917671fc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[309.912670763582,\n",
       " 280.2622925773263,\n",
       " 309.9250766009092,\n",
       " 309.9390413619578,\n",
       " 309.9804595440626]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_total_losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

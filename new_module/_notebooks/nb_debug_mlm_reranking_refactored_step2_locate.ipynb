{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38b40a4-9262-4d88-afb9-3ee8d79fa3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.1.2 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/s3/hyeryung/mucoco')\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "import mucoco.utils as utils\n",
    "import new_module.losses as lossbuilder\n",
    "import wandb\n",
    "from new_module.decode_utils import beam_rerank_v0, beam_rerank_v1, beam_rerank_v2, combi_rerank\n",
    "from new_module.evaluation.evaluate_wandb import evaluate\n",
    "from new_module.locate.locate_utils import locate_main\n",
    "from new_module.locate.locate_utils_original import locate_main_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060c3a04-8e47-4288-8d04-29c06a617d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(os.environ.get(\"LOGGING_LEVEL\", logging.DEBUG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ba687b-3a13-4314-92d4-8cc0650ccf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={#'model_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint'],\n",
    "        # 'tokenizer_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint'],\n",
    "        'model_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/step_2800_best_checkpoint/'],\n",
    "        'tokenizer_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/step_2800_best_checkpoint/'],\n",
    "        'model_types': [\"AutoModelForCausalLM\", \"AutoModelForSequenceClassification\"],\n",
    "        'cache_dir': \"hf_cache\",\n",
    "        'target_type': \"embeds\",\n",
    "        'method': \"mlm-beamsearch-v0\",\n",
    "       'losses': [\"gpt2\", \"classification_no_prefix_logprobloss\"],\n",
    "       'target_label_ids': [0,0] ,\n",
    "       'build_loss_dict': {\"coeff_steps\": 200, \"coeff_pattern\": \"constant\", \"loss_type\": \"xentropy\", \"length_normalize\": False, \"AR_temperature\": 1.0, \"AR_top_k\": 0, \"AR_top_p\": 0.96, \"max_output_length\": 20},\n",
    "       'min_epsilons': [0.75],\n",
    "       'source_data': 'new_module/data/toxicity-avoidance/testset_gpt2_2500.jsonl',\n",
    "       'locate_unit': 'word',\n",
    "       'locate_method': 'grad_norm',\n",
    "       'device': 'cuda',\n",
    "       'k_per_location': 3,\n",
    "       'closs_weight': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "162fbf50-f9be-46c7-8d2f-1e7a0c912159",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dummyArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "build_loss_args = dummyArgs(**config[\"build_loss_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bcf7dff-3442-41e4-baf1-a0ea59929f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "name2tokenizer = {}\n",
    "name2model = {}\n",
    "name2config = {}\n",
    "loss2tokenizer = {}\n",
    "embed_luts = []\n",
    "primary_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59367e36-ce2d-405f-bc9d-af29c40a4579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "Some weights of the model checkpoint at /shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/step_2800_best_checkpoint/ were not used when initializing RobertaForSequenceClassification: ['roberta.embeddings.word_embeddings.1.weight', 'roberta.embeddings.word_embeddings.0.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/step_2800_best_checkpoint/ and are newly initialized: ['roberta.embeddings.word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "for i, model_path in enumerate(config[\"model_paths\"]):\n",
    "    if (\n",
    "        model_path not in name2model\n",
    "    ):  # making sure we are not loading the model twice in case some constraints use the same model.\n",
    "        try:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=True,\n",
    "            )\n",
    "        except:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=False,\n",
    "            )\n",
    "\n",
    "        name2config[model_path] = AutoConfig.from_pretrained(\n",
    "            model_path, cache_dir=config[\"cache_dir\"]\n",
    "        )\n",
    "\n",
    "        if \"Custom\" in config[\"model_types\"][i]:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(utils, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(transformers, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        name2model[model_path].eval()\n",
    "        name2model[model_path].cuda()\n",
    "\n",
    "    input_embeds = name2model[model_path].get_input_embeddings()\n",
    "    if isinstance(input_embeds, torch.nn.Sequential):\n",
    "        input_embeds = input_embeds[0]\n",
    "    embed_luts.append(input_embeds)\n",
    "\n",
    "    if config[\"target_type\"] == \"embeds\":\n",
    "        embed_luts[-1].requires_grad = False\n",
    "\n",
    "    if i == 0:\n",
    "        primary_model = name2model[model_path]\n",
    "\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "mlm = None if config[\"method\"] == \"mlm-beamsearch-v2\" else AutoModelForMaskedLM.from_pretrained(\"roberta-base\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c115177d-1a28-46ea-b8fe-0b3d350299ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfns = []\n",
    "for i, loss in enumerate(config[\"losses\"]):\n",
    "    lossfns.append(\n",
    "        lossbuilder.build_loss(\n",
    "            loss,\n",
    "            name2model[config[\"model_paths\"][i]],\n",
    "            name2tokenizer[config[\"model_paths\"][i]],\n",
    "            build_loss_args,\n",
    "        )\n",
    "    )\n",
    "    lossfns[i].tokenizer.add_special_tokens({\"mask_token\": mlm_tokenizer.mask_token})\n",
    "    loss2tokenizer[loss] = lossfns[i].tokenizer\n",
    "\n",
    "primary_tokenizer = name2tokenizer['gpt2-large']\n",
    "secondary_tokenizer = list(name2tokenizer.values())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55858af3-ce1f-4ea7-8b0f-f3f89916443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = config[\"target_label_ids\"]  # target label's ids for each loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "021c9739-ce48-4dfa-a6b3-623c680b66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"jsonl_primary_key\"]=\"prompt\"\n",
    "config[\"jsonl_secondary_key\"]=\"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70b47582-da01-4851-b8d1-caaa745882c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = [\n",
    "    json.loads(l)[config[\"jsonl_primary_key\"]][config[\"jsonl_secondary_key\"]]\n",
    "    for l in open(config[\"source_data\"])\n",
    "]\n",
    "generation_dataset = [\n",
    "    json.loads(l)[\"generations\"] for l in open(config[\"source_data\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e1a417d-8cfc-4fe1-b4ce-911b77fad264",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize 결과가 다른 example로 다시 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d6d0d9d-ac2a-4f5d-9909-dc27a4332f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_data = []\n",
    "for row in generation_dataset:\n",
    "    for item in row:\n",
    "        all_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "641ec0ea-b977-4f4a-9e36-5f55201241e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_df = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6b8fef7-8713-4fe6-8713-7366f2e40b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=list(name2tokenizer.values())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3de4a919-1005-4b57-b6af-ae67b4f36c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_df['tokens_dec']=generation_df['tokens'].apply(lambda x: tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44fea350-e3c7-4a98-852d-f92c5e89afe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation_df['tokens_dec_enc']=generation_df['tokens'].apply(lambda x: tokenizer.encode(tokenizer.decode(x),add_special_tokens=False))\n",
    "# generation_df['tokens_equal']=generation_df['tokens']==generation_df['tokens_dec_enc']\n",
    "# generation_df_compare = generation_df.loc[(~generation_df['tokens_equal'])&(~generation_df['locate_labels'].isna()),:]\n",
    "# 450, 742 -> 바꾸고 나서 locate 결과가 더 나아보이는데?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "26e34a54-18b2-4278-9456-4b952258f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"locate_unit\"]='token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "19de82ce-16c5-480a-8244-412d62a52d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_text: More evidence for my thesis below, about how Democrats now represent regular Americans while Republicans are a bunch of cranks,\n",
      "AR_prediction:  gibbering lunatics, convinced that the Russians or some other shady perv headed by a sloppy Comey make the proper appointments.\n"
     ]
    }
   ],
   "source": [
    "text_id=45\n",
    "sample_idx=0\n",
    "source_text = source_dataset[text_id]\n",
    "if source_text == \"\":\n",
    "    source_text = lossfns[0].tokenizer.bos_token\n",
    "AR_prediction_all = [x[\"text\"] for x in generation_dataset[text_id]]\n",
    "\n",
    "predicted_batches = [x[\"tokens\"] for x in generation_dataset[text_id]]\n",
    "predicted_batches = [\n",
    "    torch.tensor([x], dtype=torch.long, device=config[\"device\"])\n",
    "    for x in predicted_batches\n",
    "]\n",
    "predicted_batch = predicted_batches[sample_idx].cuda()\n",
    "AR_prediction = primary_tokenizer.batch_decode(predicted_batch)[0]\n",
    "\n",
    "print(f\"source_text: {source_text}\")\n",
    "print(f\"AR_prediction: {AR_prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a5e8535b-2ab9-4f56-ab84-ca0c6fb0130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_locate_fn(text):\n",
    "    return locate_main(text, \n",
    "            config[\"locate_method\"], \n",
    "            name2model[config[\"model_paths\"][1]], \n",
    "            name2tokenizer[config[\"tokenizer_paths\"][1]], \n",
    "            max_num_tokens = 6, \n",
    "            unit=config[\"locate_unit\"], \n",
    "            device=\"cuda\", \n",
    "            label_id=config[\"target_label_ids\"][1],\n",
    "            num_layer=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f7375f73-5b62-418c-a046-4702c950e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_df['new_locate_result']=generation_df['tokens_dec'].apply(new_locate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ce7b418a-9c5e-4d3c-8e8a-7305682fa636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_locate_fn(tokens):\n",
    "    tokens_ = torch.LongTensor(tokens).unsqueeze(0).to(config['device'])\n",
    "    batch = {'input_ids': tokens_,\n",
    "             'attention_mask': torch.ones_like(tokens_)}\n",
    "    \n",
    "    locate_ixes, locate_scores  = locate_main_original(\n",
    "                config[\"locate_method\"], \n",
    "                name2model[config[\"model_paths\"][1]], \n",
    "                name2tokenizer[config[\"tokenizer_paths\"][1]], \n",
    "                batch,\n",
    "                max_num_tokens = 6, \n",
    "                unit=config[\"locate_unit\"], \n",
    "                use_cuda=True, \n",
    "                label_id=config[\"target_label_ids\"][1],\n",
    "                num_layer=10)\n",
    "    \n",
    "    locate_ixes = sorted(locate_ixes[0])\n",
    "    predicted_batch_masked = tokens_.clone()\n",
    "    predicted_batch_masked[:, locate_ixes] = name2tokenizer[config[\"tokenizer_paths\"][1]].mask_token_id\n",
    "\n",
    "    return name2tokenizer[config[\"tokenizer_paths\"][1]].batch_decode(predicted_batch_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "03a11f6e-7acb-44d4-9afa-0248f212a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_df['old_locate_result']=generation_df['tokens'].apply(old_locate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "20e527cb-c3ee-4492-8777-8ed12e57ddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_df['new_locate_result']=generation_df['new_locate_result'].apply(lambda x: x[0])\n",
    "generation_df['old_locate_result']=generation_df['old_locate_result'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4e5b7674-1f67-4a87-add0-f930830e86a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_df_compare=generation_df.loc[generation_df['new_locate_result']!=generation_df['old_locate_result'],:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1ee33c6c-4f10-4b82-a560-261647c8a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "73a33122-d604-49a3-846e-8e3b50e20024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "tensor([1.000, 1.000, 0.000, 0.500, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.500, 0.500, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
      "       device='cuda:0')\n",
      "tensor([ 0,  1,  3, 14, 15], device='cuda:0')\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0.5, 0.5, 0.5, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "tensor([0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        1.000, 0.000, 1.000, 1.000, 0.500, 0.500, 0.500, 0.500, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000], device='cuda:0')\n",
      "tensor([10, 12, 13, 14, 15, 16, 17], device='cuda:0')\n",
      "[0, 0, 0, 0, 0, 0.5, 1, 0.5, 0.5, 0.5, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "tensor([0.000, 0.000, 0.000, 0.000, 0.000, 0.500, 1.000, 0.500, 0.500, 0.500,\n",
      "        0.500, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 0.000], device='cuda:0')\n",
      "tensor([ 5,  6,  7,  8,  9, 10], device='cuda:0')\n",
      "[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0.5, 0.5, 0, 0, 0, 0, 0]\n",
      "tensor([1.000, 1.000, 1.000, 1.000, 1.000, 0.000, 0.000, 0.000, 0.000, 0.000,\n",
      "        0.000, 0.000, 0.000, 1.000, 1.000, 1.000, 0.000, 0.000, 0.000, 0.500,\n",
      "        0.500, 0.000, 0.000, 0.000, 0.000, 0.000], device='cuda:0')\n",
      "tensor([ 0,  1,  2,  3,  4, 13, 14, 15, 19, 20], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def locate_label(tokens, labels):\n",
    "    \n",
    "    if labels is not np.nan:\n",
    "        tokens_ = torch.LongTensor(tokens).unsqueeze(0).to(config['device'])\n",
    "        print(labels)\n",
    "        labels_ = torch.Tensor(labels).to(config['device'])\n",
    "        print(labels_)\n",
    "        labels_ = torch.where(labels_ >= 0.5)[0]\n",
    "        print(labels_)\n",
    "        predicted_batch_masked = tokens_.clone()\n",
    "        predicted_batch_masked[:, labels_] = name2tokenizer[config[\"tokenizer_paths\"][1]].mask_token_id\n",
    "        return name2tokenizer[config[\"tokenizer_paths\"][1]].batch_decode(predicted_batch_masked)[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "generation_df_compare['locate_label_result']=generation_df_compare.apply(lambda x: locate_label(x['tokens'],x['locate_labels']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24908ccc-06bf-444b-9b1d-95b0e1b0b5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"bitching and moaning about the need to'save face' because those extremist bullies in the 'lib tribe' said you had to' hold\",\n",
       "        \"<mask><mask> and<mask> about the need to'save face' because those extremist bullies in the 'lib tribe' said you<mask><mask>'<mask>\",\n",
       "        \"<mask><mask> and moaning about the need to'save face' because those extremist bullies in the 'lib tribe' said<mask><mask><mask>'<mask>\",\n",
       "        \"<mask><mask> and<mask> about the need to'save face' because those<mask><mask> in the 'lib tribe' said you had to' hold\"],\n",
       "       ['“\"He Looked at Me, Said n*ggas Don\\'t Get Women “\\'\" and #420replays2fa20251“\"\\n\\nBut',\n",
       "        '“\"<mask> Looked at Me, Said n*ggas Don\\'t Get Women “\\'\" and #420replays<mask><mask><mask><mask><mask>�\"\\n\\nBut',\n",
       "        '“\"<mask> Looked at Me, Said n*ggas Don\\'t Get Women “\\'\" and #420replays<mask><mask><mask>251<mask><mask>\"\\n\\nBut',\n",
       "        '“\"He Looked at Me, Said<mask>*<mask><mask><mask><mask><mask><mask> “\\'\" and #420replays2fa20251“\"\\n\\nBut'],\n",
       "       ['using a fish hook to perform sex on a young woman. The canal was closed for three days after the incident.',\n",
       "        '<mask><mask><mask> hook to perform sex on<mask> young<mask>. The canal was closed for three days after the<mask>.',\n",
       "        '<mask><mask><mask> hook to perform sex on a young woman. The canal was closed for three days<mask><mask><mask>.',\n",
       "        ' using a fish hook to<mask><mask><mask><mask><mask><mask>. The canal was closed for three days after the incident.'],\n",
       "       ['gibbering lunatics, convinced that the Russians or some other shady perv headed by a sloppy Comey make the proper appointments.',\n",
       "        '<mask><mask><mask> lunatics, convinced that the Russians or some other shady perv headed by a<mask> Comey make the<mask><mask>.',\n",
       "        '<mask><mask><mask> lunatics, convinced that the Russians or<mask> other shady perv headed by a sloppy Comey make the<mask> appointments.',\n",
       "        '<mask><mask><mask><mask><mask>, convinced that the Russians or some other<mask><mask><mask> headed by a<mask><mask> make the proper appointments.']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_df_compare.loc[generation_df_compare['locate_label_result'].apply(lambda x: True if x is not None else False), ['text','new_locate_result','old_locate_result','locate_label_result']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a777d22-8574-4285-8606-a9c9959e399b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

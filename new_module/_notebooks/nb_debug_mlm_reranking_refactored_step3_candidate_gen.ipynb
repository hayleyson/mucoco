{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38b40a4-9262-4d88-afb9-3ee8d79fa3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.1.2 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/s3/hyeryung/mucoco')\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "import mucoco.utils as utils\n",
    "import new_module.losses as lossbuilder\n",
    "import wandb\n",
    "from new_module.decode_utils import beam_rerank_v0, beam_rerank_v1, beam_rerank_v2, combi_rerank\n",
    "from new_module.evaluation.evaluate_wandb import evaluate\n",
    "from new_module.locate.locate_utils import locate_main\n",
    "from new_module.locate.locate_utils_original import locate_main_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060c3a04-8e47-4288-8d04-29c06a617d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(os.environ.get(\"LOGGING_LEVEL\", logging.DEBUG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ba687b-3a13-4314-92d4-8cc0650ccf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={#'model_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint'],\n",
    "        # 'tokenizer_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint'],\n",
    "        'model_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/step_2800_best_checkpoint/'],\n",
    "        'tokenizer_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/step_2800_best_checkpoint/'],\n",
    "        'model_types': [\"AutoModelForCausalLM\", \"AutoModelForSequenceClassification\"],\n",
    "        'cache_dir': \"hf_cache\",\n",
    "        'target_type': \"embeds\",\n",
    "        'method': \"mlm-beamsearch-v0\",\n",
    "       'losses': [\"gpt2\", \"classification_no_prefix_logprobloss\"],\n",
    "       'target_label_ids': [0,0] ,\n",
    "       'build_loss_dict': {\"coeff_steps\": 200, \"coeff_pattern\": \"constant\", \"loss_type\": \"xentropy\", \"length_normalize\": False, \"AR_temperature\": 1.0, \"AR_top_k\": 0, \"AR_top_p\": 0.96, \"max_output_length\": 20},\n",
    "       'min_epsilons': [0.75],\n",
    "       'source_data': 'new_module/data/toxicity-avoidance/testset_gpt2_2500.jsonl',\n",
    "       'locate_unit': 'word',\n",
    "       'locate_method': 'grad_norm',\n",
    "       'device': 'cuda',\n",
    "       'k_per_location': 3,\n",
    "       'closs_weight': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "162fbf50-f9be-46c7-8d2f-1e7a0c912159",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dummyArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "build_loss_args = dummyArgs(**config[\"build_loss_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bcf7dff-3442-41e4-baf1-a0ea59929f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "name2tokenizer = {}\n",
    "name2model = {}\n",
    "name2config = {}\n",
    "loss2tokenizer = {}\n",
    "embed_luts = []\n",
    "primary_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59367e36-ce2d-405f-bc9d-af29c40a4579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "Some weights of the model checkpoint at /shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/step_2800_best_checkpoint/ were not used when initializing RobertaForSequenceClassification: ['roberta.embeddings.word_embeddings.1.weight', 'roberta.embeddings.word_embeddings.0.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/step_2800_best_checkpoint/ and are newly initialized: ['roberta.embeddings.word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "for i, model_path in enumerate(config[\"model_paths\"]):\n",
    "    if (\n",
    "        model_path not in name2model\n",
    "    ):  # making sure we are not loading the model twice in case some constraints use the same model.\n",
    "        try:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=True,\n",
    "            )\n",
    "        except:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=False,\n",
    "            )\n",
    "\n",
    "        name2config[model_path] = AutoConfig.from_pretrained(\n",
    "            model_path, cache_dir=config[\"cache_dir\"]\n",
    "        )\n",
    "\n",
    "        if \"Custom\" in config[\"model_types\"][i]:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(utils, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(transformers, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        name2model[model_path].eval()\n",
    "        name2model[model_path].cuda()\n",
    "\n",
    "    input_embeds = name2model[model_path].get_input_embeddings()\n",
    "    if isinstance(input_embeds, torch.nn.Sequential):\n",
    "        input_embeds = input_embeds[0]\n",
    "    embed_luts.append(input_embeds)\n",
    "\n",
    "    if config[\"target_type\"] == \"embeds\":\n",
    "        embed_luts[-1].requires_grad = False\n",
    "\n",
    "    if i == 0:\n",
    "        primary_model = name2model[model_path]\n",
    "\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "mlm = None if config[\"method\"] == \"mlm-beamsearch-v2\" else AutoModelForMaskedLM.from_pretrained(\"roberta-base\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c115177d-1a28-46ea-b8fe-0b3d350299ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfns = []\n",
    "for i, loss in enumerate(config[\"losses\"]):\n",
    "    lossfns.append(\n",
    "        lossbuilder.build_loss(\n",
    "            loss,\n",
    "            name2model[config[\"model_paths\"][i]],\n",
    "            name2tokenizer[config[\"model_paths\"][i]],\n",
    "            build_loss_args,\n",
    "        )\n",
    "    )\n",
    "    lossfns[i].tokenizer.add_special_tokens({\"mask_token\": mlm_tokenizer.mask_token})\n",
    "    loss2tokenizer[loss] = lossfns[i].tokenizer\n",
    "\n",
    "primary_tokenizer = name2tokenizer['gpt2-large']\n",
    "secondary_tokenizer = list(name2tokenizer.values())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55858af3-ce1f-4ea7-8b0f-f3f89916443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = config[\"target_label_ids\"]  # target label's ids for each loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "021c9739-ce48-4dfa-a6b3-623c680b66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"jsonl_primary_key\"]=\"prompt\"\n",
    "config[\"jsonl_secondary_key\"]=\"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70b47582-da01-4851-b8d1-caaa745882c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = [\n",
    "    json.loads(l)[config[\"jsonl_primary_key\"]][config[\"jsonl_secondary_key\"]]\n",
    "    for l in open(config[\"source_data\"])\n",
    "]\n",
    "generation_dataset = [\n",
    "    json.loads(l)[\"generations\"] for l in open(config[\"source_data\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8eb137d-e335-4118-915f-2f7e1c9a278e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_text: Let me just start by saying I hate horse\n",
      "AR_prediction:  dirt. Unfortunately the majority of horses end up in shit that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?\n"
     ]
    }
   ],
   "source": [
    "text_id=0\n",
    "sample_idx=0\n",
    "source_text = source_dataset[text_id]\n",
    "if source_text == \"\":\n",
    "    source_text = lossfns[0].tokenizer.bos_token\n",
    "AR_prediction_all = [x[\"text\"] for x in generation_dataset[text_id]]\n",
    "\n",
    "predicted_batches = [x[\"tokens\"] for x in generation_dataset[text_id]]\n",
    "predicted_batches = [\n",
    "    torch.tensor([x], dtype=torch.long, device=config[\"device\"])\n",
    "    for x in predicted_batches\n",
    "]\n",
    "predicted_batch = predicted_batches[sample_idx].cuda()\n",
    "AR_prediction = primary_tokenizer.batch_decode(predicted_batch)[0]\n",
    "\n",
    "print(f\"source_text: {source_text}\")\n",
    "print(f\"AR_prediction: {AR_prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b0c12-401d-4b08-8217-31a06b73fd42",
   "metadata": {},
   "source": [
    "# Candidate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cf4a84c-a90f-4e44-a6fb-290d0ed086db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_data = []\n",
    "for row in generation_dataset:\n",
    "    for item in row:\n",
    "        all_data.append(item)\n",
    "generation_df = pd.DataFrame(all_data)\n",
    "tokenizer=list(name2tokenizer.values())[1]\n",
    "generation_df['tokens_dec']=generation_df['tokens'].apply(lambda x: tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c46cfe63-aab7-4b4d-855d-f175e7678a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"locate_unit\"]='token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2449d2e5-5548-40e4-b00a-9d38bd0b32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_df['tokens_dec_enc']=generation_df['tokens_dec'].apply(lambda x: tokenizer.encode(x,add_special_tokens=False\n",
    "                                                                                            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9f4e0d6-ef03-49d6-a04f-068dc3a9b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_df['dec_enc_eq_tokens']=generation_df['tokens_dec_enc']==generation_df['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c18713c-b129-41d5-9222-67463bcd5836",
   "metadata": {},
   "source": [
    "## New implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69d90523-0123-4c28-b292-efc94ed26c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_locate_candidate_generation_fn(text):\n",
    "    masked_text = locate_main(text, \n",
    "            config[\"locate_method\"], \n",
    "            name2model[config[\"model_paths\"][1]], \n",
    "            name2tokenizer[config[\"tokenizer_paths\"][1]], \n",
    "            max_num_tokens = 6, \n",
    "            unit=config[\"locate_unit\"], \n",
    "            device=\"cuda\", \n",
    "            label_id=config[\"target_label_ids\"][1],\n",
    "            num_layer=10)\n",
    "    inputs = mlm_tokenizer(\n",
    "                masked_text, return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = mlm(**inputs).logits\n",
    "    indices_in_mlm_tokens = (\n",
    "        inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "    )[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "    ## get top k tokens for each index\n",
    "    predicted_token_ids = torch.topk(\n",
    "        logits[0, indices_in_mlm_tokens],\n",
    "        k=config['k_per_location'],\n",
    "        dim=-1,\n",
    "    )\n",
    "\n",
    "    return predicted_token_ids\n",
    "\n",
    "predicted_token_ids = new_locate_candidate_generation_fn(AR_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "76ee4476-5981-4f9b-b84a-1292c148a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_df['new_candidates_result']=generation_df['tokens_dec'].apply(new_locate_candidate_generation_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c89f17-0837-4b62-9b32-1145d4279770",
   "metadata": {},
   "source": [
    "## Old implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7652fb65-b178-49b3-adbf-53bb06f7c26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_locate_candidate_generation_fn(tokens):\n",
    "    try:\n",
    "        tokens_ = torch.LongTensor(tokens).unsqueeze(0).to(config['device'])\n",
    "        batch = {'input_ids': tokens_,\n",
    "                 'attention_mask': torch.ones_like(tokens_)}\n",
    "        \n",
    "        locate_ixes, locate_scores  = locate_main_original(\n",
    "                    config[\"locate_method\"], \n",
    "                    name2model[config[\"model_paths\"][1]], \n",
    "                    name2tokenizer[config[\"tokenizer_paths\"][1]], \n",
    "                    batch,\n",
    "                    max_num_tokens = 6, \n",
    "                    unit=config[\"locate_unit\"], \n",
    "                    use_cuda=True, \n",
    "                    label_id=config[\"target_label_ids\"][1],\n",
    "                    num_layer=10)\n",
    "    \n",
    "        masked_sequence = tokens_.clone().detach()\n",
    "        masked_sequence[:, locate_ixes[0]] = name2tokenizer[config[\"tokenizer_paths\"][0]].mask_token_id\n",
    "        masked_sequence_text = name2tokenizer[config[\"tokenizer_paths\"][0]].batch_decode(masked_sequence.tolist())\n",
    "        inputs = mlm_tokenizer(\n",
    "                                    masked_sequence_text, return_tensors=\"pt\"\n",
    "                                )\n",
    "        ## make predictions for the masked indices\n",
    "        with torch.no_grad():\n",
    "            logits = mlm(**inputs).logits\n",
    "        indices_in_mlm_tokens = (\n",
    "            inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "        )[0].nonzero(as_tuple=True)[0]\n",
    "        \n",
    "        ## get top k tokens for each index\n",
    "        predicted_token_ids2 = torch.topk(\n",
    "            logits[0, indices_in_mlm_tokens],\n",
    "            k=config['k_per_location'],\n",
    "            dim=-1,\n",
    "        )\n",
    "        return predicted_token_ids2\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "511de6de-44df-49d5-96b9-e0b6a824921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_df['old_candidates_result']=generation_df['tokens'].apply(old_locate_candidate_generation_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3787ba57-f1f2-4ee9-9468-b0284a2ab592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare1(x):\n",
    "    try:\n",
    "        return True if (x['old_candidates_result'].values != x['new_candidates_result'].values).sum().item() == 0 else False \n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "generation_df['compare1']=generation_df.apply(compare1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "339e24a7-6bbc-4cc4-b4c1-487ecf2d8df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare2(x):\n",
    "    try:\n",
    "        return True if (x['old_candidates_result'].indices != x['new_candidates_result'].indices).sum().item() == 0 else False \n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "generation_df['compare2']=generation_df.apply(compare2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f2037c03-e710-4ba3-a3d4-f874390cd7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 8)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_df.loc[(~generation_df['compare1'])|(~generation_df['compare2']),:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6a4e7257-4d9f-47dc-8591-fc44568e73e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dec_enc_eq_tokens\n",
       "False    52\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_df.loc[(~generation_df['compare1'])|(~generation_df['compare2']),'dec_enc_eq_tokens'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f60b6132-b8e1-4649-8319-4273dfac1a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare  = generation_df.loc[(~generation_df['compare1'])|(~generation_df['compare2'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2de0526f-aac1-4ee9-ad72-af2ca727a068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>locate_labels</th>\n",
       "      <th>tokens_dec</th>\n",
       "      <th>old_candidates_result</th>\n",
       "      <th>new_candidates_result</th>\n",
       "      <th>compare1</th>\n",
       "      <th>compare2</th>\n",
       "      <th>tokens_dec_enc</th>\n",
       "      <th>dec_enc_eq_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>offering the serial murderer Birgitta Jónsdótt...</td>\n",
       "      <td>[6011, 262, 11389, 29262, 12817, 70, 48519, 44...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>offering the serial murderer Birgitta Jónsdót...</td>\n",
       "      <td>([[tensor(18.345), tensor(17.951), tensor(17.5...</td>\n",
       "      <td>([[tensor(18.256), tensor(17.320), tensor(17.1...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[6011, 262, 11389, 29262, 12817, 70, 48519, 44...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>look‼️‼️‼️At everyone. And in’a perfect world‼...</td>\n",
       "      <td>[804, 447, 120, 37929, 447, 120, 37929, 447, 1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>look‼️‼️‼️At everyone. And in’a perfect world...</td>\n",
       "      <td>([[tensor(17.685), tensor(12.269), tensor(11.5...</td>\n",
       "      <td>([[tensor(16.100), tensor(14.455), tensor(14.1...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[804, 447, 120, 37929, 447, 120, 37929, 447, 1...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>but um...Chicago because you don't give a fuck...</td>\n",
       "      <td>[475, 23781, 986, 25705, 780, 345, 836, 470, 1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>but um...Chicago because you don't give a fuc...</td>\n",
       "      <td>([[tensor(15.334), tensor(13.658), tensor(13.1...</td>\n",
       "      <td>([[tensor(14.829), tensor(14.478), tensor(14.0...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[475, 23781, 986, 25705, 780, 345, 836, 470, 1...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>with terms like \"fat pigs, dogs, slobs and dis...</td>\n",
       "      <td>[351, 2846, 588, 366, 17359, 22333, 11, 6844, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>with terms like \"fat pigs, dogs, slobs and di...</td>\n",
       "      <td>([[tensor(17.486), tensor(17.077), tensor(16.3...</td>\n",
       "      <td>([[tensor(16.689), tensor(16.604), tensor(16.3...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[351, 2846, 588, 366, 17359, 22333, 11, 6844, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>him Watson because this game is in the news, s...</td>\n",
       "      <td>[683, 14959, 780, 428, 983, 318, 287, 262, 170...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>him Watson because this game is in the news, ...</td>\n",
       "      <td>([[tensor(16.510), tensor(16.376), tensor(15.7...</td>\n",
       "      <td>([[tensor(16.146), tensor(16.003), tensor(15.5...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[683, 14959, 780, 428, 983, 318, 287, 262, 170...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>him Mi Joo-Kim.</td>\n",
       "      <td>[683, 13756, 449, 2238, 12, 42, 320, 13]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>him Mi Joo-Kim.</td>\n",
       "      <td>([[tensor(13.968), tensor(13.820), tensor(13.0...</td>\n",
       "      <td>([[tensor(13.574), tensor(13.474), tensor(12.7...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[683, 13756, 449, 2238, 12, 26374, 13]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>pie, cupcake...Can't make it if I do. Video...</td>\n",
       "      <td>[2508, 11, 6508, 30560, 986, 6090, 470, 787, 3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pie, cupcake...Can't make it if I do. Video...</td>\n",
       "      <td>([[tensor(15.152), tensor(15.124), tensor(15.0...</td>\n",
       "      <td>([[tensor(14.660), tensor(14.653), tensor(14.5...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[2508, 11, 6508, 30560, 986, 6090, 470, 787, 3...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>to have the ungodly activity of a fry making c...</td>\n",
       "      <td>[284, 423, 262, 555, 25344, 306, 3842, 286, 25...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>to have the ungodly activity of a fry making ...</td>\n",
       "      <td>([[tensor(14.949), tensor(14.805), tensor(14.5...</td>\n",
       "      <td>([[tensor(15.842), tensor(15.277), tensor(14.9...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[284, 423, 262, 555, 25344, 306, 3842, 286, 25...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>leading a rally at the Indiana Statehouse, bas...</td>\n",
       "      <td>[3756, 257, 7903, 379, 262, 10278, 1812, 4803,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>leading a rally at the Indiana Statehouse, ba...</td>\n",
       "      <td>([[tensor(16.935), tensor(15.568), tensor(15.2...</td>\n",
       "      <td>([[tensor(15.245), tensor(14.623), tensor(14.5...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[3756, 257, 7903, 379, 262, 10278, 1812, 4803,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>will either be whine or familiar moaning‖.no a...</td>\n",
       "      <td>[481, 2035, 307, 47409, 393, 5385, 47644, 447,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>will either be whine or familiar moaning‖.no ...</td>\n",
       "      <td>([[tensor(14.669), tensor(13.161), tensor(12.8...</td>\n",
       "      <td>([[tensor(15.046), tensor(13.645), tensor(12.8...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[481, 2035, 307, 47409, 393, 5385, 47644, 447,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>and you'd better wait,‬“because we’ll be waiti...</td>\n",
       "      <td>[392, 345, 1549, 1365, 4043, 11, 447, 105, 447...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>and you'd better wait,‬“because we’ll be waiti...</td>\n",
       "      <td>([[tensor(13.787), tensor(13.454), tensor(11.1...</td>\n",
       "      <td>([[tensor(16.605), tensor(14.510), tensor(12.4...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[392, 345, 1549, 1365, 4043, 11, 447, 105, 447...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>from Knoxville and the state of Tennessee ANTI...</td>\n",
       "      <td>[422, 27633, 4244, 290, 262, 1181, 286, 11287,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>from Knoxville and the state of Tennessee ANT...</td>\n",
       "      <td>([[tensor(26.556), tensor(18.387), tensor(17.4...</td>\n",
       "      <td>([[tensor(26.780), tensor(18.584), tensor(17.7...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[422, 27633, 4244, 290, 262, 1181, 286, 11287,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>'stalking radicals.' 'Not a movement. A David'...</td>\n",
       "      <td>[705, 301, 18998, 32842, 2637, 705, 3673, 257,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>'stalking radicals.' 'Not a movement. A David'...</td>\n",
       "      <td>([[tensor(19.185), tensor(19.175), tensor(19.1...</td>\n",
       "      <td>([[tensor(18.250), tensor(16.230), tensor(15.7...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[338, 48186, 32842, 2637, 705, 3673, 257, 3356...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>using a fish hook to perform sex on a young wo...</td>\n",
       "      <td>[1262, 257, 5916, 8011, 284, 1620, 1714, 319, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0.5, 1, 0.5, 0.5, 0.5, 0.5, 0,...</td>\n",
       "      <td>using a fish hook to perform sex on a young w...</td>\n",
       "      <td>([[tensor(18.326), tensor(16.673), tensor(16.0...</td>\n",
       "      <td>([[tensor(17.991), tensor(16.332), tensor(16.2...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[1262, 257, 5916, 8011, 284, 1620, 1714, 319, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>gibbering lunatics, convinced that the Russian...</td>\n",
       "      <td>[46795, 527, 278, 14678, 23372, 11, 9431, 326,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>gibbering lunatics, convinced that the Russia...</td>\n",
       "      <td>([[tensor(23.616), tensor(23.206), tensor(22.7...</td>\n",
       "      <td>([[tensor(23.580), tensor(22.775), tensor(22.7...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[46795, 527, 278, 14678, 23372, 11, 9431, 326,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>***?. He was here purely to get money for food...</td>\n",
       "      <td>[17202, 5633, 13, 679, 373, 994, 14177, 284, 6...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>***?. He was here purely to get money for foo...</td>\n",
       "      <td>([[tensor(15.578), tensor(15.041), tensor(14.9...</td>\n",
       "      <td>([[tensor(15.285), tensor(14.881), tensor(14.8...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[17202, 30, 13, 679, 373, 994, 14177, 284, 651...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>Die’’’’’’’’’’’�</td>\n",
       "      <td>[6733, 447, 247, 447, 247, 447, 247, 447, 247,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Die’’’’’’’’’’’�</td>\n",
       "      <td>([[tensor(18.742), tensor(17.814), tensor(16.4...</td>\n",
       "      <td>([[tensor(18.437), tensor(17.767), tensor(15.3...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[6733, 447, 247, 447, 247, 447, 247, 447, 247,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>Drive on America with Immediate Remove≠Toilet ...</td>\n",
       "      <td>[9974, 319, 2253, 351, 1846, 13857, 17220, 357...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Drive on America with Immediate Remove≠Toilet...</td>\n",
       "      <td>([[tensor(16.768), tensor(16.681), tensor(16.3...</td>\n",
       "      <td>([[tensor(16.569), tensor(16.444), tensor(16.3...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[9974, 319, 2253, 351, 1846, 13857, 17220, 357...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>14 and 16, on the girls' behalf to have sex wi...</td>\n",
       "      <td>[1478, 290, 1467, 11, 319, 262, 4813, 6, 8378,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14 and 16, on the girls' behalf to have sex w...</td>\n",
       "      <td>([[tensor(13.849), tensor(13.701), tensor(13.1...</td>\n",
       "      <td>([[tensor(23.370), tensor(20.690), tensor(19.6...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[1478, 290, 1467, 11, 319, 262, 4813, 6, 8378,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>a girlie boys girl here think of her different...</td>\n",
       "      <td>[257, 2576, 494, 6510, 2576, 994, 892, 286, 60...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a girlie boys girl here think of her differen...</td>\n",
       "      <td>([[tensor(14.040), tensor(13.850), tensor(13.7...</td>\n",
       "      <td>([[tensor(14.107), tensor(14.032), tensor(13.6...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[257, 2576, 494, 6510, 2576, 994, 892, 286, 60...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>over like this and he was f...</td>\n",
       "      <td>[625, 588, 428, 290, 339, 373, 277, 13, 764, 764]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>over like this and he was f...</td>\n",
       "      <td>([[tensor(11.539), tensor(10.413), tensor(10.0...</td>\n",
       "      <td>([[tensor(10.033), tensor(9.614), tensor(9.431...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[625, 588, 428, 290, 339, 373, 277, 986]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>wealthy individuals who want to censor free sp...</td>\n",
       "      <td>[11574, 3925, 508, 765, 284, 42081, 1479, 4046...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wealthy individuals who want to censor free s...</td>\n",
       "      <td>([[tensor(15.969), tensor(14.063), tensor(14.0...</td>\n",
       "      <td>([[tensor(15.258), tensor(15.115), tensor(14.4...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[11574, 3925, 508, 765, 284, 42081, 1479, 4046...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>than you, Mrs Oni and grandchildren being atta...</td>\n",
       "      <td>[621, 345, 11, 9074, 1550, 72, 290, 28986, 852...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>than you, Mrs Oni and grandchildren being att...</td>\n",
       "      <td>([[tensor(15.202), tensor(15.028), tensor(14.7...</td>\n",
       "      <td>([[tensor(15.783), tensor(15.322), tensor(14.9...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[621, 345, 11, 9074, 1550, 72, 290, 28986, 852...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>than this brave teenage girl named Jillian?\" H...</td>\n",
       "      <td>[621, 428, 14802, 16330, 2576, 3706, 22772, 66...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>than this brave teenage girl named Jillian?\" ...</td>\n",
       "      <td>([[tensor(23.630), tensor(18.627), tensor(18.5...</td>\n",
       "      <td>([[tensor(23.289), tensor(18.985), tensor(18.3...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[621, 428, 14802, 16330, 2576, 3706, 22772, 66...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>the two'stones for [the] theft'.</td>\n",
       "      <td>[262, 734, 705, 28750, 329, 685, 1169, 60, 124...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the two'stones for [the] theft'.</td>\n",
       "      <td>([[tensor(16.713), tensor(14.780), tensor(14.3...</td>\n",
       "      <td>([[tensor(17.254), tensor(16.854), tensor(14.3...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[262, 734, 338, 36257, 329, 685, 1169, 60, 124...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>1‒Louie Marquez: Two Places, One Idea, One Ars...</td>\n",
       "      <td>[198, 198, 16, 447, 240, 24016, 494, 1526, 222...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n1‒Louie Marquez: Two Places, One Idea, One...</td>\n",
       "      <td>([[tensor(19.351), tensor(13.604), tensor(13.0...</td>\n",
       "      <td>([[tensor(14.311), tensor(13.870), tensor(13.6...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[198, 198, 16, 447, 240, 24016, 494, 1526, 222...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>2 stage work for your wood turner. Accepts any...</td>\n",
       "      <td>[362, 3800, 670, 329, 534, 4898, 1210, 263, 13...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 stage work for your wood turner. Accepts an...</td>\n",
       "      <td>([[tensor(16.245), tensor(15.272), tensor(15.2...</td>\n",
       "      <td>([[tensor(15.533), tensor(15.387), tensor(15.1...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[362, 3800, 670, 329, 534, 4898, 1210, 263, 13...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>songstress eternal warchief, marshal of the fi...</td>\n",
       "      <td>[34050, 41494, 15851, 266, 998, 2086, 11, 2239...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>songstress eternal warchief, marshal of the fi...</td>\n",
       "      <td>([[tensor(12.073), tensor(12.030), tensor(11.8...</td>\n",
       "      <td>([[tensor(12.405), tensor(12.184), tensor(12.1...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[34050, 41494, 15851, 266, 998, 2086, 11, 2239...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>With BitGum pattern. http://ponywings.wikia.co...</td>\n",
       "      <td>[3152, 4722, 38, 388, 3912, 13, 2638, 1378, 79...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>With BitGum pattern. http://ponywings.wikia.co...</td>\n",
       "      <td>([[tensor(17.890), tensor(17.349), tensor(15.2...</td>\n",
       "      <td>([[tensor(18.075), tensor(17.391), tensor(15.5...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[3152, 4722, 38, 388, 3912, 13, 2638, 1378, 79...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>big springtime love scene concert), and the si...</td>\n",
       "      <td>[1263, 6076, 2435, 1842, 3715, 10010, 828, 290...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>big springtime love scene concert), and the s...</td>\n",
       "      <td>([[tensor(16.558), tensor(16.515), tensor(14.6...</td>\n",
       "      <td>([[tensor(16.372), tensor(16.310), tensor(15.2...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[1263, 6076, 2435, 1842, 3715, 10010, 828, 290...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>were dismissed as irrelevant.\\n\\n'Sex' indeed ...</td>\n",
       "      <td>[547, 11126, 355, 18046, 13, 198, 198, 6, 2339...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>were dismissed as irrelevant.\\n\\n'Sex' indeed...</td>\n",
       "      <td>([[tensor(14.845), tensor(13.927), tensor(13.8...</td>\n",
       "      <td>([[tensor(13.755), tensor(13.415), tensor(12.5...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[547, 11126, 355, 18046, 13, 198, 198, 6, 2339...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>it, in our lifetime we will at least feel sham...</td>\n",
       "      <td>[340, 11, 287, 674, 10869, 356, 481, 379, 1551...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it, in our lifetime we will at least feel sha...</td>\n",
       "      <td>([[tensor(19.653), tensor(17.600), tensor(17.1...</td>\n",
       "      <td>([[tensor(19.784), tensor(17.574), tensor(17.2...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[340, 11, 287, 674, 10869, 356, 481, 379, 1551...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>cry from'slight chance'\\n\\n\"During this time, ...</td>\n",
       "      <td>[3960, 422, 705, 82, 2971, 2863, 6, 198, 198, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cry from'slight chance'\\n\\n\"During this time,...</td>\n",
       "      <td>([[tensor(13.046), tensor(12.477), tensor(11.4...</td>\n",
       "      <td>([[tensor(15.004), tensor(14.151), tensor(12.1...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[3960, 422, 338, 2971, 2863, 6, 198, 198, 1, 7...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1324</th>\n",
       "      <td>Palmer's Palmer United party's application to ...</td>\n",
       "      <td>[18918, 338, 18918, 1578, 2151, 338, 3586, 284...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Palmer's Palmer United party's application to...</td>\n",
       "      <td>([[tensor(15.700), tensor(15.111), tensor(14.9...</td>\n",
       "      <td>([[tensor(15.384), tensor(15.065), tensor(15.0...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[18918, 338, 18918, 1578, 2151, 338, 3586, 284...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>duty and he [complained]‪ [he] got a.‪ ۰.‪It w...</td>\n",
       "      <td>[7077, 290, 339, 685, 23855, 1328, 60, 447, 10...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>duty and he [complained]‪ [he] got a.‪ ۰.‪It ...</td>\n",
       "      <td>([[tensor(14.023), tensor(13.735), tensor(12.6...</td>\n",
       "      <td>([[tensor(15.398), tensor(14.288), tensor(13.4...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[7077, 290, 339, 685, 23855, 1328, 60, 447, 10...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>bills‧after attending a gala for a doctor ‗ ‏\"...</td>\n",
       "      <td>[9024, 447, 100, 8499, 11969, 257, 308, 6081, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bills‧after attending a gala for a doctor ‗ ‏...</td>\n",
       "      <td>([[tensor(15.555), tensor(13.962), tensor(13.1...</td>\n",
       "      <td>([[tensor(16.102), tensor(15.238), tensor(15.0...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[9024, 447, 100, 8499, 11969, 257, 308, 6081, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>tab so he went on a ride-along with his friend...</td>\n",
       "      <td>[7400, 523, 339, 1816, 319, 257, 6594, 12, 241...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tab so he went on a ride-along with his frien...</td>\n",
       "      <td>([[tensor(24.700), tensor(18.974), tensor(18.6...</td>\n",
       "      <td>([[tensor(24.829), tensor(19.166), tensor(18.8...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[7400, 523, 339, 1816, 319, 257, 6594, 12, 241...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>statewide top lobbying firm in lobbying prowes...</td>\n",
       "      <td>[1181, 4421, 1353, 17502, 4081, 287, 17502, 30...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>statewide top lobbying firm in lobbying prowe...</td>\n",
       "      <td>([[tensor(15.647), tensor(14.337), tensor(14.1...</td>\n",
       "      <td>([[tensor(18.818), tensor(16.229), tensor(15.3...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[23605, 1353, 17502, 4081, 287, 17502, 30721, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>Timbers MVP, and longtime Peace Corps Voluntee...</td>\n",
       "      <td>[40200, 12742, 11, 290, 890, 2435, 12689, 1277...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Timbers MVP, and longtime Peace Corps Volunte...</td>\n",
       "      <td>([[tensor(14.138), tensor(13.967), tensor(13.8...</td>\n",
       "      <td>([[tensor(13.748), tensor(13.232), tensor(12.9...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[40200, 12742, 11, 290, 15076, 12689, 12778, 4...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>Timbers midfielder Darlington Nagbe, who has p...</td>\n",
       "      <td>[40200, 18707, 7491, 17299, 15196, 1350, 11, 5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Timbers midfielder Darlington Nagbe, who has ...</td>\n",
       "      <td>([[tensor(20.052), tensor(13.299), tensor(13.0...</td>\n",
       "      <td>([[tensor(17.757), tensor(17.640), tensor(16.7...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[40200, 18707, 7491, 17299, 15196, 1350, 11, 5...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>depends upon mechanical theology to justify ev...</td>\n",
       "      <td>[8338, 2402, 12370, 262, 1435, 284, 12051, 168...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>depends upon mechanical theology to justify e...</td>\n",
       "      <td>([[tensor(14.847), tensor(14.139), tensor(13.9...</td>\n",
       "      <td>([[tensor(14.328), tensor(13.793), tensor(13.7...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[8338, 2402, 12370, 24383, 284, 12051, 1683, 5...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>all the authors must be gone by then, and the ...</td>\n",
       "      <td>[477, 262, 7035, 1276, 307, 3750, 416, 788, 11...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all the authors must be gone by then, and the...</td>\n",
       "      <td>([[tensor(18.973), tensor(18.179), tensor(15.8...</td>\n",
       "      <td>([[tensor(21.199), tensor(17.900), tensor(17.4...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[477, 262, 7035, 1276, 307, 3750, 416, 788, 11...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>rest of the  \"towed\", held it upright into the...</td>\n",
       "      <td>[1334, 286, 262, 1849, 366, 83, 6972, 1, 837, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rest of the  \"towed\", held it upright into th...</td>\n",
       "      <td>([[tensor(16.591), tensor(16.350), tensor(15.5...</td>\n",
       "      <td>([[tensor(16.039), tensor(14.847), tensor(14.6...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[1334, 286, 262, 1849, 366, 83, 6972, 1600, 27...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>other game did not include. See how much the s...</td>\n",
       "      <td>[584, 983, 750, 407, 2291, 13, 4091, 703, 881,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other game did not include. See how much the ...</td>\n",
       "      <td>([[tensor(14.161), tensor(13.735), tensor(13.4...</td>\n",
       "      <td>([[tensor(22.111), tensor(21.115), tensor(18.7...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[584, 983, 750, 407, 2291, 13, 4091, 703, 881,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>Nation Act,. It was unanimously passed, the Co...</td>\n",
       "      <td>[8741, 2191, 11, 13, 632, 373, 28355, 3804, 11...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nation Act,. It was unanimously passed, the C...</td>\n",
       "      <td>([[tensor(14.069), tensor(13.848), tensor(13.8...</td>\n",
       "      <td>([[tensor(15.360), tensor(14.080), tensor(13.8...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[8741, 2191, 38508, 632, 373, 28355, 3804, 11,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>same type of product in stores (see SSGN, frib...</td>\n",
       "      <td>[976, 2099, 286, 1720, 287, 7000, 357, 3826, 6...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>same type of product in stores (see SSGN, fri...</td>\n",
       "      <td>([[tensor(18.182), tensor(17.094), tensor(16.8...</td>\n",
       "      <td>([[tensor(15.093), tensor(13.595), tensor(13.5...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[976, 2099, 286, 1720, 287, 7000, 357, 3826, 6...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>-out or manual application of the required pat...</td>\n",
       "      <td>[12, 448, 393, 10107, 3586, 286, 262, 2672, 16...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-out or manual application of the required pat...</td>\n",
       "      <td>([[tensor(14.005), tensor(11.973), tensor(11.8...</td>\n",
       "      <td>([[tensor(20.644), tensor(20.271), tensor(20.1...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[12, 448, 393, 10107, 3586, 286, 262, 2672, 16...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2052</th>\n",
       "      <td>Wal-Mart... unless it is a Starbucks or Pepsi ...</td>\n",
       "      <td>[6445, 12, 13143, 764, 764, 764, 4556, 340, 31...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wal-Mart... unless it is a Starbucks or Pepsi...</td>\n",
       "      <td>([[tensor(15.185), tensor(15.043), tensor(14.7...</td>\n",
       "      <td>([[tensor(14.788), tensor(14.468), tensor(14.1...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[6445, 12, 13143, 986, 4556, 340, 318, 257, 24...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>[Blackwell]?\"\\n\\nAlthough the crew took a weir...</td>\n",
       "      <td>[685, 9915, 4053, 60, 1701, 198, 198, 7003, 26...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Blackwell]?\"\\n\\nAlthough the crew took a wei...</td>\n",
       "      <td>([[tensor(23.706), tensor(21.519), tensor(20.2...</td>\n",
       "      <td>([[tensor(24.051), tensor(21.903), tensor(20.4...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[685, 9915, 4053, 60, 1701, 198, 198, 7003, 26...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>a pattern of behavior quite as sharp, with the...</td>\n",
       "      <td>[257, 3912, 286, 4069, 2407, 355, 7786, 11, 35...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a pattern of behavior quite as sharp, with th...</td>\n",
       "      <td>([[tensor(19.332), tensor(16.315), tensor(16.1...</td>\n",
       "      <td>([[tensor(20.281), tensor(16.995), tensor(16.7...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[257, 3912, 286, 4069, 2407, 355, 7786, 11, 35...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2208</th>\n",
       "      <td>rich is a bit like being told you're a liar, o...</td>\n",
       "      <td>[5527, 318, 257, 1643, 588, 852, 1297, 345, 82...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rich is a bit like being told you're a liar, ...</td>\n",
       "      <td>([[tensor(15.503), tensor(14.682), tensor(14.4...</td>\n",
       "      <td>([[tensor(17.630), tensor(16.460), tensor(16.2...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[5527, 318, 257, 1643, 588, 852, 1297, 345, 82...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>good, yes I'm not proud of it but I am proud o...</td>\n",
       "      <td>[922, 11, 3763, 314, 1101, 407, 6613, 286, 340...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>good, yes I'm not proud of it but I am proud ...</td>\n",
       "      <td>([[tensor(20.502), tensor(19.156), tensor(18.2...</td>\n",
       "      <td>([[tensor(18.766), tensor(17.934), tensor(17.7...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[922, 11, 3763, 314, 1101, 407, 6613, 286, 340...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "13    offering the serial murderer Birgitta Jónsdótt...   \n",
       "25    look‼️‼️‼️At everyone. And in’a perfect world‼...   \n",
       "58    but um...Chicago because you don't give a fuck...   \n",
       "62    with terms like \"fat pigs, dogs, slobs and dis...   \n",
       "120   him Watson because this game is in the news, s...   \n",
       "128                                     him Mi Joo-Kim.   \n",
       "145      pie, cupcake...Can't make it if I do. Video...   \n",
       "153   to have the ungodly activity of a fry making c...   \n",
       "187   leading a rally at the Indiana Statehouse, bas...   \n",
       "274   will either be whine or familiar moaning‖.no a...   \n",
       "354   and you'd better wait,‬“because we’ll be waiti...   \n",
       "372   from Knoxville and the state of Tennessee ANTI...   \n",
       "373   'stalking radicals.' 'Not a movement. A David'...   \n",
       "409   using a fish hook to perform sex on a young wo...   \n",
       "450   gibbering lunatics, convinced that the Russian...   \n",
       "471   ***?. He was here purely to get money for food...   \n",
       "483                                     Die’’’’’’’’’’’�   \n",
       "485   Drive on America with Immediate Remove≠Toilet ...   \n",
       "508   14 and 16, on the girls' behalf to have sex wi...   \n",
       "560   a girlie boys girl here think of her different...   \n",
       "611                      over like this and he was f...   \n",
       "760   wealthy individuals who want to censor free sp...   \n",
       "794   than you, Mrs Oni and grandchildren being atta...   \n",
       "795   than this brave teenage girl named Jillian?\" H...   \n",
       "825                    the two'stones for [the] theft'.   \n",
       "891   1‒Louie Marquez: Two Places, One Idea, One Ars...   \n",
       "902   2 stage work for your wood turner. Accepts any...   \n",
       "911   songstress eternal warchief, marshal of the fi...   \n",
       "916   With BitGum pattern. http://ponywings.wikia.co...   \n",
       "962   big springtime love scene concert), and the si...   \n",
       "1037  were dismissed as irrelevant.\\n\\n'Sex' indeed ...   \n",
       "1048  it, in our lifetime we will at least feel sham...   \n",
       "1221  cry from'slight chance'\\n\\n\"During this time, ...   \n",
       "1324  Palmer's Palmer United party's application to ...   \n",
       "1423  duty and he [complained]‪ [he] got a.‪ ۰.‪It w...   \n",
       "1426  bills‧after attending a gala for a doctor ‗ ‏\"...   \n",
       "1428  tab so he went on a ride-along with his friend...   \n",
       "1459  statewide top lobbying firm in lobbying prowes...   \n",
       "1461  Timbers MVP, and longtime Peace Corps Voluntee...   \n",
       "1469  Timbers midfielder Darlington Nagbe, who has p...   \n",
       "1735  depends upon mechanical theology to justify ev...   \n",
       "1769  all the authors must be gone by then, and the ...   \n",
       "1804  rest of the  \"towed\", held it upright into the...   \n",
       "1877  other game did not include. See how much the s...   \n",
       "1917  Nation Act,. It was unanimously passed, the Co...   \n",
       "1923  same type of product in stores (see SSGN, frib...   \n",
       "2033  -out or manual application of the required pat...   \n",
       "2052  Wal-Mart... unless it is a Starbucks or Pepsi ...   \n",
       "2089  [Blackwell]?\"\\n\\nAlthough the crew took a weir...   \n",
       "2150  a pattern of behavior quite as sharp, with the...   \n",
       "2208  rich is a bit like being told you're a liar, o...   \n",
       "2209  good, yes I'm not proud of it but I am proud o...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "13    [6011, 262, 11389, 29262, 12817, 70, 48519, 44...   \n",
       "25    [804, 447, 120, 37929, 447, 120, 37929, 447, 1...   \n",
       "58    [475, 23781, 986, 25705, 780, 345, 836, 470, 1...   \n",
       "62    [351, 2846, 588, 366, 17359, 22333, 11, 6844, ...   \n",
       "120   [683, 14959, 780, 428, 983, 318, 287, 262, 170...   \n",
       "128            [683, 13756, 449, 2238, 12, 42, 320, 13]   \n",
       "145   [2508, 11, 6508, 30560, 986, 6090, 470, 787, 3...   \n",
       "153   [284, 423, 262, 555, 25344, 306, 3842, 286, 25...   \n",
       "187   [3756, 257, 7903, 379, 262, 10278, 1812, 4803,...   \n",
       "274   [481, 2035, 307, 47409, 393, 5385, 47644, 447,...   \n",
       "354   [392, 345, 1549, 1365, 4043, 11, 447, 105, 447...   \n",
       "372   [422, 27633, 4244, 290, 262, 1181, 286, 11287,...   \n",
       "373   [705, 301, 18998, 32842, 2637, 705, 3673, 257,...   \n",
       "409   [1262, 257, 5916, 8011, 284, 1620, 1714, 319, ...   \n",
       "450   [46795, 527, 278, 14678, 23372, 11, 9431, 326,...   \n",
       "471   [17202, 5633, 13, 679, 373, 994, 14177, 284, 6...   \n",
       "483   [6733, 447, 247, 447, 247, 447, 247, 447, 247,...   \n",
       "485   [9974, 319, 2253, 351, 1846, 13857, 17220, 357...   \n",
       "508   [1478, 290, 1467, 11, 319, 262, 4813, 6, 8378,...   \n",
       "560   [257, 2576, 494, 6510, 2576, 994, 892, 286, 60...   \n",
       "611   [625, 588, 428, 290, 339, 373, 277, 13, 764, 764]   \n",
       "760   [11574, 3925, 508, 765, 284, 42081, 1479, 4046...   \n",
       "794   [621, 345, 11, 9074, 1550, 72, 290, 28986, 852...   \n",
       "795   [621, 428, 14802, 16330, 2576, 3706, 22772, 66...   \n",
       "825   [262, 734, 705, 28750, 329, 685, 1169, 60, 124...   \n",
       "891   [198, 198, 16, 447, 240, 24016, 494, 1526, 222...   \n",
       "902   [362, 3800, 670, 329, 534, 4898, 1210, 263, 13...   \n",
       "911   [34050, 41494, 15851, 266, 998, 2086, 11, 2239...   \n",
       "916   [3152, 4722, 38, 388, 3912, 13, 2638, 1378, 79...   \n",
       "962   [1263, 6076, 2435, 1842, 3715, 10010, 828, 290...   \n",
       "1037  [547, 11126, 355, 18046, 13, 198, 198, 6, 2339...   \n",
       "1048  [340, 11, 287, 674, 10869, 356, 481, 379, 1551...   \n",
       "1221  [3960, 422, 705, 82, 2971, 2863, 6, 198, 198, ...   \n",
       "1324  [18918, 338, 18918, 1578, 2151, 338, 3586, 284...   \n",
       "1423  [7077, 290, 339, 685, 23855, 1328, 60, 447, 10...   \n",
       "1426  [9024, 447, 100, 8499, 11969, 257, 308, 6081, ...   \n",
       "1428  [7400, 523, 339, 1816, 319, 257, 6594, 12, 241...   \n",
       "1459  [1181, 4421, 1353, 17502, 4081, 287, 17502, 30...   \n",
       "1461  [40200, 12742, 11, 290, 890, 2435, 12689, 1277...   \n",
       "1469  [40200, 18707, 7491, 17299, 15196, 1350, 11, 5...   \n",
       "1735  [8338, 2402, 12370, 262, 1435, 284, 12051, 168...   \n",
       "1769  [477, 262, 7035, 1276, 307, 3750, 416, 788, 11...   \n",
       "1804  [1334, 286, 262, 1849, 366, 83, 6972, 1, 837, ...   \n",
       "1877  [584, 983, 750, 407, 2291, 13, 4091, 703, 881,...   \n",
       "1917  [8741, 2191, 11, 13, 632, 373, 28355, 3804, 11...   \n",
       "1923  [976, 2099, 286, 1720, 287, 7000, 357, 3826, 6...   \n",
       "2033  [12, 448, 393, 10107, 3586, 286, 262, 2672, 16...   \n",
       "2052  [6445, 12, 13143, 764, 764, 764, 4556, 340, 31...   \n",
       "2089  [685, 9915, 4053, 60, 1701, 198, 198, 7003, 26...   \n",
       "2150  [257, 3912, 286, 4069, 2407, 355, 7786, 11, 35...   \n",
       "2208  [5527, 318, 257, 1643, 588, 852, 1297, 345, 82...   \n",
       "2209  [922, 11, 3763, 314, 1101, 407, 6613, 286, 340...   \n",
       "\n",
       "                                          locate_labels  \\\n",
       "13                                                  NaN   \n",
       "25                                                  NaN   \n",
       "58                                                  NaN   \n",
       "62                                                  NaN   \n",
       "120                                                 NaN   \n",
       "128                                                 NaN   \n",
       "145                                                 NaN   \n",
       "153                                                 NaN   \n",
       "187                                                 NaN   \n",
       "274                                                 NaN   \n",
       "354                                                 NaN   \n",
       "372                                                 NaN   \n",
       "373                                                 NaN   \n",
       "409   [0, 0, 0, 0, 0, 0.5, 1, 0.5, 0.5, 0.5, 0.5, 0,...   \n",
       "450   [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "471                                                 NaN   \n",
       "483                                                 NaN   \n",
       "485                                                 NaN   \n",
       "508                                                 NaN   \n",
       "560                                                 NaN   \n",
       "611                                                 NaN   \n",
       "760                                                 NaN   \n",
       "794                                                 NaN   \n",
       "795                                                 NaN   \n",
       "825                                                 NaN   \n",
       "891                                                 NaN   \n",
       "902                                                 NaN   \n",
       "911                                                 NaN   \n",
       "916                                                 NaN   \n",
       "962                                                 NaN   \n",
       "1037                                                NaN   \n",
       "1048                                                NaN   \n",
       "1221                                                NaN   \n",
       "1324                                                NaN   \n",
       "1423                                                NaN   \n",
       "1426                                                NaN   \n",
       "1428                                                NaN   \n",
       "1459                                                NaN   \n",
       "1461                                                NaN   \n",
       "1469                                                NaN   \n",
       "1735                                                NaN   \n",
       "1769                                                NaN   \n",
       "1804                                                NaN   \n",
       "1877                                                NaN   \n",
       "1917                                                NaN   \n",
       "1923                                                NaN   \n",
       "2033                                                NaN   \n",
       "2052                                                NaN   \n",
       "2089                                                NaN   \n",
       "2150                                                NaN   \n",
       "2208                                                NaN   \n",
       "2209                                                NaN   \n",
       "\n",
       "                                             tokens_dec  \\\n",
       "13     offering the serial murderer Birgitta Jónsdót...   \n",
       "25     look‼️‼️‼️At everyone. And in’a perfect world...   \n",
       "58     but um...Chicago because you don't give a fuc...   \n",
       "62     with terms like \"fat pigs, dogs, slobs and di...   \n",
       "120    him Watson because this game is in the news, ...   \n",
       "128                                     him Mi Joo-Kim.   \n",
       "145      pie, cupcake...Can't make it if I do. Video...   \n",
       "153    to have the ungodly activity of a fry making ...   \n",
       "187    leading a rally at the Indiana Statehouse, ba...   \n",
       "274    will either be whine or familiar moaning‖.no ...   \n",
       "354   and you'd better wait,‬“because we’ll be waiti...   \n",
       "372    from Knoxville and the state of Tennessee ANT...   \n",
       "373   'stalking radicals.' 'Not a movement. A David'...   \n",
       "409    using a fish hook to perform sex on a young w...   \n",
       "450    gibbering lunatics, convinced that the Russia...   \n",
       "471    ***?. He was here purely to get money for foo...   \n",
       "483                                     Die’’’’’’’’’’’�   \n",
       "485    Drive on America with Immediate Remove≠Toilet...   \n",
       "508    14 and 16, on the girls' behalf to have sex w...   \n",
       "560    a girlie boys girl here think of her differen...   \n",
       "611                      over like this and he was f...   \n",
       "760    wealthy individuals who want to censor free s...   \n",
       "794    than you, Mrs Oni and grandchildren being att...   \n",
       "795    than this brave teenage girl named Jillian?\" ...   \n",
       "825                    the two'stones for [the] theft'.   \n",
       "891   \\n\\n1‒Louie Marquez: Two Places, One Idea, One...   \n",
       "902    2 stage work for your wood turner. Accepts an...   \n",
       "911   songstress eternal warchief, marshal of the fi...   \n",
       "916   With BitGum pattern. http://ponywings.wikia.co...   \n",
       "962    big springtime love scene concert), and the s...   \n",
       "1037   were dismissed as irrelevant.\\n\\n'Sex' indeed...   \n",
       "1048   it, in our lifetime we will at least feel sha...   \n",
       "1221   cry from'slight chance'\\n\\n\"During this time,...   \n",
       "1324   Palmer's Palmer United party's application to...   \n",
       "1423   duty and he [complained]‪ [he] got a.‪ ۰.‪It ...   \n",
       "1426   bills‧after attending a gala for a doctor ‗ ‏...   \n",
       "1428   tab so he went on a ride-along with his frien...   \n",
       "1459   statewide top lobbying firm in lobbying prowe...   \n",
       "1461   Timbers MVP, and longtime Peace Corps Volunte...   \n",
       "1469   Timbers midfielder Darlington Nagbe, who has ...   \n",
       "1735   depends upon mechanical theology to justify e...   \n",
       "1769   all the authors must be gone by then, and the...   \n",
       "1804   rest of the  \"towed\", held it upright into th...   \n",
       "1877   other game did not include. See how much the ...   \n",
       "1917   Nation Act,. It was unanimously passed, the C...   \n",
       "1923   same type of product in stores (see SSGN, fri...   \n",
       "2033  -out or manual application of the required pat...   \n",
       "2052   Wal-Mart... unless it is a Starbucks or Pepsi...   \n",
       "2089   [Blackwell]?\"\\n\\nAlthough the crew took a wei...   \n",
       "2150   a pattern of behavior quite as sharp, with th...   \n",
       "2208   rich is a bit like being told you're a liar, ...   \n",
       "2209   good, yes I'm not proud of it but I am proud ...   \n",
       "\n",
       "                                  old_candidates_result  \\\n",
       "13    ([[tensor(18.345), tensor(17.951), tensor(17.5...   \n",
       "25    ([[tensor(17.685), tensor(12.269), tensor(11.5...   \n",
       "58    ([[tensor(15.334), tensor(13.658), tensor(13.1...   \n",
       "62    ([[tensor(17.486), tensor(17.077), tensor(16.3...   \n",
       "120   ([[tensor(16.510), tensor(16.376), tensor(15.7...   \n",
       "128   ([[tensor(13.968), tensor(13.820), tensor(13.0...   \n",
       "145   ([[tensor(15.152), tensor(15.124), tensor(15.0...   \n",
       "153   ([[tensor(14.949), tensor(14.805), tensor(14.5...   \n",
       "187   ([[tensor(16.935), tensor(15.568), tensor(15.2...   \n",
       "274   ([[tensor(14.669), tensor(13.161), tensor(12.8...   \n",
       "354   ([[tensor(13.787), tensor(13.454), tensor(11.1...   \n",
       "372   ([[tensor(26.556), tensor(18.387), tensor(17.4...   \n",
       "373   ([[tensor(19.185), tensor(19.175), tensor(19.1...   \n",
       "409   ([[tensor(18.326), tensor(16.673), tensor(16.0...   \n",
       "450   ([[tensor(23.616), tensor(23.206), tensor(22.7...   \n",
       "471   ([[tensor(15.578), tensor(15.041), tensor(14.9...   \n",
       "483   ([[tensor(18.742), tensor(17.814), tensor(16.4...   \n",
       "485   ([[tensor(16.768), tensor(16.681), tensor(16.3...   \n",
       "508   ([[tensor(13.849), tensor(13.701), tensor(13.1...   \n",
       "560   ([[tensor(14.040), tensor(13.850), tensor(13.7...   \n",
       "611   ([[tensor(11.539), tensor(10.413), tensor(10.0...   \n",
       "760   ([[tensor(15.969), tensor(14.063), tensor(14.0...   \n",
       "794   ([[tensor(15.202), tensor(15.028), tensor(14.7...   \n",
       "795   ([[tensor(23.630), tensor(18.627), tensor(18.5...   \n",
       "825   ([[tensor(16.713), tensor(14.780), tensor(14.3...   \n",
       "891   ([[tensor(19.351), tensor(13.604), tensor(13.0...   \n",
       "902   ([[tensor(16.245), tensor(15.272), tensor(15.2...   \n",
       "911   ([[tensor(12.073), tensor(12.030), tensor(11.8...   \n",
       "916   ([[tensor(17.890), tensor(17.349), tensor(15.2...   \n",
       "962   ([[tensor(16.558), tensor(16.515), tensor(14.6...   \n",
       "1037  ([[tensor(14.845), tensor(13.927), tensor(13.8...   \n",
       "1048  ([[tensor(19.653), tensor(17.600), tensor(17.1...   \n",
       "1221  ([[tensor(13.046), tensor(12.477), tensor(11.4...   \n",
       "1324  ([[tensor(15.700), tensor(15.111), tensor(14.9...   \n",
       "1423  ([[tensor(14.023), tensor(13.735), tensor(12.6...   \n",
       "1426  ([[tensor(15.555), tensor(13.962), tensor(13.1...   \n",
       "1428  ([[tensor(24.700), tensor(18.974), tensor(18.6...   \n",
       "1459  ([[tensor(15.647), tensor(14.337), tensor(14.1...   \n",
       "1461  ([[tensor(14.138), tensor(13.967), tensor(13.8...   \n",
       "1469  ([[tensor(20.052), tensor(13.299), tensor(13.0...   \n",
       "1735  ([[tensor(14.847), tensor(14.139), tensor(13.9...   \n",
       "1769  ([[tensor(18.973), tensor(18.179), tensor(15.8...   \n",
       "1804  ([[tensor(16.591), tensor(16.350), tensor(15.5...   \n",
       "1877  ([[tensor(14.161), tensor(13.735), tensor(13.4...   \n",
       "1917  ([[tensor(14.069), tensor(13.848), tensor(13.8...   \n",
       "1923  ([[tensor(18.182), tensor(17.094), tensor(16.8...   \n",
       "2033  ([[tensor(14.005), tensor(11.973), tensor(11.8...   \n",
       "2052  ([[tensor(15.185), tensor(15.043), tensor(14.7...   \n",
       "2089  ([[tensor(23.706), tensor(21.519), tensor(20.2...   \n",
       "2150  ([[tensor(19.332), tensor(16.315), tensor(16.1...   \n",
       "2208  ([[tensor(15.503), tensor(14.682), tensor(14.4...   \n",
       "2209  ([[tensor(20.502), tensor(19.156), tensor(18.2...   \n",
       "\n",
       "                                  new_candidates_result  compare1  compare2  \\\n",
       "13    ([[tensor(18.256), tensor(17.320), tensor(17.1...     False     False   \n",
       "25    ([[tensor(16.100), tensor(14.455), tensor(14.1...     False     False   \n",
       "58    ([[tensor(14.829), tensor(14.478), tensor(14.0...     False     False   \n",
       "62    ([[tensor(16.689), tensor(16.604), tensor(16.3...     False     False   \n",
       "120   ([[tensor(16.146), tensor(16.003), tensor(15.5...     False     False   \n",
       "128   ([[tensor(13.574), tensor(13.474), tensor(12.7...     False     False   \n",
       "145   ([[tensor(14.660), tensor(14.653), tensor(14.5...     False     False   \n",
       "153   ([[tensor(15.842), tensor(15.277), tensor(14.9...     False     False   \n",
       "187   ([[tensor(15.245), tensor(14.623), tensor(14.5...     False     False   \n",
       "274   ([[tensor(15.046), tensor(13.645), tensor(12.8...     False     False   \n",
       "354   ([[tensor(16.605), tensor(14.510), tensor(12.4...     False     False   \n",
       "372   ([[tensor(26.780), tensor(18.584), tensor(17.7...     False     False   \n",
       "373   ([[tensor(18.250), tensor(16.230), tensor(15.7...     False     False   \n",
       "409   ([[tensor(17.991), tensor(16.332), tensor(16.2...     False     False   \n",
       "450   ([[tensor(23.580), tensor(22.775), tensor(22.7...     False     False   \n",
       "471   ([[tensor(15.285), tensor(14.881), tensor(14.8...     False     False   \n",
       "483   ([[tensor(18.437), tensor(17.767), tensor(15.3...     False     False   \n",
       "485   ([[tensor(16.569), tensor(16.444), tensor(16.3...     False     False   \n",
       "508   ([[tensor(23.370), tensor(20.690), tensor(19.6...     False     False   \n",
       "560   ([[tensor(14.107), tensor(14.032), tensor(13.6...     False     False   \n",
       "611   ([[tensor(10.033), tensor(9.614), tensor(9.431...     False     False   \n",
       "760   ([[tensor(15.258), tensor(15.115), tensor(14.4...     False     False   \n",
       "794   ([[tensor(15.783), tensor(15.322), tensor(14.9...     False     False   \n",
       "795   ([[tensor(23.289), tensor(18.985), tensor(18.3...     False     False   \n",
       "825   ([[tensor(17.254), tensor(16.854), tensor(14.3...     False     False   \n",
       "891   ([[tensor(14.311), tensor(13.870), tensor(13.6...     False     False   \n",
       "902   ([[tensor(15.533), tensor(15.387), tensor(15.1...     False     False   \n",
       "911   ([[tensor(12.405), tensor(12.184), tensor(12.1...     False     False   \n",
       "916   ([[tensor(18.075), tensor(17.391), tensor(15.5...     False     False   \n",
       "962   ([[tensor(16.372), tensor(16.310), tensor(15.2...     False     False   \n",
       "1037  ([[tensor(13.755), tensor(13.415), tensor(12.5...     False     False   \n",
       "1048  ([[tensor(19.784), tensor(17.574), tensor(17.2...     False     False   \n",
       "1221  ([[tensor(15.004), tensor(14.151), tensor(12.1...     False     False   \n",
       "1324  ([[tensor(15.384), tensor(15.065), tensor(15.0...     False     False   \n",
       "1423  ([[tensor(15.398), tensor(14.288), tensor(13.4...     False     False   \n",
       "1426  ([[tensor(16.102), tensor(15.238), tensor(15.0...     False     False   \n",
       "1428  ([[tensor(24.829), tensor(19.166), tensor(18.8...     False     False   \n",
       "1459  ([[tensor(18.818), tensor(16.229), tensor(15.3...     False     False   \n",
       "1461  ([[tensor(13.748), tensor(13.232), tensor(12.9...     False     False   \n",
       "1469  ([[tensor(17.757), tensor(17.640), tensor(16.7...     False     False   \n",
       "1735  ([[tensor(14.328), tensor(13.793), tensor(13.7...     False     False   \n",
       "1769  ([[tensor(21.199), tensor(17.900), tensor(17.4...     False     False   \n",
       "1804  ([[tensor(16.039), tensor(14.847), tensor(14.6...     False     False   \n",
       "1877  ([[tensor(22.111), tensor(21.115), tensor(18.7...     False     False   \n",
       "1917  ([[tensor(15.360), tensor(14.080), tensor(13.8...     False     False   \n",
       "1923  ([[tensor(15.093), tensor(13.595), tensor(13.5...     False     False   \n",
       "2033  ([[tensor(20.644), tensor(20.271), tensor(20.1...     False     False   \n",
       "2052  ([[tensor(14.788), tensor(14.468), tensor(14.1...     False     False   \n",
       "2089  ([[tensor(24.051), tensor(21.903), tensor(20.4...     False     False   \n",
       "2150  ([[tensor(20.281), tensor(16.995), tensor(16.7...     False     False   \n",
       "2208  ([[tensor(17.630), tensor(16.460), tensor(16.2...     False     False   \n",
       "2209  ([[tensor(18.766), tensor(17.934), tensor(17.7...     False     False   \n",
       "\n",
       "                                         tokens_dec_enc  dec_enc_eq_tokens  \n",
       "13    [6011, 262, 11389, 29262, 12817, 70, 48519, 44...              False  \n",
       "25    [804, 447, 120, 37929, 447, 120, 37929, 447, 1...              False  \n",
       "58    [475, 23781, 986, 25705, 780, 345, 836, 470, 1...              False  \n",
       "62    [351, 2846, 588, 366, 17359, 22333, 11, 6844, ...              False  \n",
       "120   [683, 14959, 780, 428, 983, 318, 287, 262, 170...              False  \n",
       "128              [683, 13756, 449, 2238, 12, 26374, 13]              False  \n",
       "145   [2508, 11, 6508, 30560, 986, 6090, 470, 787, 3...              False  \n",
       "153   [284, 423, 262, 555, 25344, 306, 3842, 286, 25...              False  \n",
       "187   [3756, 257, 7903, 379, 262, 10278, 1812, 4803,...              False  \n",
       "274   [481, 2035, 307, 47409, 393, 5385, 47644, 447,...              False  \n",
       "354   [392, 345, 1549, 1365, 4043, 11, 447, 105, 447...              False  \n",
       "372   [422, 27633, 4244, 290, 262, 1181, 286, 11287,...              False  \n",
       "373   [338, 48186, 32842, 2637, 705, 3673, 257, 3356...              False  \n",
       "409   [1262, 257, 5916, 8011, 284, 1620, 1714, 319, ...              False  \n",
       "450   [46795, 527, 278, 14678, 23372, 11, 9431, 326,...              False  \n",
       "471   [17202, 30, 13, 679, 373, 994, 14177, 284, 651...              False  \n",
       "483   [6733, 447, 247, 447, 247, 447, 247, 447, 247,...              False  \n",
       "485   [9974, 319, 2253, 351, 1846, 13857, 17220, 357...              False  \n",
       "508   [1478, 290, 1467, 11, 319, 262, 4813, 6, 8378,...              False  \n",
       "560   [257, 2576, 494, 6510, 2576, 994, 892, 286, 60...              False  \n",
       "611            [625, 588, 428, 290, 339, 373, 277, 986]              False  \n",
       "760   [11574, 3925, 508, 765, 284, 42081, 1479, 4046...              False  \n",
       "794   [621, 345, 11, 9074, 1550, 72, 290, 28986, 852...              False  \n",
       "795   [621, 428, 14802, 16330, 2576, 3706, 22772, 66...              False  \n",
       "825   [262, 734, 338, 36257, 329, 685, 1169, 60, 124...              False  \n",
       "891   [198, 198, 16, 447, 240, 24016, 494, 1526, 222...              False  \n",
       "902   [362, 3800, 670, 329, 534, 4898, 1210, 263, 13...              False  \n",
       "911   [34050, 41494, 15851, 266, 998, 2086, 11, 2239...              False  \n",
       "916   [3152, 4722, 38, 388, 3912, 13, 2638, 1378, 79...              False  \n",
       "962   [1263, 6076, 2435, 1842, 3715, 10010, 828, 290...              False  \n",
       "1037  [547, 11126, 355, 18046, 13, 198, 198, 6, 2339...              False  \n",
       "1048  [340, 11, 287, 674, 10869, 356, 481, 379, 1551...              False  \n",
       "1221  [3960, 422, 338, 2971, 2863, 6, 198, 198, 1, 7...              False  \n",
       "1324  [18918, 338, 18918, 1578, 2151, 338, 3586, 284...              False  \n",
       "1423  [7077, 290, 339, 685, 23855, 1328, 60, 447, 10...              False  \n",
       "1426  [9024, 447, 100, 8499, 11969, 257, 308, 6081, ...              False  \n",
       "1428  [7400, 523, 339, 1816, 319, 257, 6594, 12, 241...              False  \n",
       "1459  [23605, 1353, 17502, 4081, 287, 17502, 30721, ...              False  \n",
       "1461  [40200, 12742, 11, 290, 15076, 12689, 12778, 4...              False  \n",
       "1469  [40200, 18707, 7491, 17299, 15196, 1350, 11, 5...              False  \n",
       "1735  [8338, 2402, 12370, 24383, 284, 12051, 1683, 5...              False  \n",
       "1769  [477, 262, 7035, 1276, 307, 3750, 416, 788, 11...              False  \n",
       "1804  [1334, 286, 262, 1849, 366, 83, 6972, 1600, 27...              False  \n",
       "1877  [584, 983, 750, 407, 2291, 13, 4091, 703, 881,...              False  \n",
       "1917  [8741, 2191, 38508, 632, 373, 28355, 3804, 11,...              False  \n",
       "1923  [976, 2099, 286, 1720, 287, 7000, 357, 3826, 6...              False  \n",
       "2033  [12, 448, 393, 10107, 3586, 286, 262, 2672, 16...              False  \n",
       "2052  [6445, 12, 13143, 986, 4556, 340, 318, 257, 24...              False  \n",
       "2089  [685, 9915, 4053, 60, 1701, 198, 198, 7003, 26...              False  \n",
       "2150  [257, 3912, 286, 4069, 2407, 355, 7786, 11, 35...              False  \n",
       "2208  [5527, 318, 257, 1643, 588, 852, 1297, 345, 82...              False  \n",
       "2209  [922, 11, 3763, 314, 1101, 407, 6613, 286, 340...              False  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41cefa0-74d6-42fc-8005-d50ab36a34df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29b7430f-ada0-49a4-a604-689ad08a55e2",
   "metadata": {},
   "source": [
    "## Check if those samples with different dec_enc and tokens have significantly lower metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38c39685-8719-48e5-aa7c-44d893f68ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_exp = 'outputs/toxicity/mlm-reranking/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/mlm-beamsearch-v0-word-nps5-k10-beam5-allsat_primary-toxic-to-nontoxic-grad_norm-oc6pto51'\n",
    "target_exp = 'outputs/toxicity/mlm-reranking/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/mlm-beamsearch-v0-word-nps5-k10-beam5-allsat_primary-toxic-to-nontoxic-grad_norm-zc28w2sy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d725f-a110-4faa-a5b8-d7f7b8403d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f4491-c1ea-4d2d-bae5-5296531eaca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ff9ff-b210-49dd-b38a-43cdd4930880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb901b-9158-4f38-ada8-0b04c06b5aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d720f-e08f-41f3-b206-f2b821177b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0924a9b-15bf-4d97-a9c2-03fe674293f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e1c6b7-22a1-40d0-9b06-f916b1ef0776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d43a9a1-0a16-43b9-8fdf-944cb6b6da7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4d821935-1508-4301-b95f-8451a4f236e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input to word2tok: words     [dirt., Unfortunately, the, majority, of, hors...\n",
      "tokens    [13647, 13, 8989, 262, 3741, 286, 14260, 886, ...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "predicted_token_ids2 = old_locate_candidate_generation_fn(predicted_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4ed0eee1-05b9-4cbd-b7f2-227b36ba5cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted_token_ids.values != predicted_token_ids2.values).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f74647f5-dfb1-4319-87be-c572da54db95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted_token_ids.indices != predicted_token_ids2.indices).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9770d414-33cb-4f4c-8c4d-81e6e51df2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c19dd9fe-1bc3-4554-be39-2f260a9cd452",
   "metadata": {},
   "outputs": [],
   "source": [
    "### \"mlm-reranking\"\n",
    "hypotheses = []\n",
    "num_located_tokens = len(indices_in_mlm_tokens)\n",
    "num_all_cases = config[\"k_per_location\"] ** num_located_tokens\n",
    "tok_cand_combo = [0 for i in range(num_located_tokens)]\n",
    "\n",
    "for case_id in range(num_all_cases):\n",
    "    for i in range(num_located_tokens):\n",
    "        tok_cand_combo[i] = (\n",
    "            case_id // (config[\"k_per_location\"] ** i)\n",
    "        ) % config[\"k_per_location\"]\n",
    "\n",
    "    tmp_seq = inputs[\"input_ids\"].clone()\n",
    "    for pos_id, tok_cand_id in enumerate(tok_cand_combo):\n",
    "        tmp_seq[\n",
    "            0, indices_in_mlm_tokens[pos_id]\n",
    "        ] = predicted_token_ids.indices[pos_id, tok_cand_id]\n",
    "\n",
    "    # need to do decode with RobertaTokenizer and encode with GPT2Tokenizer\n",
    "    # logger.debug(mlm_tokenizer.batch_decode(tmp_seq[:, indices_in_mlm_tokens], skip_special_tokens=True))\n",
    "    tmp_dec_seq = mlm_tokenizer.batch_decode(\n",
    "            tmp_seq, skip_special_tokens=True\n",
    "        )\n",
    "    hypotheses.append(tmp_dec_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3741426e-79ad-44a3-af10-900c4560bf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.57 s ± 10.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "hypotheses = constrained_beam_search(source_text,\n",
    "                               inputs.input_ids,\n",
    "                               indices_in_mlm_tokens,\n",
    "                               predicted_token_ids,\n",
    "                               mlm_tokenizer, \n",
    "                               lossfns,\n",
    "                               config, \n",
    "                               beam_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b73165a1-38c2-4eb5-b33e-f5825802314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fn():\n",
    "    beam_size= 5\n",
    "    hypotheses = [torch.LongTensor([]).to(config['device'])]\n",
    "    \n",
    "    masked_sequence = inputs[\"input_ids\"].clone()\n",
    "    L = masked_sequence.size(-1)\n",
    "    \n",
    "    for i in range(L):\n",
    "\n",
    "        if masked_sequence[0, i] != mlm_tokenizer.mask_token_id:\n",
    "            for j in range(len(hypotheses)):\n",
    "                hypotheses[j] = torch.cat([hypotheses[j], masked_sequence[0, i].unsqueeze(0).to(config['device'])], dim = -1)\n",
    "\n",
    "        else:\n",
    "            hypotheses_exp = []\n",
    "            losses = []\n",
    "            for hyp in hypotheses:\n",
    "                # logger.debug(f\"hyp: {hyp}\")\n",
    "                for j in range(config['k_per_location']):\n",
    "                    candidate = predicted_token_ids.indices[torch.where(indices_in_mlm_tokens == i)[0], j].to(config['device'])\n",
    "                    hypotheses_exp.append(torch.cat([hyp, candidate], dim=-1))\n",
    "    \n",
    "                    # logger.debug(f\"hypotheses_exp at {i}: {hypotheses_exp}\")\n",
    "                    with torch.no_grad():\n",
    "                        lossvalue = lossfns[0].compute_gold_loss(\n",
    "                            source_text, mlm_tokenizer.decode(hypotheses_exp[-1])\n",
    "                        )\n",
    "                    losses.append(lossvalue)\n",
    "    \n",
    "            hypotheses = sorted(zip(hypotheses_exp, losses), key=lambda x: x[1])[:beam_size]\n",
    "            hypotheses = [x[0] for x in hypotheses]\n",
    "            \n",
    "    return [mlm_tokenizer.decode(x) for x in hypotheses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "396a75e3-8b78-461a-9469-9fd956c792cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.61 s ± 61.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dummy_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2b5f2a9-6e0f-4e1b-ad84-db765b01682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_rerank_v0(source_text,\n",
    "                    masked_sequence,\n",
    "                    indices_in_mlm_tokens,\n",
    "                    predicted_token_ids,\n",
    "                    mlm_tokenizer, \n",
    "                    lossfns,\n",
    "                    config, \n",
    "                    beam_size = 5):\n",
    "    \n",
    "    hypotheses = [torch.LongTensor([]).to(config['device'])]\n",
    "    L = masked_sequence.size(-1)\n",
    "\n",
    "    for i in range(L):\n",
    "        if masked_sequence[0, i] != mlm_tokenizer.mask_token_id:\n",
    "            hypotheses = list(torch.cat([torch.stack(hypotheses,dim=0), \n",
    "                                        masked_sequence[:, i].unsqueeze(0).repeat((len(hypotheses),1)).to(config['device'])],dim=-1))\n",
    "        else:\n",
    "            num_hypotheses = len(hypotheses)\n",
    "            hypotheses = torch.stack(hypotheses,dim=0).unsqueeze(0)\n",
    "            hypotheses = hypotheses.repeat(config['k_per_location'], 1, 1)\n",
    "            candidates = predicted_token_ids.indices[torch.where(indices_in_mlm_tokens == i)[0], :].to(config['device']).T.unsqueeze(1)\n",
    "            candidates = candidates.repeat(1, num_hypotheses, 1)\n",
    "            hypotheses_exp = torch.cat([hypotheses, candidates], dim=-1)\n",
    "            hypotheses_exp = hypotheses_exp.view(-1, hypotheses_exp.shape[-1])\n",
    "            hypotheses_exp = list(hypotheses_exp)\n",
    "\n",
    "            losses = []\n",
    "            loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "            for hyp in hypotheses_exp:\n",
    "                curr_loss = 0.0\n",
    "                for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "                    with torch.no_grad():\n",
    "                        lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                            source_text, mlm_tokenizer.decode(hyp),\n",
    "                            label_id=config['target_label_ids'][lossid],\n",
    "                        )\n",
    "                    curr_loss += loss_weights[lossid] * lossvalue.item()\n",
    "                losses.append(curr_loss)\n",
    "\n",
    "            hypotheses = sorted(zip(hypotheses_exp, losses), key=lambda x: x[1])[:beam_size]\n",
    "            hypotheses = [x[0] for x in hypotheses]\n",
    "            \n",
    "    return [mlm_tokenizer.decode(x) for x in hypotheses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ac4f8b5-b88e-495f-b734-1b156f02b9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let me just start by saying I hate horse dong. But the majority of us grew up the horse that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'Let me just start by saying I hate horse dong. But the majority of people grow up the horse that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'Let me just start by saying I hate horse dong. But the majority of people grew up the horse that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'Let me just start by saying I hate horse dong. But the majority of us grow up the horse that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'Let me just start by saying I hate horse dong. But the majority of us grew up the fact that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_rerank_v0(source_text,\n",
    "                    inputs.input_ids,\n",
    "                    indices_in_mlm_tokens,\n",
    "                    predicted_token_ids,\n",
    "                    mlm_tokenizer, \n",
    "                    lossfns,\n",
    "                    config, \n",
    "                    beam_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8e8a3-13dd-4cb0-b524-e686a12dbfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hypotheses = len(hypotheses)\n",
    "hypotheses = torch.stack(hypotheses,dim=0).unsqueeze(0)\n",
    "hypotheses = hypotheses.repeat(config['k_per_location'], 1, 1)\n",
    "candidates = predicted_token_ids.indices[torch.where(indices_in_mlm_tokens == i)[0], :].to(config['device']).T.unsqueeze(1)\n",
    "candidates = candidates.repeat(1, num_hypotheses, 1)\n",
    "hypotheses_exp = torch.cat([hypotheses, candidates], dim=-1)\n",
    "hypotheses_exp = hypotheses_exp.view(-1, hypotheses_exp.shape[-1])\n",
    "hypotheses_exp = list(hypotheses_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66f5802c-4e68-423f-870d-14aee45b91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "hypotheses = [torch.LongTensor([]).to(config['device'])]\n",
    "L = masked_sequence.size(-1)\n",
    "\n",
    "for i in range(L):\n",
    "    if masked_sequence[0, i] != primary_tokenizer.mask_token_id:\n",
    "        # print('!')\n",
    "        # print(masked_sequence[:, i])\n",
    "        hypotheses = list(torch.cat([torch.stack(hypotheses,dim=0), \n",
    "                                    masked_sequence[:, i].unsqueeze(0).repeat((len(hypotheses),1)).to(config['device'])],dim=-1))\n",
    "        # print(hypotheses)\n",
    "    else:\n",
    "        prefix_added_hypotheses = torch.cat([source_batch.expand(len(hypotheses), -1), torch.stack(hypotheses,dim=0)], dim=-1)\n",
    "        with torch.no_grad():\n",
    "            model_output = primary_model(input_ids = prefix_added_hypotheses)\n",
    "\n",
    "        logits_t = model_output.logits[:, -1, :] # get logits for the last timestep\n",
    "        logp_t = F.log_softmax(logits_t, dim=-1) # (num_hypotheses, |V|)\n",
    "        # print(logp_t.shape)\n",
    "        top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(-logp_t, k=beam_size, largest=True, dim=-1)\n",
    "\n",
    "        candidates = top_cand_hyp_pos.T.unsqueeze(1).repeat(1, beam_size, 1)\n",
    "        hypotheses_ = torch.stack(hypotheses).unsqueeze(1).repeat(1, beam_size, 1).view(len(hypotheses)*beam_size,-1)\n",
    "        hypotheses_exp = list(torch.cat([hypotheses_, top_cand_hyp_pos.view(-1,1)], dim=-1))\n",
    "        # print(len(hypotheses_exp))\n",
    "\n",
    "        losses = []\n",
    "        for hyp in hypotheses_exp:\n",
    "            with torch.no_grad():\n",
    "                lossvalue = lossfns[0].compute_gold_loss(\n",
    "                    source_text, mlm_tokenizer.decode(hyp),\n",
    "                )\n",
    "            losses.append(lossvalue.item())\n",
    "        hypotheses = sorted(zip(hypotheses_exp, losses), key=lambda x: x[1])[:beam_size]\n",
    "        hypotheses = [x[0] for x in hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "daacccca-c78e-42ae-a4f1-be1e2b8d8930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   67,   185,    13,   212,   262,  3741,   286,   211,   208,   510,\n",
       "           208,   208,   326,   345,   550,   284,  3708,  3511,    13,  2011,\n",
       "           691, 38424,   318,   284,  3745,   340,  3589,    13,  1867,   561,\n",
       "           307,   262,  3772, 12838,   286,   616,  1204,   788,    30],\n",
       "        device='cuda:0'),\n",
       " tensor([   67,   185,    13,   212,   262,  3741,   286,   211,   208,   510,\n",
       "           208,   181,   326,   345,   550,   284,  3708,  3511,    13,  2011,\n",
       "           691, 38424,   318,   284,  3745,   340,  3589,    13,  1867,   561,\n",
       "           307,   262,  3772, 12838,   286,   616,  1204,   788,    30],\n",
       "        device='cuda:0'),\n",
       " tensor([   67,   185,    13,   212,   262,  3741,   286,   211,   208,   510,\n",
       "           208,   212,   326,   345,   550,   284,  3708,  3511,    13,  2011,\n",
       "           691, 38424,   318,   284,  3745,   340,  3589,    13,  1867,   561,\n",
       "           307,   262,  3772, 12838,   286,   616,  1204,   788,    30],\n",
       "        device='cuda:0'),\n",
       " tensor([   67,   185,    13,   212,   262,  3741,   286,   211,   208,   510,\n",
       "           216,   208,   326,   345,   550,   284,  3708,  3511,    13,  2011,\n",
       "           691, 38424,   318,   284,  3745,   340,  3589,    13,  1867,   561,\n",
       "           307,   262,  3772, 12838,   286,   616,  1204,   788,    30],\n",
       "        device='cuda:0'),\n",
       " tensor([   67,   185,    13,   212,   262,  3741,   286,   211,   208,   510,\n",
       "           203,   208,   326,   345,   550,   284,  3708,  3511,    13,  2011,\n",
       "           691, 38424,   318,   284,  3745,   340,  3589,    13,  1867,   561,\n",
       "           307,   262,  3772, 12838,   286,   616,  1204,   788,    30],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3640c1a-2035-4325-9009-59b61b23410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = [primary_tokenizer.decode(x) for x in list(hypotheses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b3cb0b1-15ac-40ef-847b-952eda34b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "closs_weight = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06f9cb59-be4d-4005-a61a-064fd38fd581",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_total_losses = []\n",
    "candidate_primary_losses = []\n",
    "candidate_losses_for_loggings = []\n",
    "candidate_allsats = []\n",
    "loss_weights = [1 - closs_weight, closs_weight]\n",
    "for hyp in hypotheses:\n",
    "    curr_loss = 0.0\n",
    "    logging_loss = []\n",
    "    allsat = True\n",
    "    for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "        with torch.no_grad():\n",
    "            lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                source_text, hyp,\n",
    "                label_id=config['target_label_ids'][lossid],\n",
    "            )\n",
    "        curr_loss += loss_weights[lossid] * lossvalue.item()\n",
    "        logging_loss.append(lossvalue.item())\n",
    "        if lossid==0:\n",
    "            candidate_primary_losses.append(lossvalue.item())\n",
    "        elif (lossid >= 1) and (\n",
    "            lossvalue.item()\n",
    "            > -np.log(config[\"min_epsilons\"][lossid - 1])\n",
    "        ):\n",
    "            allsat = False\n",
    "    candidate_total_losses.append(curr_loss)\n",
    "    candidate_losses_for_loggings.append(logging_loss)\n",
    "    candidate_allsats.append(allsat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "070ad447-be07-4c0f-980c-60917671fc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[309.912670763582,\n",
       " 280.2622925773263,\n",
       " 309.9250766009092,\n",
       " 309.9390413619578,\n",
       " 309.9804595440626]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_total_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a85228c9-ab83-4f6f-b042-949e93d3a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = torch.LongTensor([[]]).to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52e2dd6c-9b09-459a-8854-fbe10b50e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_batch = lossfns[0].tokenizer(source_text, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cebf7b3e-ccd9-44a7-bd20-f3df9dcd1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sequence = lossfns[0].tokenizer(masked_text, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62614bc3-3560-454d-943a-2cfce7dd0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_tokenizer = name2tokenizer['gpt2-large']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05aa67d0-196a-49d6-a040-657672e31fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4df267d4-267f-441e-af86-4822c5d4f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_batch = masked_sequence\n",
    "hypotheses = torch.LongTensor([[]]).to(config['device'])\n",
    "hyp_scores = torch.zeros(len(hypotheses), dtype = torch.float, device = config['device'])\n",
    "L = masked_sequence.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a104c9c9-deda-426b-aaed-ddd1c98116f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_added_hypotheses = torch.cat([source_batch.expand(hypotheses.size(0), -1), hypotheses], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18dfe978-9185-4c56-839d-a049d3dda424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5756,  502,  655,  923,  416, 2282,  314, 5465, 8223]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_added_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c2857c8-1e04-4aa5-ac84-fe27e5cb3187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primary_tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d59b7c54-4687-473e-af0b-882a91924c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = primary_model(input_ids = prefix_added_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f26acdb-3cc1-44b1-9d88-1dce7d87fbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 50257])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28c6a73b-dd2f-41e7-88f5-fcc0964ce14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fe63f89-81b8-418e-aa5d-e8dda40d9199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.69 s ± 9.44 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "predicted_batch = masked_sequence\n",
    "hypotheses = torch.LongTensor([[]]).to(config['device'])\n",
    "hyp_scores = torch.zeros(len(hypotheses), dtype = torch.float, device = config['device'])\n",
    "L = masked_sequence.size(-1)\n",
    "\n",
    "for t in range(L):\n",
    "    prefix_added_hypotheses = torch.cat([source_batch.expand(hypotheses.size(0), -1), hypotheses], dim=-1)\n",
    "    # print(prefix_added_hypotheses)\n",
    "    with torch.no_grad():\n",
    "        model_output = primary_model(input_ids = prefix_added_hypotheses)\n",
    "\n",
    "    logits_t = model_output.logits[:, -1, :] # get logits for the last timestep\n",
    "    logp_t = F.log_softmax(logits_t, dim=-1) # (num_hypotheses, |V|)\n",
    "    \n",
    "    if predicted_batch[:,t] != primary_tokenizer.mask_token_id:\n",
    "        \n",
    "        curr_nll = F.nll_loss(logp_t, predicted_batch[:, t].expand(logp_t.size(0)), reduction=\"none\") # returns (num_hypotheses)\n",
    "        hyp_scores = hyp_scores.expand_as(curr_nll) + curr_nll # (num_hypotheses)\n",
    "        hypotheses = torch.cat([hypotheses, predicted_batch[:, t].expand(hypotheses.size(0), -1)], dim=-1)\n",
    "        \n",
    "    else:\n",
    "        contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(logp_t) + (-logp_t)).view(-1) # (num_hypotheses x |V|)\n",
    "        top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=beam_size, largest=True)\n",
    "        \n",
    "        prev_hyp_ids = torch.div(top_cand_hyp_pos, len(primary_tokenizer), rounding_mode='floor') # prev_hyp_id for each of top_cand_hyp. (beam_size)\n",
    "        hyp_word_ids = top_cand_hyp_pos % len(primary_tokenizer) # hyp_word_id for each of top_cand_hyp. (beam_size)\n",
    "        \n",
    "        hypotheses = torch.cat([hypotheses[prev_hyp_ids], hyp_word_ids.unsqueeze(1)], dim=-1)\n",
    "        hyp_scores = top_cand_hyp_scores\n",
    "\n",
    "    # torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb3a43e7-867c-4f59-9a85-28ffe8338193",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = [primary_tokenizer.decode(x) for x in list(hypotheses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50b10949-2734-4e9d-87b4-7eba2d91389c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d�.� the majority of\\x17� up\\x12\\x14 that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'd�.� the majority of\\x17� up\\x12� that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'd�.� the majority of\\x17� up\\x12� that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'd�.� the majority of\\x17� up\\x12\\x18 that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?',\n",
       " 'd�.� the majority of\\x17� up\\x12� that you had to drive yourself. My only recourse is to feed it myself. What would be the happy tale of my life then?']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8910f62f-3ef5-4469-9fbd-186f17a47e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5756,  502,  655,  923,  416, 2282,  314, 5465, 8223]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b53386-b9c1-452a-aaf5-38aca0f482f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hypotheses = len(hypotheses)\n",
    "hypotheses = torch.stack(hypotheses,dim=0).unsqueeze(0)\n",
    "hypotheses = hypotheses.repeat(config['k_per_location'], 1, 1)\n",
    "candidates = predicted_token_ids.indices[torch.where(indices_in_mlm_tokens == i)[0], :].to(config['device']).T.unsqueeze(1)\n",
    "candidates = candidates.repeat(1, num_hypotheses, 1)\n",
    "hypotheses_exp = torch.cat([hypotheses, candidates], dim=-1)\n",
    "hypotheses_exp = hypotheses_exp.view(-1, hypotheses_exp.shape[-1])\n",
    "hypotheses_exp = list(hypotheses_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a418e28e-f85e-4f0e-95c6-c37c0ab9b2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  125],\n",
       "        [  212],\n",
       "        [  193],\n",
       "        [  181],\n",
       "        [36173],\n",
       "        [  125],\n",
       "        [  212],\n",
       "        [  193],\n",
       "        [  181],\n",
       "        [36173],\n",
       "        [  125],\n",
       "        [  212],\n",
       "        [  193],\n",
       "        [  181],\n",
       "        [36173],\n",
       "        [  125],\n",
       "        [  212],\n",
       "        [  193],\n",
       "        [  181],\n",
       "        [36173],\n",
       "        [  125],\n",
       "        [  212],\n",
       "        [  193],\n",
       "        [  181],\n",
       "        [36173]], device='cuda:0')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b671b-b5a5-45df-a17e-9c9eb5b05be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "974a0fab-9616-46a6-9e8c-6c0067c40d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   67,   183,    13,   125],\n",
       "        [   67,   183,    13,   212],\n",
       "        [   67,   183,    13,   193],\n",
       "        [   67,   183,    13,   181],\n",
       "        [   67,   183,    13, 36173],\n",
       "        [   67,   125,    13,   125],\n",
       "        [   67,   125,    13,   212],\n",
       "        [   67,   125,    13,   193],\n",
       "        [   67,   125,    13,   181],\n",
       "        [   67,   125,    13, 36173],\n",
       "        [   67,   185,    13,   125],\n",
       "        [   67,   185,    13,   212],\n",
       "        [   67,   185,    13,   193],\n",
       "        [   67,   185,    13,   181],\n",
       "        [   67,   185,    13, 36173],\n",
       "        [   67,   184,    13,   125],\n",
       "        [   67,   184,    13,   212],\n",
       "        [   67,   184,    13,   193],\n",
       "        [   67,   184,    13,   181],\n",
       "        [   67,   184,    13, 36173],\n",
       "        [   67,   186,    13,   125],\n",
       "        [   67,   186,    13,   212],\n",
       "        [   67,   186,    13,   193],\n",
       "        [   67,   186,    13,   181],\n",
       "        [   67,   186,    13, 36173]], device='cuda:0')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9baf71df-2dbd-4aaa-a78b-ff771708b8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  125,   212,   193,   181, 36173],\n",
       "        [  125,   212,   193,   181, 36173],\n",
       "        [  125,   212,   193,   181, 36173],\n",
       "        [  125,   212,   193,   181, 36173],\n",
       "        [  125,   212,   193,   181, 36173]], device='cuda:0')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_cand_hyp_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "64102383-7808-4212-82b2-a29af979fd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   67,   183,    13,   125],\n",
       "        [   67,   183,    13,   212],\n",
       "        [   67,   183,    13,   193],\n",
       "        [   67,   183,    13,   181],\n",
       "        [   67,   183,    13, 36173],\n",
       "        [   67,   125,    13,   125],\n",
       "        [   67,   125,    13,   212],\n",
       "        [   67,   125,    13,   193],\n",
       "        [   67,   125,    13,   181],\n",
       "        [   67,   125,    13, 36173],\n",
       "        [   67,   185,    13,   125],\n",
       "        [   67,   185,    13,   212],\n",
       "        [   67,   185,    13,   193],\n",
       "        [   67,   185,    13,   181],\n",
       "        [   67,   185,    13, 36173],\n",
       "        [   67,   184,    13,   125],\n",
       "        [   67,   184,    13,   212],\n",
       "        [   67,   184,    13,   193],\n",
       "        [   67,   184,    13,   181],\n",
       "        [   67,   184,    13, 36173],\n",
       "        [   67,   186,    13,   125],\n",
       "        [   67,   186,    13,   212],\n",
       "        [   67,   186,    13,   193],\n",
       "        [   67,   186,    13,   181],\n",
       "        [   67,   186,    13, 36173]], device='cuda:0')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d839f55-edea-4cf9-90cd-8656d13992c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfab21-dd97-420a-8e22-53a1157bdd02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a5c27-2d35-465c-9d4e-187e8872e66f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7662ba78-7413-4855-80b7-e6f44f5cd987",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 5\n",
    "top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(-logp_t, k=beam_size, largest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5a0496b5-9b45-43b5-9461-e31d82d2f113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 33])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(hypotheses).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3415125f-1529-4fa3-82a4-4006fb04327c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_cand_hyp_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69828e1c-c3ef-4a24-9e58-be81e88473a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_rerank_v2(source_batch, ## in primary tokens\n",
    "                    masked_sequence, ## in primary tokens\n",
    "                    primary_model, \n",
    "                    primary_tokenizer,\n",
    "                    config, \n",
    "                    beam_size = 5):\n",
    "    \n",
    "    hypotheses = [torch.LongTensor([]).to(config['device'])]\n",
    "    L = masked_sequence.size(-1)\n",
    "\n",
    "    for i in range(L):\n",
    "        if masked_sequence[0, i] != primary_tokenizer.mask_token_id:\n",
    "            hypotheses = list(torch.cat([torch.stack(hypotheses,dim=0), \n",
    "                                        masked_sequence[:, i].unsqueeze(0).repeat((len(hypotheses),1)).to(config['device'])],dim=-1))\n",
    "        else:\n",
    "            prefix_added_hypotheses = torch.cat([source_batch.expand(hypotheses.size(0), -1), hypotheses], dim=-1)\n",
    "            with torch.no_grad():\n",
    "                model_output = primary_model(input_ids = prefix_added_hypotheses)\n",
    "    \n",
    "            logits_t = model_output.logits[:, -1, :] # get logits for the last timestep\n",
    "            logp_t = F.log_softmax(logits_t, dim=-1) # (num_hypotheses, |V|)\n",
    "            \n",
    "            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(-logp_t, k=beam_size, largest=True)\n",
    "\n",
    "            prev_hyp_ids = torch.div(top_cand_hyp_pos, len(primary_tokenizer), rounding_mode='floor') # prev_hyp_id for each of top_cand_hyp. (beam_size)\n",
    "            hyp_word_ids = top_cand_hyp_pos % len(primary_tokenizer) # hyp_word_id for each of top_cand_hyp. (beam_size)\n",
    "            \n",
    "            hypotheses = torch.cat([hypotheses[prev_hyp_ids], hyp_word_ids.unsqueeze(1)], dim=-1)\n",
    "            hyp_scores = top_cand_hyp_scores\n",
    "\n",
    "            \n",
    "            \n",
    "            num_hypotheses = len(hypotheses)\n",
    "            hypotheses = torch.stack(hypotheses,dim=0).unsqueeze(0)\n",
    "            hypotheses = hypotheses.repeat(config['k_per_location'], 1, 1)\n",
    "            candidates = predicted_token_ids.indices[torch.where(indices_in_mlm_tokens == i)[0], :].to(config['device']).T.unsqueeze(1)\n",
    "            candidates = candidates.repeat(1, num_hypotheses, 1)\n",
    "            hypotheses_exp = torch.cat([hypotheses, candidates], dim=-1)\n",
    "            hypotheses_exp = hypotheses_exp.view(-1, hypotheses_exp.shape[-1])\n",
    "            hypotheses_exp = list(hypotheses_exp)\n",
    "\n",
    "            losses = []\n",
    "            loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "            for hyp in hypotheses_exp:\n",
    "                curr_loss = 0.0\n",
    "                for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "                    with torch.no_grad():\n",
    "                        lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                            source_text, mlm_tokenizer.decode(hyp),\n",
    "                            label_id=config['target_label_ids'][lossid],\n",
    "                        )\n",
    "                    curr_loss += loss_weights[lossid] * lossvalue.item()\n",
    "                losses.append(curr_loss)\n",
    "\n",
    "            hypotheses = sorted(zip(hypotheses_exp, losses), key=lambda x: x[1])[:beam_size]\n",
    "            hypotheses = [x[0] for x in hypotheses]\n",
    "            \n",
    "    return [mlm_tokenizer.decode(x) for x in hypotheses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cdbeb9f-2cfb-4630-a9ae-df2736ede7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = torch.LongTensor([[]]).to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d308ab65-26c8-4d0e-a1de-252969b5971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_batch = lossfns[0].tokenizer(source_text, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09906447-8132-4933-96ee-beabe4ae9c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sequence = lossfns[0].tokenizer(masked_text, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a30a3104-48d7-471a-bf36-0583e9f08c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_added_hypotheses = torch.cat([source_batch, hypotheses],dim=-1).to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "237d38b8-a2bb-491c-b969-cc3a7dff3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_added_hypotheses = torch.cat([source_batch.expand(hypotheses.size(0), -1), hypotheses], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e96ecb4e-bdf5-4469-b787-7e4d7e03039f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5756,  502,  655,  923,  416, 2282,  314, 5465, 8223]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_added_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec2eb111-f8db-4119-9022-f22affb5b469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5756,  502,  655,  923,  416, 2282,  314, 5465, 8223]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_added_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f85eaf5-b45b-47ef-b721-19190a5c7862",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = name2model['gpt2-large'](prefix_added_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a331b2e9-9c4a-40a6-90bf-4e2dc4abbd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_t = model_output.logits[:, -1, :] # get logits for the last timestep\n",
    "logp_t = F.log_softmax(logits_t, dim=-1) # (num_hypotheses, |V|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91f54bfb-8248-4f87-9cd3-3694dc85f1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50257])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logp_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed725ec7-0131-4d91-99b7-8c7a4d8e9ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "if masked_sequence[0, i] != mlm_tokenizer.mask_token_id:\n",
    "    print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6000e99e-cddc-4182-bd9f-73d3312e2bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = [torch.LongTensor([]).to(config['device'])]\n",
    "hypotheses = list(torch.cat([torch.stack(hypotheses,dim=0), \n",
    "                                        masked_sequence[:, i].unsqueeze(0).repeat((len(hypotheses),1)).to(config['device'])],dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f06f7314-4e1d-4abc-91b9-ebd850e5ffed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([50257], device='cuda:0')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba60dc3-476a-4f53-9593-56a14288820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_rerank_v2(\n",
    "                    source_batch: torch.Tensor,\n",
    "                    predicted_batch: torch.Tensor, \n",
    "                    edit_token_index_primary, \n",
    "                    primary_model: transformers.AutoModel, \n",
    "                    primary_tokenizer: transformers.AutoTokenizer,\n",
    "                    config: dict, \n",
    "                    beam_size: int\n",
    "                ) -> torch.Tensor:\n",
    "    \"\"\" Function that autoregressively edits a sequence(predicted_batch) by updating tokens at edit_token_index_primary indices and keeping the other tokens as were.\n",
    "    @param source_batch (Tensor): token ids of the prefix\n",
    "    @param predicted_batch (Tensor): token ids of the original continuation\n",
    "    @param edit_token_index_primary (Tensor): indices that indicate locations in the original continuation to edit\n",
    "    @param primary_model (AutoModel): model to calculate likelihood of candidate sequences\n",
    "    @param primary_tokenizer (AutoTokenizer): tokenizer for the primary_model\n",
    "    @param config (dict)\n",
    "    @param beam_size (int)\n",
    "\n",
    "    @returns hypotheses (Tensor): beam_size number of hypotheses to edit the original continuation. Tensor of shape (beam_size, sequence length).\n",
    "    \"\"\"\n",
    "\n",
    "    hypotheses = torch.LongTensor([[]]).to(config['device'])\n",
    "    hyp_scores = torch.zeros(len(hypotheses), dtype = torch.float, device = config['device'])\n",
    "    L = masked_sequence.size(-1)\n",
    "    \n",
    "    for t in range(seq_len):\n",
    "        \n",
    "        prefix_added_hypotheses = torch.cat([source_batch.expand(hypotheses.size(0), -1), hypotheses], dim=-1)\n",
    "        with torch.no_grad():\n",
    "            model_output = primary_model(input_ids = prefix_added_hypotheses)\n",
    "\n",
    "        logits_t = model_output.logits[:, -1, :] # get logits for the last timestep\n",
    "        logp_t = F.log_softmax(logits_t, dim=-1) # (num_hypotheses, |V|)\n",
    "        \n",
    "        if t not in edit_token_index_primary:\n",
    "            \n",
    "            curr_nll = F.nll_loss(logp_t, predicted_batch[:, t].expand(logp_t.size(0)), reduction=\"none\") # returns (num_hypotheses)\n",
    "            hyp_scores = hyp_scores.expand_as(curr_nll) + curr_nll # (num_hypotheses)\n",
    "            hypotheses = torch.cat([hypotheses, predicted_batch[:, t].expand(hypotheses.size(0), -1)], dim=-1)\n",
    "            \n",
    "        else:\n",
    "            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(logp_t) + (-logp_t)).view(-1) # (num_hypotheses x |V|)\n",
    "            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=beam_size, largest=True)\n",
    "            \n",
    "            prev_hyp_ids = torch.div(top_cand_hyp_pos, len(primary_tokenizer), rounding_mode='floor') # prev_hyp_id for each of top_cand_hyp. (beam_size)\n",
    "            hyp_word_ids = top_cand_hyp_pos % len(primary_tokenizer) # hyp_word_id for each of top_cand_hyp. (beam_size)\n",
    "            \n",
    "            hypotheses = torch.cat([hypotheses[prev_hyp_ids], hyp_word_ids.unsqueeze(1)], dim=-1)\n",
    "            hyp_scores = top_cand_hyp_scores\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    return hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851e5f6a-60d9-4e9b-9598-88ffc8c5612e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c6f54-e7e6-42e5-a7f4-81a440506980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9431a0a0-ff8c-48af-a365-79f810e46ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5863025f-9490-4f11-b22f-cb1f63f54868",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=pd.read_json('outputs/toxicity/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/gbi-word-netps3-nls1-os20-es4-allsat-toxic-to-nontoxic-attention-kf1j15xk/outputs_epsilon-3.txt',\n",
    "            lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d5aae-a025-48eb-b594-0c6bb2aabef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## unravel\n",
    "outputs=outputs.explode('generations')\n",
    "\n",
    "outputs.shape\n",
    "\n",
    "outputs['prompt']=outputs['prompt'].apply(lambda x: x['text'])\n",
    "\n",
    "outputs['text']=outputs['generations'].apply(lambda x: x['text'])\n",
    "\n",
    "gen_dict=outputs['generations'].values[0]\n",
    "\n",
    "for col in gen_dict.keys():\n",
    "    outputs[col] = outputs['generations'].apply(lambda x: x[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "245ff163-4865-4872-83ff-b1f0f85be984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>generations</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>allsat</th>\n",
       "      <th>losses</th>\n",
       "      <th>weighted_loss</th>\n",
       "      <th>repeat_count</th>\n",
       "      <th>mucoco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [prompt, generations, text, tokens, allsat, losses, weighted_loss, repeat_count, mucoco]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.loc[~outputs['mucoco'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f8b3362-dfa0-4491-a5a7-99919b92ed9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>generations</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>allsat</th>\n",
       "      <th>losses</th>\n",
       "      <th>weighted_loss</th>\n",
       "      <th>repeat_count</th>\n",
       "      <th>mucoco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>According to the criminal complaint, Edah kick...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>According to the criminal complaint, Edah kick...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>According to the criminal complaint, Edah kick...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>An advertisement for the product shows a girl ...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>An advertisement for the product shows a girl ...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Cruz began by mocking Trump’s reputation for i...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Behold, My Little Deathwing: Ponywing ...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>The lawsuits dispute Airbnb's assertion that i...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>The lawsuits dispute Airbnb's assertion that i...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Dotan says the bridge will serve as a runway f...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>And for those of you who are happily and comfo...</td>\n",
       "      <td>{'text': '', 'tokens': [], 'allsat': False, 'l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt  \\\n",
       "46   According to the criminal complaint, Edah kick...   \n",
       "46   According to the criminal complaint, Edah kick...   \n",
       "46   According to the criminal complaint, Edah kick...   \n",
       "51   An advertisement for the product shows a girl ...   \n",
       "51   An advertisement for the product shows a girl ...   \n",
       "70   Cruz began by mocking Trump’s reputation for i...   \n",
       "91           Behold, My Little Deathwing: Ponywing ...   \n",
       "134  The lawsuits dispute Airbnb's assertion that i...   \n",
       "134  The lawsuits dispute Airbnb's assertion that i...   \n",
       "167  Dotan says the bridge will serve as a runway f...   \n",
       "223  And for those of you who are happily and comfo...   \n",
       "\n",
       "                                           generations text tokens  allsat  \\\n",
       "46   {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "46   {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "46   {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "51   {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "51   {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "70   {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "91   {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "134  {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "134  {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "167  {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "223  {'text': '', 'tokens': [], 'allsat': False, 'l...          []   False   \n",
       "\n",
       "    losses  weighted_loss  repeat_count  mucoco  \n",
       "46      -1           -1.0            -1    True  \n",
       "46      -1           -1.0            -1    True  \n",
       "46      -1           -1.0            -1    True  \n",
       "51      -1           -1.0            -1    True  \n",
       "51      -1           -1.0            -1    True  \n",
       "70      -1           -1.0            -1    True  \n",
       "91      -1           -1.0            -1    True  \n",
       "134     -1           -1.0            -1    True  \n",
       "134     -1           -1.0            -1    True  \n",
       "167     -1           -1.0            -1    True  \n",
       "223     -1           -1.0            -1    True  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.loc[outputs['weighted_loss']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d560e-75af-4ff5-b938-943ac1293fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

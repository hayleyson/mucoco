{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of this notebook\n",
    "- to check if any code change is needed to use qwen 2.5 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hyeryung/.conda/envs/loc-edit/lib/python3.8/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from evaluation.prompted_sampling.evaluate import conditional_perplexity\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model\n",
    "- when loading 14B model, need to load as float16\n",
    "    - when float16, 30.7gb\n",
    "    - when float32, 61.4gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-14B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4250f474a4144717965a0388cb136ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e9d774b6ef43d788801ad0e93a528a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype = torch.float16)\n",
    "eval_model = eval_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX A6000\n",
      "Memory Usage:\n",
      "Free:  16.9 GB\n",
      "Allocated: 30.7 GB\n",
      "Cached:    30.7 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print('Memory Usage:')\n",
    "f = torch.cuda.get_device_properties(0).total_memory\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print('Free: ', round((f-a)/1024**3,1), 'GB')\n",
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run conditional perplexity code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generations_df = pd.read_json('/data/hyeryung/mucoco/new_module/llm_experiments/generate_with_llm/baselm_gens/gpt-3.5-turbo-0125/nontoxic/gpt-3.5-turbo-0125_realtoxicityprompts_0shot_150.jsonl',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '/data/hyeryung/mucoco/new_module/llm_experiments/generate_with_llm/baselm_gens/gpt-3.5-turbo-0125/nontoxic/gpt-3.5-turbo-0125_realtoxicityprompts_noprompt_150.jsonl'\n",
    "output_dir = Path(os.path.dirname(output_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating PPL:   0%|                                                    | 0/250 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Evaluating PPL: 100%|██████████████████████████████████████████| 250/250 [02:44<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.687756477163672 2500 2500 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    ppl, total_ppl = conditional_perplexity(generations_df, eval_model, eval_tokenizer, device=device, write_file=output_dir / (output_file+\".ppl-big\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.687756477163672, 4.99912035925632)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl, total_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check tokenization\n",
    "- no special bos token; eos token exists but not added to a single sentence\n",
    "- padding token equals eos token; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '<|endoftext|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<|object_ref_start|>',\n",
       "  '<|object_ref_end|>',\n",
       "  '<|box_start|>',\n",
       "  '<|box_end|>',\n",
       "  '<|quad_start|>',\n",
       "  '<|quad_end|>',\n",
       "  '<|vision_start|>',\n",
       "  '<|vision_end|>',\n",
       "  '<|vision_pad|>',\n",
       "  '<|image_pad|>',\n",
       "  '<|video_pad|>']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special tokens\n",
    "eval_tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [49680, 13, 1084, 594, 537, 429, 358, 47869, 26830, 26, 304, 2097, 11, 358, 1477, 1105, 5008, 80289, 9898, 13, 1988, 369, 1045, 2874, 11, 279, 4522, 315, 1660, 389, 264, 15223, 594, 1182, 3643, 752, 2666, 82272, 13, 18765, 432, 594, 279, 8679, 315, 15679, 476, 1101, 537, 8266, 304, 2524, 13, 32639, 279, 2874, 11, 15223, 1419, 19837, 374, 537, 458, 5702, 358, 4669, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if any special token added at the beginning & end \n",
    "eval_tokenizer(generations_df['generations'][0][0]['text'], add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"riding. It's not that I dislike horses; in fact, I find them quite majestic animals. But for some reason, the idea of being on a horse's back makes me feel uneasy. Perhaps it's the fear of falling or just not feeling in control. Whatever the reason, horseback riding is not an activity I enjoy.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no special token added\n",
    "eval_tokenizer.decode(eval_tokenizer(generations_df['generations'][0][0]['text'])['input_ids'], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = eval_tokenizer([gen['text'] for gen in generations_df['generations'][0]], add_special_tokens=True, padding=True, truncation=True,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 49680,     13,   1084,    594,    537,    429,    358,  47869,  26830,\n",
      "             26,    304,   2097,     11,    358,   1477,   1105,   5008,  80289,\n",
      "           9898,     13,   1988,    369,   1045,   2874,     11,    279,   4522,\n",
      "            315,   1660,    389,    264,  15223,    594,   1182,   3643,    752,\n",
      "           2666,  82272,     13,  18765,    432,    594,    279,   8679,    315,\n",
      "          15679,    476,   1101,    537,   8266,    304,   2524,     13,  32639,\n",
      "            279,   2874,     11,  15223,   1419,  19837,    374,    537,    458,\n",
      "           5702,    358,   4669,     13, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643],\n",
      "        [ 49680,     13,    576,   3381,    315,  11699,    389,    264,   3460,\n",
      "           9864,    646,    387,  63030,    369,   1045,   1251,     11,    714,\n",
      "           1052,    525,   3800,    879,   1477,    432,  90726,   1095,     13,\n",
      "          13139,    432,    594,    279,  53804,    315,  15369,    385,   9989,\n",
      "           1526,   1787,   5043,    476,    279,  25650,   3633,   1948,  37126,\n",
      "            323,  15223,     11,  15223,   1419,  19837,    374,    264,   5411,\n",
      "           3267,    545,    369,   1657,     13, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643],\n",
      "        [ 49680,     13,   2619,    594,   1101,   2494,    911,   1660,    389,\n",
      "           1741,    264,   3460,    323,   7988,   9864,    429,   3643,    752,\n",
      "           2666,    264,   2699,  82272,     13, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643],\n",
      "        [ 49680,     13,    576,   8266,    315,   1660,    389,   1741,    264,\n",
      "           7988,    323,  80289,   9864,    646,    387,   2176,  90726,   1095,\n",
      "            323,  63030,     13,    576,  28071,  21220,   5112,    315,  11386,\n",
      "          10088,  19558,    279,   4910,     11,    279,   2021,  17270,  11379,\n",
      "            438,    279,  15223,  10797,     11,    323,    279,  10815,    429,\n",
      "            646,   1352,   1948,  37126,    323,  15223,    525,   9428,   3281,\n",
      "             13,   2619,    374,   2494,  23702,    911,    279,   3633,   1948,\n",
      "          12677,    323,  26830,    429,    702,   1012,  80071,    369,  23631,\n",
      "             13],\n",
      "        [ 49680,     13,   1084,    594,    537,    847,   3166,     13,    576,\n",
      "           4522,    315,   1660,    389,   1741,    264,   3460,    323,   7988,\n",
      "           9864,   3643,    752,   2666,  82272,     13,    358,  10702,    803,\n",
      "          25650,   7488,   1075,  37364,    476,  66080,     13, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643],\n",
      "        [ 49680,     13,   1084,    594,    537,   2494,    358,   4669,   3730,\n",
      "             11,   1496,   3498,   1657,   1251,   1477,    432,    311,    387,\n",
      "            264,   2464,    323,  33848,   5702,     13, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643],\n",
      "        [ 39814,     11,    358,  12473,   3535,     13,    472,  22434,    646,\n",
      "            387,  24660,   4849,   9898,    369,   1045,   1251,     13,  18885,\n",
      "            498,   1075,    311,   4332,    803,    911,   3170,    498,  47869,\n",
      "           1105,     30, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643],\n",
      "        [  3983,    358,   5091,    429,    807,    525,  80289,   9898,    429,\n",
      "            614,   6342,   5089,  12783,   6814,   3840,     13, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643],\n",
      "        [ 49680,     13,    576,   1616,    279,  60758,  11074,  23969,    752,\n",
      "            323,    279,  21700,  36290,    315,    279,  15223,    594,    342,\n",
      "           1315,    646,    387,    773,  77629,     13,   2619,    594,   2494,\n",
      "           9428,   3281,    911,    279,  10815,   1948,    264,  37126,    323,\n",
      "            862,  15223,     11,   1513,    944,    498,   1744,     30,   1084,\n",
      "            594,   1075,   1660,    304,  25240,    448,   2441,   5382,   1660,\n",
      "            304,    264,   1616,    429,   4244,    646,    944,   5008,  12322,\n",
      "             13, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643],\n",
      "        [ 49680,     13,   1084,    594,    537,   2494,    429,  34630,    311,\n",
      "            752,     11,   1496,   3498,   1657,   1251,   1477,    432,  31080,\n",
      "            323,  90726,   1095,     13,    576,   4522,    315,  11699,    389,\n",
      "            264,   3460,   9864,    323,   4460,    311,   2524,   1181,  19029,\n",
      "           1101,   3171,    944,   2444,   1290,    448,    752,     13,    358,\n",
      "           1753,  10702,   1008,   7488,    979,    432,   4041,    311,  10164,\n",
      "            882,  33532,     13, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643]])\n",
      "<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding occurs -- token id 151643\n",
    "print(outputs['input_ids']), print(eval_tokenizer.decode([151643]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a98b0cd1-787b-4f3a-8467-a4f33bcfef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/s3/hyeryung/mucoco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72853578-afe1-45b0-a106-480dceee6164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from new_module.locate import evaluate_locate as eval_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e6eb4d4-f0b5-449e-91ac-74558ab60445",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_tokenizer = AutoTokenizer.from_pretrained(\"/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/step_2800_best_checkpoint\")\n",
    "em_tokenizer = AutoTokenizer.from_pretrained('/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "977097c2-e461-4d47-abbe-ea5f6d28fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pad_token(x):\n",
    "    return np.sum(np.array(x)==primary_tokenizer.pad_token_id)\n",
    "\n",
    "def remove_label_for_pad_token(row, colname):\n",
    "    return list(np.array(row[colname])[np.array(row['tokens'])!=primary_tokenizer.pad_token_id])\n",
    "\n",
    "def remove_pad_token(x):\n",
    "    return list(np.array(x)[np.array(x)!=primary_tokenizer.pad_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7483e112-c33b-40ba-8054-d34870ebb6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load labels data\n",
    "label_path = \"new_module/data/toxicity-avoidance/testset_gpt2_2500.jsonl\" #\"new_module/data/toxicity-avoidance/testset_gpt2_2500.jsonl\"\n",
    "labels = pd.read_json(label_path, lines=True)\n",
    "labels['prompt']=labels['prompt'].apply(lambda x: x['text'])\n",
    "\n",
    "labels = labels.explode('generations')\n",
    "\n",
    "labels['text']=labels['generations'].apply(lambda x: x['text'])\n",
    "labels['tokens']=labels['generations'].apply(lambda x: x['tokens'])\n",
    "\n",
    "labels['locate_labels']=labels['generations'].apply(lambda x: x.get('locate_labels', np.nan))\n",
    "\n",
    "del labels['generations']\n",
    "\n",
    "labels = labels.rename(columns={'locate_labels':'labels'})\n",
    "labels = labels.dropna(subset='labels')\n",
    "\n",
    "## correct minor errors -> remove trailing pad_token in the generations.\n",
    "has_pad_token = labels['tokens'].apply(count_pad_token) > 0\n",
    "labels.loc[has_pad_token, 'labels'] = labels.loc[has_pad_token,:].apply(lambda x: remove_label_for_pad_token(x, 'labels'),axis=1).values\n",
    "labels.loc[has_pad_token, 'tokens'] = labels.loc[has_pad_token, 'tokens'].apply(remove_pad_token)\n",
    "\n",
    "## similarly, clean text column -> remove \"<|endoftext|>\" text\n",
    "labels['text']=labels['text'].str.replace(\"<|endoftext|>\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93a3c8c6-b118-424e-a531-3b1f875ec3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30920c77-bec2-45cc-bfad-fc73f4d1ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_labels_binary_word(row, dataset_type=\"gpt2\"):\n",
    "\n",
    "    if dataset_type == \"gpt2\":\n",
    "        labels_token_index = np.where(np.array(row['labels_binary'])==1)[0]\n",
    "        labels_word_index = list(set([row['tok2word'][id] for id in labels_token_index]))\n",
    "    elif dataset_type == \"tsd\":\n",
    "        labels_char_index = np.array(row['labels_index_char'])\n",
    "        labels_word_index = list(set([row['char2word'][id] for id in labels_char_index]))\n",
    "\n",
    "    return [1 if id in labels_word_index else 0 for id in range(len(row['words']))]\n",
    "\n",
    "def get_labels_binary_char(row, dataset_type=\"gpt2\"):\n",
    "\n",
    "    if dataset_type == \"gpt2\":\n",
    "        labels_token_index = np.where(np.array(row['labels_binary'])==1)[0]\n",
    "        labels_char_index = list(set(sum([list(row['tok2char'][id]) for id in labels_token_index],[])))\n",
    "        return [1 if id in labels_char_index else 0 for id in range(len(row['char']))]\n",
    "    elif dataset_type == \"tsd\": ## for tsd, this function is not needed.\n",
    "        raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "340986e4-f5e3-4091-b6ec-dae4e3234986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tok2char(row: pd.Series, dataset_type:str=\"gpt2\") -> dict:\n",
    "    \"\"\"\n",
    "    A function to convert a list of tokens into a mapping between each token's index and its corresponding character offsets.\n",
    "    @param row: A row from dataframe\n",
    "    @return tok2char: A dictionary with token's location index as keys and tuples of corresponding character offsets as values.\n",
    "\n",
    "    Example:\n",
    "    row=pd.Series()\n",
    "    row['text']='wearing games and holy ****ing shit do I hate horse wearing games .'\n",
    "    row['tokens']=[86, 6648, 1830, 290, 11386, 25998, 278, 7510, 466, 314, 5465, 8223, 5762, 1830, 764]\n",
    "    tok2char=get_tok2char(row, \"tsd\")\n",
    "    tok2char\n",
    "    {0: (0,),\n",
    "    1: (1, 2, 3, 4, 5, 6),\n",
    "    2: (7, 8, 9, 10, 11, 12),\n",
    "    3: (13, 14, 15, 16),\n",
    "    ...\n",
    "    13: (59, 60, 61, 62, 63, 64),\n",
    "    14: (65,66)}\n",
    "    \"\"\"\n",
    "    # if dataset_type == \"gpt2\":        \n",
    "    #     tok2char=dict()\n",
    "    #     token_offsets=[0]\n",
    "    #     for i in range(1,len(row['tokens'])+1):\n",
    "    #         decoded=tokenizer.decode(row['tokens'][:i])\n",
    "    #         token_offsets.append(len(decoded))\n",
    "    #         tok2char[i-1]=tuple(range(token_offsets[i-1],token_offsets[i]))\n",
    "    #     return tok2char\n",
    "    \n",
    "    # elif dataset_type == \"tsd\":\n",
    "    tok2char=dict()\n",
    "    token_offsets=[0]\n",
    "    j = 0\n",
    "    for i in range(1,len(row['tokens'])+1):\n",
    "        while True:\n",
    "            if tokenizer.decode(tokenizer.encode(row['text'][:j],add_special_tokens=False)) != tokenizer.decode(row['tokens'][:i]):\n",
    "                if tokenizer.decode(row['tokens'][:i])[-1]=='�':#handle a case where a character is split into multiple tokens\n",
    "                    break\n",
    "                j+=1\n",
    "            else:\n",
    "                token_offsets.append(j)\n",
    "                tok2char[i-1]=tuple(range(token_offsets[-2],token_offsets[-1]))\n",
    "                tmp_id = i-2\n",
    "                while (tmp_id >= 0 and tmp_id not in tok2char):\n",
    "                    tok2char[tmp_id]=tuple(range(token_offsets[-2],token_offsets[-1]))\n",
    "                    tmp_id-=1\n",
    "                j+=1\n",
    "                break\n",
    "    return tok2char\n",
    "\n",
    "def get_word2char(row: pd.Series, ws: str) -> dict:\n",
    "    \"\"\"\n",
    "    A function to convert a list of words into a mapping between each word's index and its corresponding character offsets.\n",
    "    @param row: A row from dataframe\n",
    "    @return word2char: A dictionary with word's location index as keys and tuples of corresponding character offsets as values.\n",
    "\n",
    "    Caveat:\n",
    "    This code assumes that words are separated by only one type of whitespace, e.g. space.\n",
    "\n",
    "    Example:\n",
    "    row=pd.Series()\n",
    "    row['words']=['wearing', 'games', 'and', 'holy', '****ing', 'shit', 'do', 'I', 'hate', 'horse', 'wearing', 'games.']\n",
    "    word2char=get_word2char(row)\n",
    "    word2char\n",
    "    {0: (0, 1, 2, 3, 4, 5, 6),\n",
    "    1: (7, 8, 9, 10, 11, 12),...\n",
    "    9: (45, 46, 47, 48, 49, 50),\n",
    "    10: (51, 52, 53, 54, 55, 56, 57, 58),\n",
    "    11: (59, 60, 61, 62, 63, 64, 65)}\n",
    "    \"\"\"\n",
    "    \n",
    "    word_offsets=[0]\n",
    "    word2char=dict()\n",
    "    for i in range(1,len(row['words'])+1):\n",
    "        decoded=ws.join(row['words'][:i])\n",
    "        word_offsets.append(len(decoded))\n",
    "        word2char[i-1]=tuple(range(word_offsets[i-1],word_offsets[i]))\n",
    "    return word2char\n",
    "\n",
    "## group token indices that belong to the same word\n",
    "\n",
    "def get_word2tok(row: pd.Series, ws: str=None) -> dict:\n",
    "    \"\"\"\n",
    "    A function that take a list of words and a corresponding list of tokens \n",
    "    into a mapping between each word's index and its corresponding token indexes.\n",
    "    @param row: A row from dataframe\n",
    "    @return word2char: A dictionary with word's location index as keys and tuples of corresponding token location indexes as values.\n",
    "\n",
    "    Example:\n",
    "    row=pd.Series()\n",
    "    row['words']=['wearing', 'games', 'and', 'holy', '****ing', 'shit', 'do', 'I', 'hate', 'horse', 'wearing', 'games.']\n",
    "    row['tokens']=[86, 6648, 1830, 290, 11386, 25998, 278, 7510, 466, 314, 5465, 8223, 5762, 1830, 13]\n",
    "    word2tok=get_word2tok(row)\n",
    "    word2tok\n",
    "    {0: [0, 1],\n",
    "    1: [2],\n",
    "    2: [3],\n",
    "    ...\n",
    "    10: [12],\n",
    "    11: [13, 14]}\n",
    "    \"\"\"\n",
    "    global tokenizer\n",
    "    \n",
    "    jl, jr, k = 0, 0, 0\n",
    "    grouped_tokens = []\n",
    "    if ws is not None:\n",
    "        while jr <= len(row['tokens'])+1 and k < len(row['words']):\n",
    "            # print(f\"{jl}, {jr}, {k}: {tokenizer.decode(row['tokens'][jl:jr]).strip(' ')}\")\n",
    "            if tokenizer.decode(row['tokens'][jl:jr]).strip(' ') == row['words'][k]:\n",
    "                grouped_tokens.append(list(range(jl,jr)))\n",
    "                k += 1\n",
    "                jl = jr\n",
    "                jr += 1\n",
    "            else:\n",
    "                jr += 1\n",
    "        word2tok = dict(zip(range(len(grouped_tokens)), grouped_tokens))\n",
    "    else:\n",
    "        while jr <= len(row['tokens'])+1 and k < len(row['words']):\n",
    "            # print(f\"{jl}, {jr}, {k}: {tokenizer.decode(row['tokens'][jl:jr]).strip()}\")\n",
    "            if tokenizer.decode(row['tokens'][jl:jr]).strip() == row['words'][k]:\n",
    "                grouped_tokens.append(list(range(jl,jr)))\n",
    "                k += 1\n",
    "                jl = jr\n",
    "                jr += 1\n",
    "            else:\n",
    "                jr += 1\n",
    "        word2tok = dict(zip(range(len(grouped_tokens)), grouped_tokens))\n",
    "    return word2tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d01dfe58-19e7-4bee-8cc6-227677ceb20e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m sample_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39msample_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m sample_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords_index\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39msample_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x))))\n\u001b[0;32m---> 16\u001b[0m sample_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtok2char\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[43msample_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_tok2char\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m sample_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword2char\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39msample_text\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: get_word2char(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# if args.dataset_type == \"gpt2\":\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     sample_text['word2tok']=sample_text.apply(lambda x: get_word2tok(x),axis=1)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# elif args.dataset_type == \"tsd\":\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#     sample_text['word2tok']=sample_text.apply(lambda x: get_word2tok(x, \" \"),axis=1)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/loc-edit/lib/python3.11/site-packages/pandas/core/frame.py:10347\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10333\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10335\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10336\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10337\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10345\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10346\u001b[0m )\n\u001b[0;32m> 10347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/loc-edit/lib/python3.11/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/loc-edit/lib/python3.11/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/miniconda3/envs/loc-edit/lib/python3.11/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[32], line 16\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m sample_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39msample_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m sample_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords_index\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39msample_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x))))\n\u001b[0;32m---> 16\u001b[0m sample_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtok2char\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39msample_text\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mget_tok2char\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m sample_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword2char\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39msample_text\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: get_word2char(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# if args.dataset_type == \"gpt2\":\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     sample_text['word2tok']=sample_text.apply(lambda x: get_word2tok(x),axis=1)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# elif args.dataset_type == \"tsd\":\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#     sample_text['word2tok']=sample_text.apply(lambda x: get_word2tok(x, \" \"),axis=1)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 36\u001b[0m, in \u001b[0;36mget_tok2char\u001b[0;34m(row, dataset_type)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(tokenizer\u001b[38;5;241m.\u001b[39mencode(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][:j],add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)) \u001b[38;5;241m!=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     37\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m][:i])[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m�\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;66;03m#handle a case where a character is split into multiple tokens\u001b[39;00m\n\u001b[1;32m     38\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/loc-edit/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3479\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3452\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3453\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3454\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3467\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3468\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m   3470\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[1;32m   3471\u001b[0m             seq,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3476\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3477\u001b[0m     ]\n\u001b[0;32m-> 3479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\n\u001b[1;32m   3480\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3481\u001b[0m     token_ids: Union[\u001b[38;5;28mint\u001b[39m, List[\u001b[38;5;28mint\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   3482\u001b[0m     skip_special_tokens: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   3483\u001b[0m     clean_up_tokenization_spaces: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3484\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3485\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   3486\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3487\u001b[0m \u001b[38;5;124;03m    Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;124;03m    tokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3504\u001b[0m \u001b[38;5;124;03m        `str`: The decoded sentence.\u001b[39;00m\n\u001b[1;32m   3505\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3506\u001b[0m     \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ### Token ↔︎ Word ↔︎ Char 이 가능한 mapping 정의\n",
    "sample_text = labels[['text','tokens']].copy()\n",
    "sample_text['char']=sample_text['text'].apply(list)\n",
    "sample_text['char_index']=sample_text['char'].apply(lambda x: list(range(len(x))))\n",
    "assert (sample_text['char'].apply(len) != sample_text['char_index'].apply(len)).sum() == 0\n",
    "\n",
    "sample_text['tokens_index']=sample_text['tokens'].apply(lambda x: list(range(len(x))))\n",
    "\n",
    "# if args.dataset_type == \"gpt2\":\n",
    "#     sample_text['words']=sample_text['text'].str.split()\n",
    "# elif args.dataset_type == \"tsd\":\n",
    "#     sample_text['words']=sample_text['text'].str.split(' ')\n",
    "sample_text['words']=sample_text['text'].str.split(' ')\n",
    "sample_text['words_index']=sample_text['words'].apply(lambda x: list(range(len(x))))\n",
    "\n",
    "sample_text['tok2char']=sample_text.apply(lambda x: get_tok2char(x, \"gpt2\"),axis=1)\n",
    "sample_text['word2char']=sample_text.apply(lambda x: get_word2char(x, \" \"),axis=1)\n",
    "# if args.dataset_type == \"gpt2\":\n",
    "#     sample_text['word2tok']=sample_text.apply(lambda x: get_word2tok(x),axis=1)\n",
    "# elif args.dataset_type == \"tsd\":\n",
    "#     sample_text['word2tok']=sample_text.apply(lambda x: get_word2tok(x, \" \"),axis=1)\n",
    "sample_text['word2tok']=sample_text.apply(lambda x: get_word2tok(x, \" \"),axis=1)\n",
    "sample_text['tok2word']=sample_text['word2tok'].apply(kv_swap)\n",
    "sample_text['char2tok']=sample_text['tok2char'].apply(kv_swap)\n",
    "sample_text['char2word']=sample_text['word2char'].apply(kv_swap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b303963b-80fe-41f9-9aff-ff989473e528",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tok2word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/loc-edit/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tok2word'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_binary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[0;32m----> 2\u001b[0m labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_binary_word\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_labels_binary_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_binary_char\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mlabels\u001b[38;5;241m.\u001b[39mapply(get_labels_binary_char,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/loc-edit/lib/python3.11/site-packages/pandas/core/frame.py:10347\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10333\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10335\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10336\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10337\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10345\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10346\u001b[0m )\n\u001b[0;32m> 10347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/loc-edit/lib/python3.11/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/loc-edit/lib/python3.11/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/miniconda3/envs/loc-edit/lib/python3.11/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_binary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[0;32m----> 2\u001b[0m labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_binary_word\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mlabels\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mget_labels_binary_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_binary_char\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mlabels\u001b[38;5;241m.\u001b[39mapply(get_labels_binary_char,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m, in \u001b[0;36mget_labels_binary_word\u001b[0;34m(row, dataset_type)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      4\u001b[0m     labels_token_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39marray(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_binary\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m     labels_word_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtok2word\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabels_token_index\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dataset_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtsd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      7\u001b[0m     labels_char_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_index_char\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      4\u001b[0m     labels_token_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39marray(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_binary\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m     labels_word_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtok2word\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;28mid\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m labels_token_index]))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dataset_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtsd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      7\u001b[0m     labels_char_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_index_char\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/loc-edit/lib/python3.11/site-packages/pandas/core/series.py:1111\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/miniconda3/envs/loc-edit/lib/python3.11/site-packages/pandas/core/series.py:1227\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1227\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/miniconda3/envs/loc-edit/lib/python3.11/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tok2word'"
     ]
    }
   ],
   "source": [
    "labels['labels_binary'] = labels['labels'].apply(lambda x: [1 if i >= 0.5 else 0 for i in x])\n",
    "labels['labels_binary_word']=labels.apply(lambda x: get_labels_binary_word(x, dataset_type=\"gpt2\"),axis=1)\n",
    "labels['labels_binary_char']=labels.apply(get_labels_binary_char,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9116b1fc-06bd-45cc-b35e-77adb0c814ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import new_module.losses as lossbuilder\n",
    "from new_module.utils.robertacustom import RobertaCustomForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel, pipeline,AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_HOME']='/shared/s3/lab07/hyeryung/hf_cache'\n",
    "os.environ['HF_DATASETS_CACHE']='/shared/s3/lab07/hyeryung/hf_cache'\n",
    "os.environ['TRANSFORMERS_CACHE']='/shared/s3/lab07/hyeryung/hf_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('gpt2-large',cache_dir='/shared/s3/lab07/hyeryung/hf_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2-large',cache_dir='/shared/s3/lab07/hyeryung/hf_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts=[\"Once upon a time\",\n",
    "\"The book\",\n",
    "\"The chicken\",\n",
    "\"The city\",\n",
    "\"The country\",\n",
    "\"The horse\",\n",
    "\"The lake\",\n",
    "\"The last time\",\n",
    "\"The movie\",\n",
    "\"The painting\",\n",
    "\"The pizza\",\n",
    "\"The potato\",\n",
    "\"The president of the country\",\n",
    "\"The road\",\n",
    "\"The year is 1910\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s3/hyeryung/miniconda3/envs/loc-edit/lib/python3.11/site-packages/transformers/pipelines/base.py:1081: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gens_data_all=[]\n",
    "for seq_len in [12,20,50]:\n",
    "    pipe = pipeline(task='text-generation',\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                device='cuda:0',\n",
    "                do_sample=True,\n",
    "                top_p=1,\n",
    "                max_length=seq_len,\n",
    "                num_return_sequences=20,\n",
    "                pad_token_id=50256)\n",
    "    gens_all=[]\n",
    "    for prompt in prompts:\n",
    "        gens = pipe(prompt)\n",
    "        for gen in gens:\n",
    "            gen['generated_text']=gen['generated_text'][len(prompt):]\n",
    "            gen['text']=gen['generated_text']\n",
    "            del gen['generated_text']\n",
    "        gens_all.append(gens)\n",
    "    gens_data=pd.DataFrame({'prompt': [{'text':x} for x in prompts], 'generations': gens_all, 'seq_lengths': [seq_len]*len(prompts)})\n",
    "    gens_data_all.append(gens_data)\n",
    "gens_data_all=pd.concat(gens_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gens_data_all.to_json('new_module/data/sentiment/dev_set.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>generations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'text': 'Once upon a time'}</td>\n",
       "      <td>[{'text': ', many people who thought the world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'text': 'The book'}</td>\n",
       "      <td>[{'text': ' of Leviticus 18, called Deuteronom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'text': 'The chicken'}</td>\n",
       "      <td>[{'text': ' is very lean, it's about 20 pounds...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'text': 'The city'}</td>\n",
       "      <td>[{'text': ' had already started a program to \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'text': 'The country'}</td>\n",
       "      <td>[{'text': ''s political leaders seemed to be o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'text': 'The horse'}</td>\n",
       "      <td>[{'text': ' was a good example,\" Covington sai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'text': 'The lake'}</td>\n",
       "      <td>[{'text': ' of the world is only water; but th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'text': 'The last time'}</td>\n",
       "      <td>[{'text': ' I remember me, I had black hair an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'text': 'The movie'}</td>\n",
       "      <td>[{'text': ' has also attracted criticism. In o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'text': 'The painting'}</td>\n",
       "      <td>[{'text': ' is being displayed on display at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'text': 'The pizza'}</td>\n",
       "      <td>[{'text': ' is a bit more expensive (with 3 ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'text': 'The potato'}</td>\n",
       "      <td>[{'text': ' and the man were both standing by,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'text': 'The president of the country'}</td>\n",
       "      <td>[{'text': ' – who has a strong reputation in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'text': 'The road'}</td>\n",
       "      <td>[{'text': ' can be a scary place, and I still ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'text': 'The year is 1910'}</td>\n",
       "      <td>[{'text': ', as a child. A British immigrant b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      prompt  \\\n",
       "0               {'text': 'Once upon a time'}   \n",
       "1                       {'text': 'The book'}   \n",
       "2                    {'text': 'The chicken'}   \n",
       "3                       {'text': 'The city'}   \n",
       "4                    {'text': 'The country'}   \n",
       "5                      {'text': 'The horse'}   \n",
       "6                       {'text': 'The lake'}   \n",
       "7                  {'text': 'The last time'}   \n",
       "8                      {'text': 'The movie'}   \n",
       "9                   {'text': 'The painting'}   \n",
       "10                     {'text': 'The pizza'}   \n",
       "11                    {'text': 'The potato'}   \n",
       "12  {'text': 'The president of the country'}   \n",
       "13                      {'text': 'The road'}   \n",
       "14              {'text': 'The year is 1910'}   \n",
       "\n",
       "                                          generations  \n",
       "0   [{'text': ', many people who thought the world...  \n",
       "1   [{'text': ' of Leviticus 18, called Deuteronom...  \n",
       "2   [{'text': ' is very lean, it's about 20 pounds...  \n",
       "3   [{'text': ' had already started a program to \"...  \n",
       "4   [{'text': ''s political leaders seemed to be o...  \n",
       "5   [{'text': ' was a good example,\" Covington sai...  \n",
       "6   [{'text': ' of the world is only water; but th...  \n",
       "7   [{'text': ' I remember me, I had black hair an...  \n",
       "8   [{'text': ' has also attracted criticism. In o...  \n",
       "9   [{'text': ' is being displayed on display at t...  \n",
       "10  [{'text': ' is a bit more expensive (with 3 ex...  \n",
       "11  [{'text': ' and the man were both standing by,...  \n",
       "12  [{'text': ' – who has a strong reputation in t...  \n",
       "13  [{'text': ' can be a scary place, and I still ...  \n",
       "14  [{'text': ', as a child. A British immigrant b...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gens_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-2/step_2600_best_checkpoint/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read original data\n",
    "gpt2_outputs=pd.read_json('new_module/data/toxicity-avoidance/testset_gpt2_2500.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={'build_loss_dict': {\"coeff_steps\": 200, \"coeff_pattern\": \"constant\", \"loss_type\": \"xentropy\", \"length_normalize\": False, \"AR_temperature\": 1.0, \"AR_top_k\": 0, \"AR_top_p\": 0.96, \"max_output_length\": 20},\n",
    "       }\n",
    "class dummyArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "build_loss_args = dummyArgs(**config[\"build_loss_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpts = [### clsf-embed-share\n",
    "'/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-2/step_2600_best_checkpoint/',\n",
    "### em-embed-share\n",
    "'/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/step_2800_best_checkpoint/',\n",
    "### clsf\n",
    "'/shared/s3/lab07/hyeryung/loc_edit/models_re/roberta-base-jigsaw-toxicity-classifier/step_500_best_checkpoint/',\n",
    "### em\n",
    "'/shared/s3/lab07/hyeryung/loc_edit/models_re/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint/'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n",
      "50265\n"
     ]
    }
   ],
   "source": [
    "lossfns=[]\n",
    "for ckpt in ckpts:\n",
    "    if 'with-gpt2-large-embeds' in ckpt:\n",
    "        model=RobertaCustomForSequenceClassification.from_pretrained(ckpt)\n",
    "    else:\n",
    "        model=AutoModelForSequenceClassification.from_pretrained(ckpt)\n",
    "    model.eval()\n",
    "    tokenizer=AutoTokenizer.from_pretrained(ckpt)\n",
    "    \n",
    "    lossfn = \\\n",
    "        lossbuilder.build_loss(\n",
    "            \"classification_no_prefix_logprobloss\",\n",
    "            model,\n",
    "            tokenizer,\n",
    "            build_loss_args,\n",
    "        )\n",
    "    lossfns.append(lossfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_texts = []\n",
    "AR_predictions = []\n",
    "\n",
    "for i, row in gpt2_outputs.iterrows():\n",
    "    \n",
    "    source_tokens = row.prompt['tokens']\n",
    "    source_text = tokenizer.decode(source_tokens)\n",
    "    \n",
    "    curr_predictions = []\n",
    "    for j in range(10):\n",
    "        predicted_tokens = row.generations[j]['tokens']\n",
    "        AR_prediction = tokenizer.decode(predicted_tokens)\n",
    "        lossvals = np.array([])\n",
    "        with torch.no_grad():\n",
    "            for lossfn in lossfns:\n",
    "                lossvalue = lossfn.compute_gold_loss(\n",
    "                    source_text, AR_prediction,\n",
    "                    label_id=0,\n",
    "                )\n",
    "                lossvals = np.append(lossvals, lossvalue.squeeze().item())\n",
    "        if (lossvals > -np.log(0.75)).sum() >= 1:\n",
    "            curr_predictions.append({'text':AR_prediction})\n",
    "    source_texts.append({'text':source_text})\n",
    "    AR_predictions.append(curr_predictions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>generations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'text': 'PP June ever value already hasn left...</td>\n",
       "      <td>[{'text': '- Slip hockey for'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'text': ' are denies interestingweight murder...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'text': ' workingA knows left counterparts wo...</td>\n",
       "      <td>[{'text': ' front risk family system family se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'text': ' learn There shortas invited 25A loc...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'text': ' implementation brother in win calle...</td>\n",
       "      <td>[{'text': ' July workscent 8 35ucraft family 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>{'text': ' land If Cabinet added news rifles F...</td>\n",
       "      <td>[{'text': ' BB in 8 principal forado in news L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>{'text': ' journalism seem 15 7 treaties steps...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>{'text': ' areback leave towncount\",back t Act...</td>\n",
       "      <td>[{'text': ' abuse for'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>{'text': 'inations Tommy called Dr claimed fam...</td>\n",
       "      <td>[{'text': 'ization As Liverpool hour said ensu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>{'text': ' loops For recruit brain officials A...</td>\n",
       "      <td>[{'text': 'lings office l As 7x true Hugo repr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt  \\\n",
       "0    {'text': 'PP June ever value already hasn left...   \n",
       "1    {'text': ' are denies interestingweight murder...   \n",
       "2    {'text': ' workingA knows left counterparts wo...   \n",
       "3    {'text': ' learn There shortas invited 25A loc...   \n",
       "4    {'text': ' implementation brother in win calle...   \n",
       "..                                                 ...   \n",
       "245  {'text': ' land If Cabinet added news rifles F...   \n",
       "246  {'text': ' journalism seem 15 7 treaties steps...   \n",
       "247  {'text': ' areback leave towncount\",back t Act...   \n",
       "248  {'text': 'inations Tommy called Dr claimed fam...   \n",
       "249  {'text': ' loops For recruit brain officials A...   \n",
       "\n",
       "                                           generations  \n",
       "0                      [{'text': '- Slip hockey for'}]  \n",
       "1                                                   []  \n",
       "2    [{'text': ' front risk family system family se...  \n",
       "3                                                   []  \n",
       "4    [{'text': ' July workscent 8 35ucraft family 7...  \n",
       "..                                                 ...  \n",
       "245  [{'text': ' BB in 8 principal forado in news L...  \n",
       "246                                                 []  \n",
       "247                           [{'text': ' abuse for'}]  \n",
       "248  [{'text': 'ization As Liverpool hour said ensu...  \n",
       "249  [{'text': 'lings office l As 7x true Hugo repr...  \n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## select examples that are problematic\n",
    "pd.DataFrame({'prompt': source_texts, 'generations': AR_predictions})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/data/hyeryung/mucoco')\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from new_module.locate.new_locate_utils import LocateMachine\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import new_module.locate.new_locate_utils\n",
    "import importlib\n",
    "importlib.reload(new_module.locate.new_locate_utils)\n",
    "from new_module.locate.new_locate_utils import LocateMachine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='abc'\n",
    "prediction=['dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe','dsxe<s>','sdvbfe']\n",
    "ckpt_path = '/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint/'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckpt_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n",
    "device='cuda'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.41 s ± 595 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "loc_machine=LocateMachine(model,tokenizer)\n",
    "res = loc_machine.locate_main(prediction, \"attention\", max_num_tokens = 6, unit=\"word\", num_layer=10, label_id=0, \n",
    "                              num_processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.94 s ± 282 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "loc_machine=LocateMachine(model,tokenizer)\n",
    "res = loc_machine.locate_main(prediction, \"attention\", max_num_tokens = 6, unit=\"word\", num_layer=10, label_id=0, \n",
    "                              num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.6 ms ± 4.54 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "## when modified the code to run a for loop\n",
    "loc_machine=LocateMachine(model,tokenizer)\n",
    "res = loc_machine.locate_main(prediction, \"attention\", max_num_tokens = 6, unit=\"word\", num_layer=10, label_id=0, \n",
    "                              num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel(outputs_df):\n",
    "    outputs_df=outputs_df.explode('generations',ignore_index=True)\n",
    "    outputs_df['prompt']=outputs_df['prompt'].apply(lambda x: x['text'])\n",
    "    outputs_df['generations']=outputs_df['generations'].apply(lambda x: x['text'] if isinstance(x, dict) else x)\n",
    "    outputs_df = outputs_df.dropna().reset_index(drop=True)\n",
    "    return outputs_df\n",
    "toxic_gen=pd.read_json('new_module/data/toxicity-avoidance/dev_set.jsonl',lines=True)\n",
    "toxic_gen=unravel(toxic_gen)\n",
    "prompt = '“I went to use the bathroom, and I went in and Lance was bent'\n",
    "prediction = toxic_gen.loc[toxic_gen['prompt']==prompt,'generations'].tolist()\n",
    "ckpt_path = '/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint/'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckpt_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n",
    "device='cuda'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.1 ms ± 1.57 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "## when modified the code to run a for loop\n",
    "loc_machine=LocateMachine(model,tokenizer)\n",
    "res = loc_machine.locate_main(prediction, \"attention\", max_num_tokens = 6, unit=\"word\", num_layer=10, label_id=0, \n",
    "                              num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.92 s ± 1.37 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "loc_machine=LocateMachine(model,tokenizer)\n",
    "res = loc_machine.locate_main(prediction, \"attention\", max_num_tokens = 6, unit=\"word\", num_layer=10, label_id=0, \n",
    "                              num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.5 ms ± 1.03 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "## with for-loop implementation\n",
    "## after changing the function to return masked sequences (text) instead of located_indices\n",
    "loc_machine=LocateMachine(model,tokenizer)\n",
    "res = loc_machine.locate_main(prediction, \"attention\", max_num_tokens = 6, unit=\"word\", num_layer=10, label_id=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc-edit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7a97b9a0-ba55-47b0-8e16-f7706480e0ef",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Prototyping code for evaluating the accuracy of locating tokens to edit against ground truth.\n",
    "For metrics, MRR (mean reciprocal rank),Average Precision,and F1 score is used.\n",
    "Other candidate metrics include AUC and Recall @ K.\n",
    "The unit of calculating the metric is \"token\" at the moment.\n",
    "But it will expand to \"character\" and \"word\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2fcc9928-ae73-4d42-9454-d1dd00cc7698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, fbeta_score, average_precision_score\n",
    "\n",
    "os.chdir(\"/home/s3/hyeryung/mucoco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e321d5-5442-4ed0-9b40-161d13b1ac00",
   "metadata": {},
   "source": [
    "### Prepare dataset (predictions & labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "f9f47f5b-b4eb-43de-9e99-417761a885a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read predicted file\n",
    "pred_path = \"new_module/locate/results/toxicity/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/tsd_test_gn.jsonl\"\n",
    "method=\"grad_norm\"\n",
    "\n",
    "predictions = pd.read_json(pred_path, lines=True)\n",
    "predictions = predictions[['text',f'pred_indices_{method}', f'pred_scores_{method}']].copy()\n",
    "predictions = predictions.rename(columns={f'pred_indices_{method}':'pred',\n",
    "                                        f'pred_scores_{method}':'pred_scores'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "aae8ddb2-20d1-4752-8620-a0e83e3a9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read ground truth file\n",
    "label_path = \"new_module/locate/results/toxicity/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/tsd_test_gn.jsonl\"\n",
    "labels = pd.read_json(label_path, lines=True)\n",
    "labels = labels[['text','spans']]\n",
    "labels = labels.rename(columns={'spans':'labels_index_char'})\n",
    "labels['labels_index_char'] = labels['labels_index_char'].apply(eval)\n",
    "\n",
    "## join predictions & labels \n",
    "predictions = pd.merge(predictions, labels, on=['text'],how='left')\n",
    "## drop duplicates (cases where different labels for the same text)\n",
    "predictions = predictions.drop_duplicates(subset=['text'],keep=False)\n",
    "\n",
    "predictions = predictions.loc[predictions['labels_index_char'].apply(len) > 0, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ab65e3ab-df39-429f-b75b-f970c3d47d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds-energy-training/step_2800_best_checkpoint\")\n",
    "\n",
    "def count_pad_token(x):\n",
    "    return np.sum(np.array(x)==tokenizer.pad_token_id)\n",
    "def remove_pad_token(x):\n",
    "    return list(np.array(x)[np.array(x)!=tokenizer.pad_token_id])\n",
    "def remove_label_for_pad_token(row, colname):\n",
    "    return list(np.array(row[colname])[np.array(row['tokens'])!=tokenizer.pad_token_id])\n",
    "\n",
    "def index2binary(row, index_colname='pred', len_colname='tokens'):\n",
    "    return [1 if i in row[index_colname] else 0 for i in range(len(row[len_colname]))]\n",
    "\n",
    "\n",
    "def apply_f1(row, suffix=''):\n",
    "    \"\"\"suffix should start with _\"\"\"\n",
    "    return f1_score(row[f'labels_binary{suffix}'],row[f'pred_binary{suffix}'], zero_division=np.nan)\n",
    "def apply_f2(row, suffix=''):\n",
    "    \"\"\"suffix should start with _\"\"\"\n",
    "    return fbeta_score(row[f'labels_binary{suffix}'],row[f'pred_binary{suffix}'], beta=2, zero_division=np.nan)\n",
    "def apply_ap(row, suffix=''):\n",
    "    \"\"\"suffix should start with _\"\"\"\n",
    "    if sum(row[f'labels_binary{suffix}'])==0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return average_precision_score(row[f'labels_binary{suffix}'],row[f'pred_scores{suffix}'])\n",
    "def apply_precision(row, suffix=''):\n",
    "    \"\"\"suffix should start with _\"\"\"\n",
    "    return precision_score(row[f'labels_binary{suffix}'],row[f'pred_binary{suffix}'], zero_division=np.nan)\n",
    "def apply_recall(row, suffix=''):\n",
    "    \"\"\"suffix should start with _\"\"\"\n",
    "    return recall_score(row[f'labels_binary{suffix}'],row[f'pred_binary{suffix}'], zero_division=np.nan)\n",
    "\n",
    "\n",
    "def rr(out, labels, k = 6): #implement mean reciprocal rank\n",
    "    idx_array = stats.rankdata(-out, axis=-1, method='min')\n",
    "    # print(idx_array)\n",
    "    labels = np.where(labels==1)[0].astype(int)\n",
    "    # print(labels)\n",
    "    rank = np.take_along_axis(idx_array, labels, axis=-1)\n",
    "    # print(rank)\n",
    "    rr=1/rank.min() if rank.min() <= k else 0.\n",
    "    return rr\n",
    "    \n",
    "def get_rr(row, suffix=''):\n",
    "    \"\"\"suffix should start with _\"\"\"\n",
    "    if sum(row[f'labels_binary{suffix}'])==0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return rr(np.array(row[f'pred_scores{suffix}']),np.array(row[f'labels_binary{suffix}']))\n",
    "    \n",
    "def get_tok2char(row: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    A function to convert a list of tokens into a mapping between each token's index and its corresponding character offsets.\n",
    "    @param row: A row from dataframe\n",
    "    @return tok2char: A dictionary with token's location index as keys and tuples of corresponding character offsets as values.\n",
    "\n",
    "    Example:\n",
    "    row=pd.Series()\n",
    "    row['text']='wearing games and holy ****ing shit do I hate horse wearing games .'\n",
    "    row['tokens']=[86, 6648, 1830, 290, 11386, 25998, 278, 7510, 466, 314, 5465, 8223, 5762, 1830, 764]\n",
    "    tok2char=get_tok2char(row)\n",
    "    tok2char\n",
    "    {0: (0,),\n",
    "    1: (1, 2, 3, 4, 5, 6),\n",
    "    2: (7, 8, 9, 10, 11, 12),\n",
    "    3: (13, 14, 15, 16),\n",
    "    ...\n",
    "    13: (59, 60, 61, 62, 63, 64),\n",
    "    14: (65,66)}\n",
    "    \"\"\"\n",
    "    global tokenizer\n",
    "    tok2char=dict()\n",
    "    token_offsets=[0]\n",
    "    j = 0\n",
    "    for i in range(1,len(row['tokens'])+1):\n",
    "        while True:\n",
    "            if tokenizer.decode(tokenizer.encode(row['text'][:j],add_special_tokens=False)) != tokenizer.decode(row['tokens'][:i]):\n",
    "                if tokenizer.decode(row['tokens'][:i])[-1]=='ï¿½':#handle a case where a character is split into multiple tokens\n",
    "                    break\n",
    "                j+=1\n",
    "            else:\n",
    "                token_offsets.append(j)\n",
    "                tok2char[i-1]=tuple(range(token_offsets[-2],token_offsets[-1]))\n",
    "                tmp_id = i-2\n",
    "                while (tmp_id >= 0 and tmp_id not in tok2char):\n",
    "                    tok2char[tmp_id]=tuple(range(token_offsets[-2],token_offsets[-1]))\n",
    "                    tmp_id-=1\n",
    "                j+=1\n",
    "                break\n",
    "    return tok2char\n",
    "\n",
    "def get_word2char(row: pd.Series, ws: str) -> dict:\n",
    "    \"\"\"\n",
    "    A function to convert a list of words into a mapping between each word's index and its corresponding character offsets.\n",
    "    @param row: A row from dataframe\n",
    "    @return word2char: A dictionary with word's location index as keys and tuples of corresponding character offsets as values.\n",
    "\n",
    "    Caveat:\n",
    "    This code assumes that words are separated by only one type of whitespace, e.g. space.\n",
    "\n",
    "    Example:\n",
    "    row=pd.Series()\n",
    "    row['words']=['wearing', 'games', 'and', 'holy', '****ing', 'shit', 'do', 'I', 'hate', 'horse', 'wearing', 'games.']\n",
    "    word2char=get_word2char(row)\n",
    "    word2char\n",
    "    {0: (0, 1, 2, 3, 4, 5, 6),\n",
    "    1: (7, 8, 9, 10, 11, 12),...\n",
    "    9: (45, 46, 47, 48, 49, 50),\n",
    "    10: (51, 52, 53, 54, 55, 56, 57, 58),\n",
    "    11: (59, 60, 61, 62, 63, 64, 65)}\n",
    "    \"\"\"\n",
    "    \n",
    "    word_offsets=[0]\n",
    "    word2char=dict()\n",
    "    for i in range(1,len(row['words'])+1):\n",
    "        decoded=ws.join(row['words'][:i])\n",
    "        word_offsets.append(len(decoded))\n",
    "        word2char[i-1]=tuple(range(word_offsets[i-1],word_offsets[i]))\n",
    "    return word2char\n",
    "\n",
    "## group token indices that belong to the same word\n",
    "\n",
    "def get_word2tok(row: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    A function that take a list of words and a corresponding list of tokens \n",
    "    into a mapping between each word's index and its corresponding token indexes.\n",
    "    @param row: A row from dataframe\n",
    "    @return word2char: A dictionary with word's location index as keys and tuples of corresponding token location indexes as values.\n",
    "\n",
    "    Example:\n",
    "    row=pd.Series()\n",
    "    row['words']=['wearing', 'games', 'and', 'holy', '****ing', 'shit', 'do', 'I', 'hate', 'horse', 'wearing', 'games.']\n",
    "    row['tokens']=[86, 6648, 1830, 290, 11386, 25998, 278, 7510, 466, 314, 5465, 8223, 5762, 1830, 13]\n",
    "    word2tok=get_word2tok(row)\n",
    "    word2tok\n",
    "    {0: [0, 1],\n",
    "    1: [2],\n",
    "    2: [3],\n",
    "    ...\n",
    "    10: [12],\n",
    "    11: [13, 14]}\n",
    "    \"\"\"\n",
    "    global tokenizer\n",
    "    \n",
    "    jl, jr, k = 0, 0, 0\n",
    "    grouped_tokens = []\n",
    "    while jr <= len(row['tokens'])+1 and k < len(row['words']):\n",
    "        # print(f\"{jl}, {jr}, {k}: {tokenizer.decode(row['tokens'][jl:jr]).strip(' ')}\")\n",
    "        if tokenizer.decode(row['tokens'][jl:jr]).strip(' ') == row['words'][k]:\n",
    "            grouped_tokens.append(list(range(jl,jr)))\n",
    "            k += 1\n",
    "            jl = jr\n",
    "            jr += 1\n",
    "        else:\n",
    "            jr += 1\n",
    "    word2tok = dict(zip(range(len(grouped_tokens)), grouped_tokens))\n",
    "    return word2tok\n",
    "\n",
    "def kv_swap(x):\n",
    "\n",
    "    return_dict=dict()\n",
    "    for k,v in x.items():\n",
    "        for item in v:\n",
    "            return_dict[item]=k\n",
    "    return return_dict\n",
    "\n",
    "def get_pred_word(row):\n",
    "\n",
    "    return sorted(list(set([row['tok2word'][id] for id in row['pred']])))\n",
    "def get_pred_binary_word(row):\n",
    "\n",
    "    return [1 if id in row['pred_word'] else 0 for id in range(len(row['words']))]\n",
    "\n",
    "def get_labels_binary_word(row, dataset_type=\"gpt2\"):\n",
    "\n",
    "    if dataset_type == \"gpt2\":\n",
    "        labels_token_index = np.where(np.array(row['labels_binary'])==1)[0]\n",
    "        labels_word_index = list(set([row['tok2word'][id] for id in labels_token_index]))\n",
    "    elif dataset_type == \"tsd\":\n",
    "        labels_char_index = np.array(row['labels_index_char'])\n",
    "        labels_word_index = list(set([row['char2word'][id] for id in labels_char_index]))\n",
    "\n",
    "    return [1 if id in labels_word_index else 0 for id in range(len(row['words']))]\n",
    "\n",
    "def get_labels_binary_char(row, dataset_type=\"gpt2\"):\n",
    "\n",
    "    if dataset_type == \"gpt2\":\n",
    "        labels_token_index = np.where(np.array(row['labels_binary'])==1)[0]\n",
    "        labels_char_index = list(set(sum([list(row['tok2char'][id]) for id in labels_token_index],[])))\n",
    "        return [1 if id in labels_char_index else 0 for id in range(len(row['char']))]\n",
    "    elif dataset_type == \"tsd\": ## for tsd, this function is not needed.\n",
    "        raise NotImplementedError\n",
    "    \n",
    "def get_labels_binary_token(row, dataset_type=\"gpt2\"):\n",
    "\n",
    "    if dataset_type == \"gpt2\": ## for gpt2, this function is not needed.\n",
    "        raise NotImplementedError\n",
    "    elif dataset_type == \"tsd\":\n",
    "        labels_char_index = np.array(row['labels_index_char'])\n",
    "        labels_token_index = list(set([row['char2tok'][id] for id in labels_char_index]))\n",
    "        return [1 if id in labels_token_index else 0 for id in range(len(row['tokens']))]\n",
    "\n",
    "\n",
    "def get_pred_char(row):\n",
    "    \n",
    "    return sorted(list(set(sum([list(row['tok2char'][id]) for id in row['pred']],[]))))\n",
    "\n",
    "def get_pred_binary_char(row):\n",
    "\n",
    "    return [1 if id in row['pred_char'] else 0 for id in range(len(row['char']))]\n",
    "\n",
    "def get_pred_scores_char(row):\n",
    "    \"\"\"\n",
    "    Each character gets the score of the token it belongs.\n",
    "    \"\"\"\n",
    "    return_list=[]\n",
    "    for char_id in range(len(row['char'])):\n",
    "        try:\n",
    "            return_list.append(row['pred_scores'][row['char2tok'][char_id]])\n",
    "        except:\n",
    "            print(row['text'])\n",
    "            print(len(row['char']))\n",
    "            print(len(row['char2tok'].keys()))\n",
    "    return return_list\n",
    "\n",
    "def get_pred_scores_word(row,method='sum'):\n",
    "    return_list=[]\n",
    "    if method=='sum':\n",
    "        func=np.sum\n",
    "    elif method=='max':\n",
    "        func=np.max\n",
    "    elif method=='mean':\n",
    "        func=np.mean\n",
    "    for word_id in range(len(row['words'])):\n",
    "        return_list.append(func(np.array(row['pred_scores'])[row['word2tok'][word_id]]))\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d8b88ff4-05f3-4832-87fb-6265559ef794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Token âï¸ Word âï¸ Char ì´ ê°ë¥í mapping ì ì\n",
    "sample_text = predictions_labels[['text','tokens']].copy()\n",
    "sample_text['char']=sample_text['text'].apply(list)\n",
    "sample_text['char_index']=sample_text['char'].apply(lambda x: list(range(len(x))))\n",
    "assert (sample_text['char'].apply(len) != sample_text['char_index'].apply(len)).sum() == 0\n",
    "\n",
    "sample_text['tokens_index']=sample_text['tokens'].apply(lambda x: list(range(len(x))))\n",
    "\n",
    "sample_text['words']=sample_text['text'].str.split(' ')\n",
    "sample_text['words_index']=sample_text['words'].apply(lambda x: list(range(len(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "41edf8f7-44eb-4e84-86d9-2568a39c3a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  That's right. They are not normal. And I am st...   \n",
      "1  \"Watch people die from taking away their healt...   \n",
      "2  I live in an area that saw major flood damage ...   \n",
      "3    Elaine Marie Jeffers sounds like a sociopath ð.   \n",
      "4  A rock has more mental horsepower than the ind...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [2504, 338, 826, 13, 1119, 389, 407, 3487, 13,...   \n",
      "1  [1, 10723, 661, 4656, 422, 2263, 1497, 511, 11...   \n",
      "2  [40, 2107, 287, 281, 1989, 326, 2497, 1688, 69...   \n",
      "3  [9527, 5718, 20492, 5502, 364, 5238, 588, 257,...   \n",
      "4  [32, 3881, 468, 517, 5110, 36696, 621, 262, 77...   \n",
      "\n",
      "                                                char  \\\n",
      "0  [T, h, a, t, ', s,  , r, i, g, h, t, .,  , T, ...   \n",
      "1  [\", W, a, t, c, h,  , p, e, o, p, l, e,  , d, ...   \n",
      "2  [I,  , l, i, v, e,  , i, n,  , a, n,  , a, r, ...   \n",
      "3  [E, l, a, i, n, e,  , M, a, r, i, e,  , J, e, ...   \n",
      "4  [A,  , r, o, c, k,  , h, a, s,  , m, o, r, e, ...   \n",
      "\n",
      "                                          char_index  \\\n",
      "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "1  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "2  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "3  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "4  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "\n",
      "                                        tokens_index  \\\n",
      "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "1  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "2  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "3         [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]   \n",
      "4  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "\n",
      "                                               words  \\\n",
      "0  [That's, right., They, are, not, normal., And,...   \n",
      "1  [\"Watch, people, die, from, taking, away, thei...   \n",
      "2  [I, live, in, an, area, that, saw, major, floo...   \n",
      "3  [Elaine, Marie, Jeffers, sounds, like, a, soci...   \n",
      "4  [A, rock, has, more, mental, horsepower, than,...   \n",
      "\n",
      "                                         words_index  \\\n",
      "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "1  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "2  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "3                           [0, 1, 2, 3, 4, 5, 6, 7]   \n",
      "4  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "\n",
      "                                            tok2char  \\\n",
      "0  {0: (0, 1, 2, 3), 1: (4, 5), 2: (6, 7, 8, 9, 1...   \n",
      "1  {0: (0,), 1: (1, 2, 3, 4, 5), 2: (6, 7, 8, 9, ...   \n",
      "2  {0: (0,), 1: (1, 2, 3, 4, 5), 2: (6, 7, 8), 3:...   \n",
      "3  {0: (0, 1), 1: (2, 3, 4, 5), 2: (6, 7, 8, 9, 1...   \n",
      "4  {0: (0,), 1: (1, 2, 3, 4, 5), 2: (6, 7, 8, 9),...   \n",
      "\n",
      "                                           word2char  \\\n",
      "0  {0: (0, 1, 2, 3, 4, 5), 1: (6, 7, 8, 9, 10, 11...   \n",
      "1  {0: (0, 1, 2, 3, 4, 5), 1: (6, 7, 8, 9, 10, 11...   \n",
      "2  {0: (0,), 1: (1, 2, 3, 4, 5), 2: (6, 7, 8), 3:...   \n",
      "3  {0: (0, 1, 2, 3, 4, 5), 1: (6, 7, 8, 9, 10, 11...   \n",
      "4  {0: (0,), 1: (1, 2, 3, 4, 5), 2: (6, 7, 8, 9),...   \n",
      "\n",
      "                                            word2tok  \\\n",
      "0  {0: [0, 1], 1: [2, 3], 2: [4], 3: [5], 4: [6],...   \n",
      "1  {0: [0, 1], 1: [2], 2: [3], 3: [4], 4: [5], 5:...   \n",
      "2  {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...   \n",
      "3  {0: [0, 1], 1: [2], 2: [3, 4], 3: [5], 4: [6],...   \n",
      "4  {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...   \n",
      "\n",
      "                                            tok2word  \\\n",
      "0  {0: 0, 1: 0, 2: 1, 3: 1, 4: 2, 5: 3, 6: 4, 7: ...   \n",
      "1  {0: 0, 1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: ...   \n",
      "2  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...   \n",
      "3  {0: 0, 1: 0, 2: 1, 3: 2, 4: 2, 5: 3, 6: 4, 7: ...   \n",
      "4  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...   \n",
      "\n",
      "                                            char2tok  \\\n",
      "0  {0: 0, 1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 2, 7: ...   \n",
      "1  {0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 2, 7: ...   \n",
      "2  {0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 2, 7: ...   \n",
      "3  {0: 0, 1: 0, 2: 1, 3: 1, 4: 1, 5: 1, 6: 2, 7: ...   \n",
      "4  {0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 2, 7: ...   \n",
      "\n",
      "                                           char2word  \n",
      "0  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 1, 7: ...  \n",
      "1  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 1, 7: ...  \n",
      "2  {0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 2, 7: ...  \n",
      "3  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 1, 7: ...  \n",
      "4  {0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 2, 7: ...  \n",
      "0    {0: 0, 1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 2, 7: ...\n",
      "1    {0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 2, 7: ...\n",
      "2    {0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 2, 7: ...\n",
      "3    {0: 0, 1: 0, 2: 1, 3: 1, 4: 1, 5: 1, 6: 2, 7: ...\n",
      "4    {0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 2, 7: ...\n",
      "Name: char2tok, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sample_text['tok2char']=sample_text.apply(get_tok2char,axis=1)\n",
    "sample_text['word2char']=sample_text.apply(lambda x: get_word2char(x, \" \"),axis=1)\n",
    "sample_text['word2tok']=sample_text.apply(lambda x: get_word2tok(x),axis=1)\n",
    "sample_text['tok2word']=sample_text['word2tok'].apply(kv_swap)\n",
    "sample_text['char2tok']=sample_text['tok2char'].apply(kv_swap)\n",
    "sample_text['char2word']=sample_text['word2char'].apply(kv_swap)\n",
    "print(sample_text.head())\n",
    "print(sample_text['char2tok'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "e857ece6-47e3-438c-a973-1737b091f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_labels=predictions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a1217a89-3bd5-4bb3-ac68-53137b0da819",
   "metadata": {},
   "outputs": [],
   "source": [
    "## predictions_labelsì ë¤ì merge\n",
    "predictions_labels = pd.merge(predictions_labels, sample_text[['text','tokens','words','char','tok2char', 'word2char', 'word2tok','tok2word', 'char2tok', 'char2word']],on='text',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "e8010b3d-0cd8-4f6b-b117-acd2a807f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert list of indices into a list of binary labels of length len(seq)        \n",
    "predictions_labels['pred_binary']=predictions_labels.apply(lambda x: index2binary(x, len_colname=\"tokens\", index_colname=\"pred\"),axis=1)\n",
    "\n",
    "predictions_labels['pred_word']=predictions_labels.apply(get_pred_word,axis=1)\n",
    "predictions_labels['pred_binary_word']=predictions_labels.apply(get_pred_binary_word,axis=1)\n",
    "predictions_labels['pred_scores_word']=predictions_labels.apply(lambda x: get_pred_scores_word(x,method='max'), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "ddf890aa-042d-443c-9bbf-af6de476a1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_labels['pred_char']=predictions_labels.apply(get_pred_char,axis=1)\n",
    "predictions_labels['pred_binary_char']=predictions_labels.apply(get_pred_binary_char,axis=1)\n",
    "predictions_labels['pred_scores_char']=predictions_labels.apply(lambda x: get_pred_scores_char(x), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "34f27b2a-fd1a-4f9f-aa78-aea6bccf3688",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type=\"tsd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "c25a4971-3017-4039-92e2-27c2d023b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_type == \"gpt2\":\n",
    "    ## convert list of indices into a list of binary labels of length len(seq)        \n",
    "    predictions_labels['labels_binary'] = predictions_labels['labels'].apply(lambda x: [1 if i >= 0.5 else 0 for i in x])\n",
    "    predictions_labels['labels_binary_word']=predictions_labels.apply(lambda x: get_labels_binary_word(x, dataset_type=dataset_type),axis=1)\n",
    "    predictions_labels['labels_binary_char']=predictions_labels.apply(get_labels_binary_char,axis=1)\n",
    "elif dataset_type == \"tsd\":\n",
    "    predictions_labels['labels_binary_char'] = predictions_labels.apply(lambda x: index2binary(x, len_colname=\"char\", index_colname=\"labels_index_char\"),axis=1)\n",
    "    predictions_labels['labels_binary_word']=predictions_labels.apply(lambda x: get_labels_binary_word(x, dataset_type=dataset_type),axis=1)\n",
    "    predictions_labels['labels_binary']=predictions_labels.apply(lambda x: get_labels_binary_token(x, dataset_type=dataset_type),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "74188180-7b8b-4fca-96d3-314ac9a60f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### Calculate Token-level Metrics\n",
    "predictions_labels['f1']=predictions_labels.apply(apply_f1,axis=1)\n",
    "predictions_labels['f2']=predictions_labels.apply(apply_f2,axis=1)\n",
    "predictions_labels['rr']=predictions_labels.apply(get_rr, axis=1)\n",
    "predictions_labels['ap']=predictions_labels.apply(apply_ap,axis=1)\n",
    "predictions_labels['precision']=predictions_labels.apply(apply_precision,axis=1)\n",
    "predictions_labels['recall']=predictions_labels.apply(apply_recall,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "484e2016-078d-4b14-abb7-f06f18cd1b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Calculate Word-level Metrics\n",
    "predictions_labels['f1_word']=predictions_labels.apply(lambda x: apply_f1(x,\"_word\"),axis=1)\n",
    "predictions_labels['f2_word']=predictions_labels.apply(lambda x: apply_f2(x,\"_word\"),axis=1)\n",
    "predictions_labels['ap_word']=predictions_labels.apply(lambda x: get_rr(x,\"_word\"),axis=1)\n",
    "predictions_labels['rr_word']=predictions_labels.apply(lambda x: apply_ap(x,\"_word\"),axis=1)\n",
    "predictions_labels['precision_word']=predictions_labels.apply(lambda x: apply_precision(x,\"_word\"),axis=1)\n",
    "predictions_labels['recall_word']=predictions_labels.apply(lambda x: apply_recall(x,\"_word\"),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "da95ad44-2be1-4cea-80a6-815623d8e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Calculate Character-level Metrics\n",
    "predictions_labels['f1_char']=predictions_labels.apply(lambda x: apply_f1(x,\"_char\"),axis=1)\n",
    "predictions_labels['f2_char']=predictions_labels.apply(lambda x: apply_f2(x,\"_char\"),axis=1)\n",
    "predictions_labels['ap_char']=predictions_labels.apply(lambda x: get_rr(x,\"_char\"),axis=1)\n",
    "predictions_labels['rr_char']=predictions_labels.apply(lambda x: apply_ap(x,\"_char\"),axis=1)\n",
    "predictions_labels['precision_char']=predictions_labels.apply(lambda x: apply_precision(x,\"_char\"),axis=1)\n",
    "predictions_labels['recall_char']=predictions_labels.apply(lambda x: apply_recall(x,\"_char\"),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "cbe805a2-25cc-4767-bf38-d3949de6ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf1 = predictions_labels['f1'].mean()\n",
    "mf2 = predictions_labels['f2'].mean()\n",
    "mrr = predictions_labels['rr'].mean()\n",
    "map_score =  predictions_labels['ap'].mean()\n",
    "precision = predictions_labels['precision'].mean()\n",
    "recall = predictions_labels['recall'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "f6dea8e0-5a0c-4edf-b765-23e3607b9f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.40339010277386655,\n",
       " 0.5664254168183847,\n",
       " 0.8908883354088833,\n",
       " 0.8548534323932531,\n",
       " 0.2988147430522856,\n",
       " 0.9049444498054332)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf1, mf2,mrr, map_score, precision,recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import argparse\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "os.chdir('/data/hyeryung/mucoco')\n",
    "from itertools import repeat\n",
    "import torch.multiprocessing as mp\n",
    "from typing import List\n",
    "from itertools import permutations,product\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "# from datasets import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForMaskedLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "from new_module.utils.robertacustom import RobertaCustomForSequenceClassification\n",
    "from new_module.locate.new_locate_utils import *\n",
    "import new_module.losses as lossbuilder\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from typing import List,Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup for prototyping\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(os.environ.get(\"LOGGING_LEVEL\", logging.DEBUG))\n",
    "\n",
    "config={#'model_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint'],\n",
    "        # 'tokenizer_paths':['gpt2-large','/shared/s3/lab07/hyeryung/loc_edit/roberta-base-jigsaw-toxicity-classifier-energy-training/step_600_best_checkpoint'],\n",
    "        'model_paths':['gpt2-large','/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint'],\n",
    "        'tokenizer_paths':['gpt2-large','/data/hyeryung/loc_edit/models/roberta-base-jigsaw-toxicity-classifier-energy-training/step_1000_best_checkpoint'],\n",
    "        'model_types': [\"AutoModelForCausalLM\", \"AutoModelForSequenceClassification\"],\n",
    "        'cache_dir': \"/data/hyeryung/hf_cache\",\n",
    "        'target_type': \"embeds\",\n",
    "        'method': \"mlm-beamsearch-v0\",\n",
    "       'losses': [\"gpt2\", \"classification_no_prefix_logprobloss\"],\n",
    "       'target_label_ids': [0,0] ,\n",
    "       'build_loss_dict': {\"coeff_steps\": 200, \"coeff_pattern\": \"constant\", \"loss_type\": \"xentropy\", \"length_normalize\": False, \"AR_temperature\": 1.0, \"AR_top_k\": 0, \"AR_top_p\": 0.96, \"max_output_length\": 20},\n",
    "       'min_epsilons': [0.75],\n",
    "       'source_data': 'new_module/data/toxicity-avoidance/testset_gpt2_2500.jsonl',\n",
    "       'locate_unit': 'token',\n",
    "       'locate_method': 'grad_norm',\n",
    "       'device': 'cuda:7',\n",
    "       'k_per_location': 3,\n",
    "       'closs_weight': 0.9,\n",
    "       'beam_size': 3,\n",
    "       'selection_criteria':\"weighted_sum\"}\n",
    "\n",
    "class dummyArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "build_loss_args = dummyArgs(**config[\"build_loss_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /gpt2-large/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "name2tokenizer = {}\n",
    "name2model = {}\n",
    "name2config = {}\n",
    "loss2tokenizer = {}\n",
    "embed_luts = []\n",
    "primary_model = None\n",
    "for i, model_path in enumerate(config[\"model_paths\"]):\n",
    "    if (\n",
    "        model_path not in name2model\n",
    "    ):  # making sure we are not loading the model twice in case some constraints use the same model.\n",
    "        try:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=True,\n",
    "            )\n",
    "        except:\n",
    "            name2tokenizer[model_path] = AutoTokenizer.from_pretrained(\n",
    "                config[\"tokenizer_paths\"][i],\n",
    "                cache_dir=config[\"cache_dir\"],\n",
    "                use_fast=False,\n",
    "            )\n",
    "\n",
    "        name2config[model_path] = AutoConfig.from_pretrained(\n",
    "            model_path, cache_dir=config[\"cache_dir\"]\n",
    "        )\n",
    "\n",
    "        if \"Custom\" in config[\"model_types\"][i]:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(utils, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(\n",
    "                getattr(transformers, config[\"model_types\"][i]).from_pretrained(\n",
    "                    model_path,\n",
    "                    config=name2config[model_path],\n",
    "                    cache_dir=config[\"cache_dir\"],\n",
    "                )\n",
    "            )\n",
    "        name2model[model_path].eval()\n",
    "        name2model[model_path].to(config['device'])\n",
    "\n",
    "    input_embeds = name2model[model_path].get_input_embeddings()\n",
    "    if isinstance(input_embeds, torch.nn.Sequential):\n",
    "        input_embeds = input_embeds[0]\n",
    "    embed_luts.append(input_embeds)\n",
    "\n",
    "    if config[\"target_type\"] == \"embeds\":\n",
    "        embed_luts[-1].requires_grad = False\n",
    "\n",
    "    if i == 0:\n",
    "        primary_model = name2model[model_path]\n",
    "\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "mlm = None if config[\"method\"] == \"mlm-beamsearch-v2\" else AutoModelForMaskedLM.from_pretrained(\"roberta-base\").to(config['device'])  \n",
    "\n",
    "lossfns = []\n",
    "for i, loss in enumerate(config[\"losses\"]):\n",
    "    lossfns.append(\n",
    "        lossbuilder.build_loss(\n",
    "            loss,\n",
    "            name2model[config[\"model_paths\"][i]],\n",
    "            name2tokenizer[config[\"model_paths\"][i]],\n",
    "            build_loss_args,\n",
    "        )\n",
    "    )\n",
    "    lossfns[i].tokenizer.add_special_tokens({\"mask_token\": mlm_tokenizer.mask_token})\n",
    "    loss2tokenizer[loss] = lossfns[i].tokenizer\n",
    "\n",
    "primary_tokenizer = name2tokenizer['gpt2-large']\n",
    "secondary_tokenizer = list(name2tokenizer.values())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel(outputs_df):\n",
    "    outputs_df=outputs_df.explode('generations',ignore_index=True)\n",
    "    outputs_df['prompt']=outputs_df['prompt'].apply(lambda x: x['text'])\n",
    "    outputs_df['generations']=outputs_df['generations'].apply(lambda x: x['text'] if isinstance(x, dict) else x)\n",
    "    outputs_df = outputs_df.dropna().reset_index(drop=True)\n",
    "    return outputs_df\n",
    "toxic_gen=pd.read_json('new_module/data/toxicity-avoidance/dev_set.jsonl',lines=True)\n",
    "toxic_gen=unravel(toxic_gen)\n",
    "source_text = '“I went to use the bathroom, and I went in and Lance was bent'\n",
    "prediction = toxic_gen.loc[toxic_gen['prompt']==source_text,'generations'].tolist()\n",
    "ckpt_path = config['model_paths'][1]\n",
    "model = name2model[ckpt_path] #AutoModelForSequenceClassification.from_pretrained(ckpt_path)\n",
    "tokenizer = name2tokenizer[ckpt_path] #AutoTokenizer.from_pretrained(ckpt_path)\n",
    "\n",
    "model = model.to(config['device'])\n",
    "loc_machine=LocateMachine(model,tokenizer)\n",
    "masked_text = loc_machine.locate_main(prediction, \"grad_norm\", max_num_tokens = 6, unit=config['locate_unit'], num_layer=10, label_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace tokens at the indices with mask tokens\n",
    "inputs = mlm_tokenizer(\n",
    "    masked_text, return_tensors=\"pt\", padding=True, truncation=True\n",
    ")\n",
    "inputs = inputs.to(config['device']) \n",
    "masked_sequence=inputs['input_ids']\n",
    "\n",
    "## make predictions for the masked indices\n",
    "with torch.no_grad():\n",
    "    logits = mlm(**inputs).logits\n",
    "\n",
    "special_token_ids = mlm_tokenizer.convert_tokens_to_ids(mlm_tokenizer.all_special_tokens)\n",
    "logits[:, :, special_token_ids] = -float(\"inf\")\n",
    "\n",
    "indices_in_mlm_tokens = (\n",
    "    inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    ").nonzero(as_tuple=True)\n",
    "\n",
    "## get top k tokens for each index\n",
    "predicted_token_ids = torch.topk(\n",
    "    logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "    k=config['k_per_location'],\n",
    "    dim=-1,\n",
    ")\n",
    "# print(f\"predicted_token_ids: {predicted_token_ids}\")\n",
    "# print(f\"mlm_tokenizer.batch_decode(predicted_token_ids.indices): {mlm_tokenizer.batch_decode(predicted_token_ids.indices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_rerank(source_text:str, \n",
    "                    masked_sequence:torch.Tensor, \n",
    "                    indices_in_mlm_tokens:tuple,\n",
    "                    # predicted_token_ids:torch.return_types.topk,\n",
    "                    predicted_token_ids:torch.Tensor,\n",
    "                    mlm_tokenizer:transformers.AutoTokenizer, \n",
    "                    lossfns:List[lossbuilder.BaseLoss],\n",
    "                    config:dict):\n",
    "    \"\"\"params: \n",
    "    source_text(prompt) should be text. \n",
    "    masked_sequence should be token ids tokenized by MLM's tokenizer.\n",
    "    indices_in_mlm_tokens should be a result of running \n",
    "    `indices_in_mlm_tokens = (\n",
    "                                inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "                                ).nonzero(as_tuple=True)`\n",
    "    predicted_token_ids should be a result of running\n",
    "    `predicted_token_ids = torch.topk(\n",
    "                            logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "                            k=config['k_per_location'],\n",
    "                            dim=-1,).indices`\n",
    "    \"\"\"\n",
    "    \n",
    "    hypotheses = masked_sequence[:, None, :].repeat((1,config['beam_size'],1))\n",
    "    edit_indices = sorted(list(set(indices_in_mlm_tokens[1].tolist())))\n",
    "\n",
    "    # for i in tqdm(edit_indices,total=len(edit_indices)):\n",
    "    for i in edit_indices:\n",
    "        \n",
    "        batch_ids_to_edit = indices_in_mlm_tokens[0][indices_in_mlm_tokens[1]==i]\n",
    "\n",
    "        tmp_hypotheses = hypotheses[batch_ids_to_edit].detach().clone()\n",
    "        tmp_hypotheses=tmp_hypotheses.repeat((1,config['k_per_location'],1))\n",
    "\n",
    "        # candidates = predicted_token_ids.indices[(indices_in_mlm_tokens[1]==i).nonzero().squeeze(-1),:]\n",
    "        candidates = predicted_token_ids[(indices_in_mlm_tokens[1]==i).nonzero().squeeze(-1),:]\n",
    "        candidates = candidates[:, :, None].repeat((1,1, config['beam_size'])).reshape(candidates.shape[0], -1,1)\n",
    "\n",
    "        tmp_hypotheses = torch.cat((tmp_hypotheses[:, :, :i], candidates),dim=-1) ## tmp_hypotheses: [(a,b,c),(a,b,c), ..., (a,b,c)], candidates: [(p,p,p), (q,q,q), ..., (v,v,v)]\n",
    "        tmp_hypotheses = tmp_hypotheses.reshape(-1, tmp_hypotheses.shape[-1])\n",
    "        \n",
    "        loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "        curr_loss = torch.zeros(tmp_hypotheses.shape[0]).to(config['device'])\n",
    "        for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "            if config['method'] == 'mlm-beamsearch-v1' and lossid > 0:\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                    source_text, mlm_tokenizer.batch_decode(tmp_hypotheses,skip_special_tokens=True),\n",
    "                    label_id=config['target_label_ids'][lossid],\n",
    "                )\n",
    "            torch.cuda.empty_cache()\n",
    "            curr_loss += loss_weights[lossid] * lossvalue\n",
    "            \n",
    "        curr_loss = torch.stack(torch.split(curr_loss, config['beam_size'] * config['k_per_location'], dim=0),dim=0)\n",
    "        top_beams=torch.topk(curr_loss, k=(config['beam_size']*2+1), dim=-1, largest=False).indices\n",
    "\n",
    "        tmp_hypotheses = torch.split(tmp_hypotheses, config['beam_size'] * config['k_per_location'], dim=0) ## 아래의 작업을 더 간단히 할 수 있는 방법?\n",
    "        tmp_hypotheses = torch.stack([x[top_beams[j]] for j, x in enumerate(tmp_hypotheses)],dim=0)\n",
    "        tmp_hypotheses = torch.unique(tmp_hypotheses, dim=1)[:, :config['beam_size'], :]\n",
    "        \n",
    "        hypotheses[batch_ids_to_edit,:, i]=tmp_hypotheses[:, :, i]\n",
    "            \n",
    "    return [mlm_tokenizer.batch_decode(hypotheses[j], skip_special_tokens=True) for j in range(hypotheses.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get_combi_hypotheses \n",
    "def get_combi_hypotheses(masked_sequence:torch.Tensor, \n",
    "                 indices_in_mlm_tokens:tuple,\n",
    "                 predicted_token_ids:torch.Tensor,\n",
    "                 mlm_tokenizer:transformers.AutoTokenizer,\n",
    "                 config:dict) -> List[str]:\n",
    "    \"\"\"params: \n",
    "    masked_sequence should be token ids tokenized by MLM's tokenizer.\n",
    "    indices_in_mlm_tokens should be a result of running \n",
    "    `indices_in_mlm_tokens = (\n",
    "                                inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "                                ).nonzero(as_tuple=True)`\n",
    "    predicted_token_ids should be a result of running\n",
    "    `predicted_token_ids = torch.topk(\n",
    "                            logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "                            k=config['k_per_location'],\n",
    "                            dim=-1,).indices`\n",
    "    \"\"\"\n",
    "\n",
    "    k = config['k_per_location']\n",
    "    hypotheses = []\n",
    "    num_batches = masked_sequence.shape[0]\n",
    "    for i in range(num_batches):\n",
    "        \n",
    "        l = (indices_in_mlm_tokens[0] == i).sum().item()\n",
    "        tok_cand_combos = list(product(range(k),repeat=l))\n",
    "        \n",
    "        tmp_hypotheses = masked_sequence[i,:].repeat((k**l,1))\n",
    "        tmp_hypotheses[:, indices_in_mlm_tokens[1][indices_in_mlm_tokens[0] == i]] = \\\n",
    "            predicted_token_ids[indices_in_mlm_tokens[0] == i, tok_cand_combos]\n",
    "            \n",
    "        tmp_dec_seq = mlm_tokenizer.batch_decode(\n",
    "                    tmp_hypotheses, skip_special_tokens=True\n",
    "            )\n",
    "        hypotheses.append(tmp_dec_seq)\n",
    "    return hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:50<00:00,  5.01s/it]\n",
      "100%|██████████| 10/10 [00:50<00:00,  5.08s/it]\n",
      "100%|██████████| 10/10 [00:52<00:00,  5.26s/it]\n",
      "100%|██████████| 10/10 [00:52<00:00,  5.27s/it]\n",
      "100%|██████████| 10/10 [00:51<00:00,  5.13s/it]\n",
      "100%|██████████| 10/10 [00:52<00:00,  5.21s/it]\n",
      "100%|██████████| 10/10 [00:51<00:00,  5.11s/it]\n",
      "100%|██████████| 10/10 [00:51<00:00,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.1 s ± 680 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "\n",
    "# my_hypotheses = get_combi_hypotheses(masked_sequence, \n",
    "#                             indices_in_mlm_tokens,\n",
    "#                             predicted_token_ids.indices,\n",
    "#                             mlm_tokenizer,\n",
    "#                             config)\n",
    "\n",
    "# loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "# selection_criteria = \"weighted_sum\"\n",
    "# hypotheses = deepcopy(my_hypotheses)\n",
    "# best_ixes = []\n",
    "# best_weighted_loss = []\n",
    "# best_allsat = []\n",
    "# best_logging_loss = []\n",
    "# num_batches = masked_sequence.shape[0]\n",
    "# for i in tqdm(range(num_batches)):\n",
    "       \n",
    "#     curr_loss = torch.zeros(len(hypotheses[i])).to(config['device'])\n",
    "#     logging_loss = torch.zeros((len(hypotheses[i]),2)).to(config['device'])\n",
    "\n",
    "#     hyp_data = CustomDataset(hypotheses[i])\n",
    "#     data_loader = DataLoader(hyp_data,batch_size=64)\n",
    "\n",
    "#     for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "#         lossvalues=[]\n",
    "#         with torch.no_grad():\n",
    "#             for batch in data_loader:\n",
    "#                 lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "#                     source_text, batch,\n",
    "#                     label_id=config['target_label_ids'][lossid],\n",
    "#                 )\n",
    "#                 lossvalues.append(lossvalue)\n",
    "#                 torch.cuda.empty_cache()\n",
    "#         lossvalue = torch.cat(lossvalues,dim=0)\n",
    "#         curr_loss += loss_weights[lossid] * lossvalue\n",
    "#         logging_loss[:, lossid] = lossvalue.clone()\n",
    "        \n",
    "#     allsat_ix = torch.where(logging_loss[:,1]> -np.log(config[\"min_epsilons\"][0]))[0].squeeze(0)\n",
    "#     if (allsat_ix.shape[0] > 0) and (selection_criteria == \"allsat_primary\"):\n",
    "#         best_ix = allsat_ix[curr_loss[allsat_ix].argmin()]\n",
    "#     else: ## in case selection_criteria == \"weighted_sum\" or allsat is all False\n",
    "#         best_ix = torch.argmin(curr_loss)\n",
    "\n",
    "    \n",
    "#     hypotheses[i]=hypotheses[i][best_ix]\n",
    "#     best_weighted_loss.append(curr_loss[best_ix].item())\n",
    "#     best_allsat.append(1 if best_ix in allsat_ix else 0)\n",
    "#     best_logging_loss.append(logging_loss[best_ix].cpu().numpy())\n",
    "    \n",
    "#     del curr_loss, logging_loss\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def final_reranking(hypotheses:List[List[str]],\n",
    "                    lossfns:List[lossbuilder.BaseLoss],\n",
    "                    config:dict,\n",
    "                    batch_size:int=64) -> Tuple[List[str],List[float],List[int],List[List[float]]]:\n",
    "    \"\"\"params: \n",
    "        hypotheses: list of hypotheses of editing results\n",
    "        lossfns:\n",
    "        config:\n",
    "        batch_size:             \n",
    "    returns:\n",
    "        hypotheses: list of one best hypothesis(editing result) for each of original texts. length same as masked_sequence.shape[0]\n",
    "        best_weighted_loss: list of weighted loss for the best hypotheses.\n",
    "        best_allsat: list of indicator(1,0) whether the best hypotheses satisfy cutoff (min_epsilons) for constraint energy score.\n",
    "        best_logging_loss: list of list of fluency energy score and constraint energy score for each best hypothesis.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, hypotheses_data:List[str]):\n",
    "            self.hypotheses_data = hypotheses_data\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.hypotheses_data)\n",
    "\n",
    "        def __getitem__(self, idx:int):\n",
    "            return self.hypotheses_data[idx]\n",
    "        \n",
    "        def __getitems__(self, idx:List[int]):\n",
    "            return [self.hypotheses_data[j] for j in idx]\n",
    "    \n",
    "    final_hypotheses = []\n",
    "    best_weighted_loss = []\n",
    "    best_allsat = []\n",
    "    best_logging_loss = []\n",
    "    \n",
    "    loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "    \n",
    "    for i in tqdm(range(len(hypotheses))):\n",
    "        curr_loss = torch.zeros(len(hypotheses[i])).to(config['device'])\n",
    "        logging_loss = torch.zeros((len(hypotheses[i]),2)).to(config['device'])\n",
    "        data_loader = DataLoader(CustomDataset(hypotheses[i]),batch_size=batch_size)\n",
    "\n",
    "        for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "            lossvalues=[]\n",
    "            with torch.no_grad():\n",
    "                for batch in data_loader:\n",
    "                    lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                        source_text, batch,\n",
    "                        label_id=config['target_label_ids'][lossid],\n",
    "                    )\n",
    "                    lossvalues.append(lossvalue)\n",
    "                    torch.cuda.empty_cache()\n",
    "            lossvalue = torch.cat(lossvalues,dim=0)\n",
    "            curr_loss += loss_weights[lossid] * lossvalue\n",
    "            logging_loss[:, lossid] = lossvalue.clone()\n",
    "            \n",
    "        allsat_ix = torch.where(logging_loss[:,1]> -math.log(config[\"min_epsilons\"][0]))[0].squeeze(0)\n",
    "        if (allsat_ix.shape[0] > 0) and (config['selection_criteria'] == \"allsat_primary\"):\n",
    "            best_ix = allsat_ix[curr_loss[allsat_ix].argmin()]\n",
    "        else: ## in case config['selection_criteria'] == \"weighted_sum\" or allsat is all False\n",
    "            best_ix = torch.argmin(curr_loss)\n",
    "\n",
    "        final_hypotheses.append(hypotheses[i][best_ix])\n",
    "        best_weighted_loss.append(curr_loss[best_ix].item())\n",
    "        best_allsat.append(1 if best_ix in allsat_ix else 0)\n",
    "        best_logging_loss.append(logging_loss[best_ix].cpu().tolist())\n",
    "    \n",
    "        del curr_loss, logging_loss\n",
    "        torch.cuda.empty_cache()\n",
    "    return final_hypotheses, best_weighted_loss, best_allsat, best_logging_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypotheses = beam_rerank(source_text, \n",
    "#                         masked_sequence, \n",
    "#                         indices_in_mlm_tokens,\n",
    "#                         predicted_token_ids.indices,\n",
    "#                         mlm_tokenizer, \n",
    "#                         lossfns,\n",
    "#                         config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 25.15it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 25.57it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 25.10it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 25.80it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 25.42it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 25.59it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 25.06it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 25.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.31 s ± 15.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "my_hypotheses = beam_rerank(source_text, \n",
    "                        masked_sequence, \n",
    "                        indices_in_mlm_tokens,\n",
    "                        predicted_token_ids.indices,\n",
    "                        mlm_tokenizer, \n",
    "                        lossfns,\n",
    "                        config)\n",
    "final_result = final_reranking(my_hypotheses,\n",
    "                                lossfns,\n",
    "                                config,\n",
    "                                batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.87s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.88s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.88s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.89s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.89s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.90s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.90s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 s ± 65.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "my_hypotheses = get_combi_hypotheses(masked_sequence, \n",
    "                            indices_in_mlm_tokens,\n",
    "                            predicted_token_ids.indices,\n",
    "                            mlm_tokenizer,\n",
    "                            config)\n",
    "final_result = final_reranking(my_hypotheses,\n",
    "                                lossfns,\n",
    "                                config,\n",
    "                                batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_beam_hypotheses(source_text:str, \n",
    "                    masked_sequence:torch.Tensor, \n",
    "                    indices_in_mlm_tokens:Tuple[torch.Tensor],\n",
    "                    predicted_token_ids:torch.Tensor,\n",
    "                    mlm_tokenizer:transformers.AutoTokenizer, \n",
    "                    lossfns:List[lossbuilder.BaseLoss],\n",
    "                    config:dict) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    A function to get hypotheses of beam size via editing beam search with reranking.\n",
    "    Run this function if config['method'] == 'mlm-beamsearch-v0' or config['method'] == 'mlm-beamsearch-v1'\n",
    "    If config['method'] == 'mlm-beamsearch-v1', rerank beam only with fluency energy.\n",
    "    If config['method'] == 'mlm-beamsearch-v0', rerank beam with a weighted sum of fluency and constraint energy.\n",
    "    \n",
    "    #ToDo\n",
    "    #Implement mlm-beamsearch-v0 with allsat-primary and compare \n",
    "    \n",
    "    params: \n",
    "        source_text: a prompt text \n",
    "        masked_sequence: token ids of original generation text with located indices masked. tokenized by MLM's tokenizer.\n",
    "        indices_in_mlm_tokens: a result of running \n",
    "                                    `indices_in_mlm_tokens = (\n",
    "                                                                inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "                                                                ).nonzero(as_tuple=True)`\n",
    "        predicted_token_ids: a result of running\n",
    "                                    `predicted_token_ids = torch.topk(\n",
    "                                                                logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "                                                                k=config['k_per_location'],\n",
    "                                                                dim=-1,).indices`\n",
    "        mlm_tokenizer: tokenizer of MLM\n",
    "        lossfns: a list of loss functions\n",
    "        config: a dictionary of configurations\n",
    "    \n",
    "    returns:\n",
    "        hypotheses: a list of a list of the beam number of hypotheses for each sample         \n",
    "    \"\"\"\n",
    "    \n",
    "    hypotheses = masked_sequence[:, None, :].repeat((1,config['beam_size'],1))\n",
    "    edit_indices = sorted(list(set(indices_in_mlm_tokens[1].tolist())))\n",
    "\n",
    "    for i in edit_indices:\n",
    "        \n",
    "        batch_ids_to_edit = indices_in_mlm_tokens[0][indices_in_mlm_tokens[1]==i]\n",
    "\n",
    "        tmp_hypotheses = hypotheses[batch_ids_to_edit].detach().clone()\n",
    "        tmp_hypotheses=tmp_hypotheses.repeat((1,config['k_per_location'],1))\n",
    "\n",
    "        candidates = predicted_token_ids[(indices_in_mlm_tokens[1]==i).nonzero().squeeze(-1),:]\n",
    "        candidates = candidates[:, :, None].repeat((1,1, config['beam_size'])).reshape(candidates.shape[0], -1,1)\n",
    "\n",
    "        tmp_hypotheses = torch.cat((tmp_hypotheses[:, :, :i], candidates),dim=-1) ## tmp_hypotheses: [(a,b,c),(a,b,c), ..., (a,b,c)], candidates: [(p,p,p), (q,q,q), ..., (v,v,v)]\n",
    "        tmp_hypotheses = tmp_hypotheses.reshape(-1, tmp_hypotheses.shape[-1])\n",
    "        \n",
    "        loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "        curr_loss = torch.zeros(tmp_hypotheses.shape[0]).to(config['device'])\n",
    "        for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "            if config['method'] == 'mlm-beamsearch-v1' and lossid > 0:\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                    source_text, mlm_tokenizer.batch_decode(tmp_hypotheses,skip_special_tokens=True),\n",
    "                    label_id=config['target_label_ids'][lossid],\n",
    "                )\n",
    "            torch.cuda.empty_cache()\n",
    "            curr_loss += loss_weights[lossid] * lossvalue\n",
    "            \n",
    "        curr_loss = torch.stack(torch.split(curr_loss, config['beam_size'] * config['k_per_location'], dim=0),dim=0)\n",
    "        top_beams=torch.topk(curr_loss, k=(config['beam_size']*2+1), dim=-1, largest=False).indices\n",
    "\n",
    "        tmp_hypotheses = torch.split(tmp_hypotheses, config['beam_size'] * config['k_per_location'], dim=0) ## 아래의 작업을 더 간단히 할 수 있는 방법?\n",
    "        tmp_hypotheses = torch.stack([x[top_beams[j]] for j, x in enumerate(tmp_hypotheses)],dim=0)\n",
    "        tmp_hypotheses = torch.unique(tmp_hypotheses, dim=1)[:, :config['beam_size'], :]\n",
    "        \n",
    "        hypotheses[batch_ids_to_edit,:, i]=tmp_hypotheses[:, :, i]\n",
    "            \n",
    "    return [mlm_tokenizer.batch_decode(hypotheses[j], skip_special_tokens=True) for j in range(hypotheses.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_beam_hypotheses(source_text:str, \n",
    "                    masked_sequence:torch.Tensor, \n",
    "                    indices_in_mlm_tokens:Tuple[torch.Tensor],\n",
    "                    predicted_token_ids:torch.Tensor,\n",
    "                    mlm_tokenizer:transformers.AutoTokenizer, \n",
    "                    lossfns:List[lossbuilder.BaseLoss],\n",
    "                    config:dict) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    A function to get hypotheses of beam size via editing beam search with reranking.\n",
    "    Run this function if config['method'] == 'mlm-beamsearch-v0' or config['method'] == 'mlm-beamsearch-v1'\n",
    "    If config['method'] == 'mlm-beamsearch-v1', rerank beam only with fluency energy.\n",
    "    If config['method'] == 'mlm-beamsearch-v0', rerank beam with a weighted sum of fluency and constraint energy.\n",
    "    \n",
    "    #ToDo\n",
    "    #Implement mlm-beamsearch-v0 with allsat-primary and compare \n",
    "    \n",
    "    params: \n",
    "        source_text: a prompt text \n",
    "        masked_sequence: token ids of original generation text with located indices masked. tokenized by MLM's tokenizer.\n",
    "        indices_in_mlm_tokens: a result of running \n",
    "                                    `indices_in_mlm_tokens = (\n",
    "                                                                inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "                                                                ).nonzero(as_tuple=True)`\n",
    "        predicted_token_ids: a result of running\n",
    "                                    `predicted_token_ids = torch.topk(\n",
    "                                                                logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "                                                                k=config['k_per_location'],\n",
    "                                                                dim=-1,).indices`\n",
    "        mlm_tokenizer: tokenizer of MLM\n",
    "        lossfns: a list of loss functions\n",
    "        config: a dictionary of configurations\n",
    "    \n",
    "    returns:\n",
    "        hypotheses: a list of a list of the beam number of hypotheses for each sample         \n",
    "    \"\"\"\n",
    "    \n",
    "    hypotheses = masked_sequence[:, None, :].repeat((1,config['beam_size'],1))\n",
    "    edit_indices = sorted(list(set(indices_in_mlm_tokens[1].tolist())))\n",
    "\n",
    "    for i in edit_indices:\n",
    "        \n",
    "        batch_ids_to_edit = indices_in_mlm_tokens[0][indices_in_mlm_tokens[1]==i]\n",
    "\n",
    "        tmp_hypotheses = hypotheses[batch_ids_to_edit].detach().clone()\n",
    "        tmp_hypotheses=tmp_hypotheses.repeat((1,config['k_per_location'],1))\n",
    "\n",
    "        candidates = predicted_token_ids[(indices_in_mlm_tokens[1]==i).nonzero().squeeze(-1),:]\n",
    "        candidates = candidates[:, :, None].repeat((1,1, config['beam_size'])).reshape(candidates.shape[0], -1,1)\n",
    "\n",
    "        tmp_hypotheses = torch.cat((tmp_hypotheses[:, :, :i], candidates),dim=-1) ## tmp_hypotheses: [(a,b,c),(a,b,c), ..., (a,b,c)], candidates: [(p,p,p), (q,q,q), ..., (v,v,v)]\n",
    "        tmp_hypotheses = tmp_hypotheses.reshape(-1, tmp_hypotheses.shape[-1])\n",
    "        \n",
    "        loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "        curr_loss = torch.zeros(tmp_hypotheses.shape[0]).to(config['device'])\n",
    "        for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "            if config['method'] == 'mlm-beamsearch-v1' and lossid > 0:\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "                    source_text, mlm_tokenizer.batch_decode(tmp_hypotheses,skip_special_tokens=True),\n",
    "                    label_id=config['target_label_ids'][lossid],\n",
    "                )\n",
    "            torch.cuda.empty_cache()\n",
    "            curr_loss += loss_weights[lossid] * lossvalue\n",
    "            \n",
    "        curr_loss = torch.stack(torch.split(curr_loss, config['beam_size'] * config['k_per_location'], dim=0),dim=0)\n",
    "        top_beams=torch.topk(curr_loss, k=(config['beam_size']*2+1), dim=-1, largest=False).indices\n",
    "\n",
    "        tmp_hypotheses = torch.split(tmp_hypotheses, config['beam_size'] * config['k_per_location'], dim=0) ## 아래의 작업을 더 간단히 할 수 있는 방법?\n",
    "        tmp_hypotheses = torch.stack([x[top_beams[j]] for j, x in enumerate(tmp_hypotheses)],dim=0)\n",
    "        tmp_hypotheses = torch.unique(tmp_hypotheses, dim=1)[:, :config['beam_size'], :]\n",
    "        \n",
    "        hypotheses[batch_ids_to_edit,:, i]=tmp_hypotheses[:, :, i]\n",
    "            \n",
    "    return [mlm_tokenizer.batch_decode(hypotheses[j], skip_special_tokens=True) for j in range(hypotheses.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([inf, inf]),\n",
       "indices=tensor([0, 1]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(torch.Tensor([float(\"inf\"),float(\"inf\"),float(\"inf\")]), k=2, dim=-1, largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hypotheses = get_beam_hypotheses(source_text, \n",
    "                        masked_sequence, \n",
    "                        indices_in_mlm_tokens,\n",
    "                        predicted_token_ids.indices,\n",
    "                        mlm_tokenizer, \n",
    "                        lossfns,\n",
    "                        config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\" over me, the up. I could hear him starting to fight, but I didn't stop him. Thank God he had these clothes on me.\",\n",
       "  \" over me was he him. I could hear him starting to cry, but I didn't stop him. Thank God he had these headphones on me.\",\n",
       "  \" over me he I down. I could hear him starting to scream, but I didn't stop him. Thank God he had these handcuffs on me.\"],\n",
       " [' over like this and he was like.',\n",
       "  ' over like this and he was like:',\n",
       "  ' over like this and he was like…'],\n",
       " ['ran me and put his hand in my hair and my hand forced his hand up and down my back and then he put his hand back in my hair and he held it.This is',\n",
       "  'handed me and put his hands in my face and my hand forced his hand up and down my arm and then he put his hand back in my shirt and he squeezed it.This is',\n",
       "  'held me and put his hand in my hand and my hand forced his hand up and down my back and then he put his hand back in my pocket and he kissed it.This is'],\n",
       " [' over, looking for his phone, and so, like, yked the stall door open and I get out,\" he said.',\n",
       "  ' over, asked for his hand, and so, like, yked the stall door open and I get out,\" she said.',\n",
       "  ' over, waiting for his hand, and so, like, stroked the stall door open and I get out,\" Johnson said.'],\n",
       " ['behind me and I was his him. And I pulled away and he was holding me, and I was in fear.I had no idea what it was going to',\n",
       "  'to me and he was me me. And I pulled away and he was grabbing me, and I was in fear.I had no idea what it was going to',\n",
       "  'grab me and he started me me. And I pulled away and he was choking me, and I was in fear.I had no idea what it was going to'],\n",
       " [' over the bathroom. It was hot. The whole place was empty, the sink and the tub.',\n",
       "  ' over the shower. It was hot. The whole place was empty, the tub and the tub.',\n",
       "  ' over the tub. It was empty. The whole place was bare, the shower and the tub.'],\n",
       " [' over the game and then was standing over one of the other, and being a goon in his life.',\n",
       "  ' over the war and then was standing over one of the polices and being a peon in his death.',\n",
       "  ' over the bridge and then was running over one of the big boys and being a moron in his defense.'],\n",
       " [' over the end. He just kept all this time going like a kid trying to put a little zombo in.',\n",
       "  ' over the game. He just kept all this time going like a boxer trying to put a little zebo in.',\n",
       "  ' over the way. He just kept all this time going like a kid trying to put a little zippo in.'],\n",
       " [' over and I flicked my right down to a certain thing I had discovered before, with my eyes.',\n",
       "  ' over and I flicked my right down to a certain secret I had discovered before, with my imagination.',\n",
       "  ' over and I flicked the back down to a certain thing I had discovered before, with my imagination.'],\n",
       " [' over\\n\\n▲The first tree is seen\\n\\n▲The image of.',\n",
       "  ' over\\n\\n▲The first scene is seen\\n\\n▲A image is:',\n",
       "  ' over\\n\\n▲The first that is seen\\n\\n▲The is of.']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## combi_rerank \n",
    "# def combi_rerank(masked_sequence:torch.Tensor, \n",
    "#                  indices_in_mlm_tokens:tuple,\n",
    "#                  predicted_token_ids:torch.Tensor,\n",
    "#                  mlm_tokenizer:transformers.AutoTokenizer,\n",
    "#                  config:dict) -> Tuple[List[str],List[float],List[int],List[np.array]]:\n",
    "#     \"\"\"params: \n",
    "#         masked_sequence should be token ids tokenized by MLM's tokenizer.\n",
    "#         indices_in_mlm_tokens should be a result of running \n",
    "#         `indices_in_mlm_tokens = (\n",
    "#                                     inputs.input_ids == mlm_tokenizer.mask_token_id\n",
    "#                                     ).nonzero(as_tuple=True)`\n",
    "#         predicted_token_ids should be a result of running\n",
    "#         `predicted_token_ids = torch.topk(\n",
    "#                                 logits[indices_in_mlm_tokens[0], indices_in_mlm_tokens[1], :],\n",
    "#                                 k=config['k_per_location'],\n",
    "#                                 dim=-1,).indices`\n",
    "                                \n",
    "#     returns:\n",
    "#         hypotheses: list of one best hypothesis(editing result) for each of original texts. length same as masked_sequence.shape[0]\n",
    "#         best_weighted_loss: list of weighted loss for the best hypotheses.\n",
    "#         best_allsat: list of indicator whether the best hypotheses satisfy cutoff (min_epsilons) for constraint energy score.\n",
    "#         best_logging_loss: list of a tuple of fluency energy score and constraint energy score for each best hypothesis.\n",
    "#     \"\"\"\n",
    "\n",
    "#     num_batches = masked_sequence.shape[0]\n",
    "#     loss_weights = [1 - config['closs_weight'], config['closs_weight']]\n",
    "    \n",
    "#     hypotheses = []\n",
    "#     best_weighted_loss = []\n",
    "#     best_allsat = []\n",
    "#     best_logging_loss = []\n",
    "    \n",
    "#     for i in tqdm(range(num_batches)):\n",
    "        \n",
    "#         l = (indices_in_mlm_tokens[0] == i).sum().item()\n",
    "#         tok_cand_combos = list(product(range(config['k_per_location']),repeat=l))\n",
    "        \n",
    "#         tmp_hypotheses = masked_sequence[i,:].repeat((config['k_per_location']**l,1))\n",
    "#         tmp_hypotheses[:, indices_in_mlm_tokens[1][indices_in_mlm_tokens[0] == i]] = \\\n",
    "#             predicted_token_ids[indices_in_mlm_tokens[0] == i, tok_cand_combos]\n",
    "            \n",
    "#         tmp_dec_seq = mlm_tokenizer.batch_decode(\n",
    "#                     tmp_hypotheses, skip_special_tokens=True\n",
    "#             )\n",
    "        \n",
    "#         curr_loss = torch.zeros(len(tmp_dec_seq)).to(config['device'])\n",
    "#         logging_loss = torch.zeros((len(tmp_dec_seq),2)).to(config['device'])\n",
    "#         data_loader = DataLoader(CustomDataset(tmp_dec_seq),batch_size=64)\n",
    "\n",
    "#         for lossid, lossname in enumerate(config[\"losses\"]):\n",
    "#             lossvalues=[]\n",
    "#             with torch.no_grad():\n",
    "#                 for batch in data_loader:\n",
    "#                     lossvalue = lossfns[lossid].compute_gold_loss(\n",
    "#                         source_text, batch,\n",
    "#                         label_id=config['target_label_ids'][lossid],\n",
    "#                     )\n",
    "#                     lossvalues.append(lossvalue)\n",
    "#                     torch.cuda.empty_cache()\n",
    "#             lossvalue = torch.cat(lossvalues,dim=0)\n",
    "#             curr_loss += loss_weights[lossid] * lossvalue\n",
    "#             logging_loss[:, lossid] = lossvalue.clone()\n",
    "            \n",
    "#         allsat_ix = torch.where(logging_loss[:,1]> -np.log(config[\"min_epsilons\"][0]))[0].squeeze(0)\n",
    "#         if (allsat_ix.shape[0] > 0) and (config['selection_criteria'] == \"allsat_primary\"):\n",
    "#             best_ix = allsat_ix[curr_loss[allsat_ix].argmin()]\n",
    "#         else: ## in case config['selection_criteria'] == \"weighted_sum\" or allsat is all False\n",
    "#             best_ix = torch.argmin(curr_loss)\n",
    "\n",
    "#         hypotheses.append(tmp_dec_seq[best_ix])\n",
    "#         best_weighted_loss.append(curr_loss[best_ix].item())\n",
    "#         best_allsat.append(1 if best_ix in allsat_ix else 0)\n",
    "#         best_logging_loss.append(logging_loss[best_ix].cpu().numpy())\n",
    "        \n",
    "#         del curr_loss, logging_loss\n",
    "#         torch.cuda.empty_cache()\n",
    "#     return hypotheses, best_weighted_loss, best_allsat, best_logging_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:50<00:00,  5.05s/it]\n",
      "100%|██████████| 10/10 [00:51<00:00,  5.12s/it]\n",
      "100%|██████████| 10/10 [00:51<00:00,  5.14s/it]\n",
      "100%|██████████| 10/10 [00:50<00:00,  5.10s/it]\n",
      "100%|██████████| 10/10 [00:50<00:00,  5.08s/it]\n",
      "100%|██████████| 10/10 [00:50<00:00,  5.05s/it]\n",
      "100%|██████████| 10/10 [00:50<00:00,  5.08s/it]\n",
      "100%|██████████| 10/10 [00:50<00:00,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.9 s ± 310 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "# my_hypotheses = combi_rerank(masked_sequence, \n",
    "#                             indices_in_mlm_tokens,\n",
    "#                             predicted_token_ids.indices,\n",
    "#                             mlm_tokenizer,\n",
    "#                             config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/data/hyeryung/.conda/envs/loc-edit/lib/python3.8/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not going to be able to do that. I'm going to be able to do that\n",
      "I'm not going to be able to do that. I'll just have to go out and play\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Initializing the model and tokenizer for it\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "inputs = tokenizer([\"I'm not going to\"], return_tensors=\"pt\")\n",
    "\n",
    "# This shows a normal generate without any specific parameters\n",
    "summary_ids = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0])\n",
    "\n",
    "# This generates a penalty for repeated tokens\n",
    "penalized_ids = model.generate(**inputs, repetition_penalty=1.1)\n",
    "print(tokenizer.batch_decode(penalized_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aedb0a1cdf74bf1a362cddd7986c2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6fdd0393bf4a3b9e2232967c1d734b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36114f4f3e084cdfb410cc01793778dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2f56c046764b249fc29828af20d3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3a44c279014c2b91ad3a3eebf718a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ae47d43d664416bcaa686731185027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9ee581cfe34bd5b4513d0de8aeb3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/data/hyeryung/.conda/envs/loc-edit/lib/python3.8/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not going to be able to do that. I'm going to be able to do that\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Initializing the model and tokenizer for it\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "inputs = tokenizer([\"I'm not going to\"], return_tensors=\"pt\")\n",
    "\n",
    "# This shows a normal generate without any specific parameters\n",
    "summary_ids = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This generates a penalty for repeated tokens\n",
    "penalized_ids = model.generate(**inputs, repetition_penalty=1.1)\n",
    "print(tokenizer.batch_decode(penalized_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1301, 28628, 18435, 2159)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now let's control generation through a bias. Please note that the tokenizer is initialized differently!\n",
    "tokenizer_with_prefix_space = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", add_prefix_space=True)\n",
    "\n",
    "\n",
    "def get_tokens_as_tuple(word):\n",
    "    return tuple(tokenizer_with_prefix_space([word], add_special_tokens=False).input_ids[0])\n",
    "\n",
    "print(get_tokens_as_tuple(\"Trump\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.30.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['sequence_bias'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# If we add a negative bias without beam search, it may become \"stuck\" in a prefix without good continuations\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sequence_bias \u001b[38;5;241m=\u001b[39m {get_tokens_as_tuple(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrump\u001b[39m\u001b[38;5;124m\"\u001b[39m): \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10.0\u001b[39m}\n\u001b[0;32m----> 3\u001b[0m biased_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(biased_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/data/hyeryung/.conda/envs/loc-edit/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/hyeryung/.conda/envs/loc-edit/lib/python3.8/site-packages/transformers/generation/utils.py:1271\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1269\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m generation_config\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[0;32m-> 1271\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m logits_processor \u001b[38;5;28;01mif\u001b[39;00m logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m/data/hyeryung/.conda/envs/loc-edit/lib/python3.8/site-packages/transformers/generation/utils.py:1144\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1147\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['sequence_bias'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "# If we add a negative bias without beam search, it may become \"stuck\" in a prefix without good continuations\n",
    "sequence_bias = {get_tokens_as_tuple(\"Trump\"): -10.0}\n",
    "biased_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, sequence_bias=sequence_bias)\n",
    "print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m biased_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, sequence_bias\u001b[38;5;241m=\u001b[39msequence_bias)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(biased_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# We can also add a positive bias to nudge the model towards specific tokens or continuations\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "biased_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, num_beams=4, sequence_bias=sequence_bias)\n",
    "print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])\n",
    "\n",
    "# We can also add a positive bias to nudge the model towards specific tokens or continuations\n",
    "sequence_bias = {get_tokens_as_tuple(\"Donald Duck\"): 10.0}\n",
    "biased_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, num_beams=4, sequence_bias=sequence_bias)\n",
    "print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc-edit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

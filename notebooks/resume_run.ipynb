{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "194ea089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt at resuming from a terminated run (originally trained with transformers v 4.2.1)\n",
    "# problems 1) Trainer.train(resume_from_checkpoint = xx) only came out after v4.2.1 (ver. I originally used.)\n",
    "#            So, I updated the transformers to .. v4.26.1.. Behaviors might be slightly different between two versions.\n",
    "#          2) In the github, there's statement that you need to install transformers >= 4.5.1. \n",
    "#            But I originally used v4.2.1. The training might not replicate Sachin's experiments.\n",
    "# --> 2 is critical. \n",
    "# ----> Should I retrain the model from scratch?\n",
    "# ----> If so, better write up a continuous training script and use it...T.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7bed14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train the classifier\n",
    "# python -u examples/training_constraint_models/train_classifier.py\\\n",
    "#     data/toxicity/jigsaw-unintended-bias-in-toxicity-classification\\\n",
    "#     0,1\\\n",
    "#     train\\\n",
    "#     dev\\\n",
    "#     test\\\n",
    "#     roberta-base\\\n",
    "#     models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds\\\n",
    "#     gpt2-roberta full gpt2-large freeze-vecmap dontbinarize jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04e41252",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ['', 'data/toxicity/jigsaw-unintended-bias-in-toxicity-classification',\n",
    " '0,1',\n",
    " 'train',\n",
    " 'dev',\n",
    " 'test',\n",
    " 'roberta-base',\n",
    " 'models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds',\n",
    " 'gpt2-roberta',\n",
    " 'full',\n",
    " 'gpt2-large',\n",
    " 'freeze-vecmap',\n",
    " 'dontbinarize',\n",
    " 'jsonl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6825cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, AddedToken\n",
    "from torch import optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2130fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makeirs(params[7], exist_ok=True)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "005e834d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_dataset 311564 311564 {0, 1}\n",
      "create_dataset 4000 4000 {0, 1}\n",
      "create_dataset 4000 4000 {0, 1}\n"
     ]
    }
   ],
   "source": [
    "base_path = params[1]\n",
    "binarize_labels = False\n",
    "if len(params) > 12:\n",
    "    binarize_labels = params[12] == \"binarize_labels\"\n",
    "\n",
    "filetype = \"txt\"\n",
    "if len(params) > 13:\n",
    "    filetype = params[13]\n",
    "\n",
    "labels = [int(label) for label in params[2].split(\",\")]\n",
    "train_paths = []\n",
    "valid_paths = []\n",
    "test_paths = []\n",
    "for label in labels:\n",
    "    train_paths.append(open(f\"{base_path}/{params[3]}_{label}.{filetype}\"))\n",
    "    valid_paths.append(open(f\"{base_path}/{params[4]}_{label}.{filetype}\"))\n",
    "    test_paths.append(open(f\"{base_path}/{params[5]}_{label}.{filetype}\"))\n",
    "\n",
    "def create_dataset(paths, labelses):\n",
    "    texts, labels = [], []\n",
    "    # print(paths)\n",
    "    for i, path in enumerate(paths):\n",
    "        for l in path:\n",
    "            if filetype == \"jsonl\":\n",
    "                text = json.loads(l)[\"text\"]\n",
    "            else:\n",
    "                text = l.strip()\n",
    "            if binarize_labels:\n",
    "                label = 0\n",
    "                if labelses[i] <= 2:\n",
    "                    label = 0\n",
    "                    texts.append(text)\n",
    "                    labels.append(label)\n",
    "                elif labelses[i] >= 3:\n",
    "                    label = 1\n",
    "                    texts.append(text)\n",
    "                    labels.append(label)\n",
    "            else:\n",
    "                labels.append(labelses[i])\n",
    "                texts.append(text)\n",
    "            \n",
    "    print(\"create_dataset\", len(texts), len(labels), set(labels))\n",
    "    return texts, labels\n",
    "    \n",
    "train_texts, train_labels = create_dataset(train_paths, labels)\n",
    "val_texts, val_labels = create_dataset(valid_paths, labels)\n",
    "test_texts, test_labels = create_dataset(test_paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f89f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(params[6], cache_dir=\"hf_cache\")\n",
    "# config = AutoConfig.from_pretrained(params[6], cache_dur=\"hf_cache\", num_labels=len(labels))\n",
    "\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(params[6], cache_dir=\"hf_cache\")\n",
    "if params[10] != \"none\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(params[10], cache_dir=\"hf_cache\")\n",
    "    tokenizer.model_max_length = min(tokenizer_.model_max_length, tokenizer.model_max_length)\n",
    "else:\n",
    "    tokenizer = tokenizer_\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(params[6], cache_dir=\"hf_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a060c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "look above for padding\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(params[6], cache_dur=\"hf_cache\", num_labels=len(labels))\n",
    "config2 = None\n",
    "if params[10] != \"none\":  \n",
    "    config2 = AutoConfig.from_pretrained(params[10], cache_dur=\"hf_cache\", num_labels=len(labels))\n",
    "    print(config2.pad_token_id)\n",
    "    config2.pad_token_id = tokenizer.pad_token_id\n",
    "    print(config2.pad_token_id)\n",
    "    print(\"look above for padding\")\n",
    "\n",
    "    tokenizer_ = AutoTokenizer.from_pretrained(params[6], config=config)\n",
    "    tokenizer.model_max_length = min(tokenizer_.model_max_length, tokenizer.model_max_length)\n",
    "\n",
    "# config.n_positions = max_length\n",
    "# config.max_position_embeddings = max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8342080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best/tokenizer_config.json',\n",
       " 'models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best/special_tokens_map.json',\n",
       " 'models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best/vocab.json',\n",
       " 'models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best/merges.txt',\n",
       " 'models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best/added_tokens.json',\n",
       " 'models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if params[8] == \"krishna\":\n",
    "#     SPECIAL_TOKENS = {\n",
    "#         \"additional_special_tokens\": [\"<dense-vectors>\", \"<tokens>\", \"<verb>\", \"<ARG0>\", \"<ARG1>\", \"<global-dense-vectors>\"],\n",
    "#         \"pad_token\": \"<eos>\",\n",
    "#         \"bos_token\": \"<bos>\",\n",
    "#         \"eos_token\": \"<eos>\"\n",
    "#     }\n",
    "#     print(\"Adding special tokens\")\n",
    "#     tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "#     config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# elif params[8] == \"roberta\" :\n",
    "#     SPECIAL_TOKENS = {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\"}\n",
    "#     print(\"Adding special tokens\")\n",
    "#     tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "#     config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# elif params[8] == \"dialogpt\": \n",
    "#     SPECIAL_TOKENS = {\"pad_token\": tokenizer.eos_token}\n",
    "#     print(\"Adding special tokens\")\n",
    "#     tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "#     config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# elif params[8] == \"gpt2\":\n",
    "#     SPECIAL_TOKENS = {\"pad_token\": tokenizer.eos_token}\n",
    "#     config.pad_token_id = tokenizer.eos_token_id\n",
    "#     # tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "#     print(\"Adding special tokens\")\n",
    "#     tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "#     print(tokenizer)\n",
    "\n",
    "# elif params[8] == \"bert\":\n",
    "#     # SPECIAL_TOKENS = {\"pad_token\": tokenizer.eos_token}\n",
    "#     # config.pad_token_id = tokenizer.eos_token_id\n",
    "#     # print(\"Adding special tokens\")\n",
    "#     # tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "#     pass\n",
    "\n",
    "# elif params[8] == \"gpt2-roberta\":\n",
    "if params[8] == \"gpt2-roberta\":\n",
    "    SPECIAL_TOKENS = {\"pad_token\": tokenizer.eos_token}\n",
    "    # config.pad_token_id = tokenizer.eos_token_id\n",
    "    print(\"Adding special tokens\")\n",
    "    tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "\n",
    "# elif params[8] == \"gpt2-distilbert\":\n",
    "#     SPECIAL_TOKENS = {\"pad_token\": tokenizer.eos_token}\n",
    "#     # config.pad_token_id = tokenizer.eos_token_id\n",
    "#     print(\"Adding special tokens\")\n",
    "#     tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "\n",
    "\n",
    "tokenizer.save_pretrained(f\"{params[7]}/checkpoint_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14a7320d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded\n",
      "[26287, 5398, 6641, 5057, 318, 379, 16677, 4, 357, 11275, 83, 14, 12519, 352, 13, 22, 1058, 352, 1267, 530, 286, 262, 4511, 287, 262, 995, 0, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256] 346\n",
      "datasets loaded and tokenizer\n"
     ]
    }
   ],
   "source": [
    "# if params[9] != \"only_tokenizer\": # all below are inside this if statement\n",
    "\n",
    "print(\"tokenizer loaded\")\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "print(test_encodings['input_ids'][0], len(test_encodings['input_ids'][0]))\n",
    "# input()\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = Dataset(train_encodings, train_labels)\n",
    "val_dataset = Dataset(val_encodings, val_labels)\n",
    "test_dataset = Dataset(test_encodings, test_labels)\n",
    "\n",
    "print(\"datasets loaded and tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fcf4112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-large and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# if params[10] != \"none\":\n",
    "model = AutoModelForSequenceClassification.from_pretrained(params[10], config=config2) # unindented\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c1e5675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#         if params[11] == \"random\":  \n",
    "#             embeds = model.get_input_embeddings()\n",
    "#             embeds.weight.data.normal_(mean=0.0, std=0.02)\n",
    "#             if embeds.padding_idx is not None:\n",
    "#                 embeds.weight.data[module.padding_idx].zero_()\n",
    "#             model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#         elif params[11] == \"freeze\":\n",
    "#             model = AutoModelForSequenceClassification.from_pretrained(params[6], config=config)\n",
    "#             embeds = model.get_input_embeddings()\n",
    "#             for p in embeds.parameters():\n",
    "#                 p.requires_grad = False\n",
    "#             # embeds.requires_grad=False\n",
    "#             model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#         elif params[11] == \"freeze-project\":\n",
    "#             embeds = model.get_input_embeddings()\n",
    "#             new_embeds = torch.nn.Embedding(embeds.num_embeddings, embeds.embedding_dim)\n",
    "#             for p in new_embeds.parameters():\n",
    "#                 p.requires_grad = False\n",
    "#             # new_embeds.requires_grad = False\n",
    "#             new_embeds.weight.data.copy_(embeds.weight)\n",
    "#             print(model.device)\n",
    "#             config.new_n_embd = new_embeds.embedding_dim\n",
    "#             config.new_vocab_size = new_embeds.num_embeddings\n",
    "#             # if params[8] == \"gpt2-roberta\":\n",
    "#             #     config.pad_token_id = tokenizer.eos_token_id\n",
    "#             model_ = AutoModelForSequenceClassification.from_pretrained(params[6], config=config)\n",
    "#             new_embeds = torch.nn.Sequential(new_embeds, torch.nn.Linear(new_embeds.embedding_dim, model_.get_input_embeddings().embedding_dim, bias=False))\n",
    "#             model_.set_input_embeddings(new_embeds)\n",
    "#             model = model_\n",
    "\n",
    "#         elif params[11] == \"freeze-eye\":\n",
    "#             embeds = model.get_input_embeddings()\n",
    "#             new_embeds = torch.nn.Embedding(embeds.num_embeddings, embeds.embedding_dim)\n",
    "#             for p in new_embeds.parameters():\n",
    "#                 p.requires_grad = False\n",
    "#             # new_embeds.requires_grad = False\n",
    "#             new_embeds.weight.data.copy_(embeds.weight)\n",
    "#             print(model.device)\n",
    "#             config.new_n_embd = new_embeds.embedding_dim\n",
    "#             # if params[8] == \"gpt2-roberta\":\n",
    "#             #     config.pad_token_id = tokenizer.eos_token_id\n",
    "#             model_ = AutoModelForSequenceClassification.from_pretrained(params[6], config=config)\n",
    "#             eye = torch.nn.Linear(new_embeds.embedding_dim, model_.get_input_embeddings().embedding_dim, bias=False)\n",
    "#             eye.weight.data.copy_(torch.eye(new_embeds.embedding_dim).data)\n",
    "#             new_embeds = torch.nn.Sequential(new_embeds, eye)\n",
    "#             model_.set_input_embeddings(new_embeds)\n",
    "#             model = model_\n",
    "\n",
    "#         elif params[8] == \"gpt2-roberta\": # part below unindented\n",
    "def learn_vecmap(X, y):\n",
    "    print(\"computing vecmap\")\n",
    "    w = torch.inverse(X.t().matmul(X)).matmul(X.t()).matmul(y)\n",
    "    vecmap = torch.nn.Linear(w.size(0), w.size(1), bias=False)\n",
    "    print(w.size(), vecmap.weight.size())\n",
    "    vecmap.weight.data.copy_(w.data.t())\n",
    "    return vecmap\n",
    "\n",
    "def vocab_permutation(vocab1, vocab2):\n",
    "    vocab2itos = {k:v for v,k in vocab2.items()}\n",
    "    vocab2list = [vocab2itos[k] for k in range(len(vocab2itos))]\n",
    "\n",
    "    perm1 = []\n",
    "    perm2 = []\n",
    "    unincluded = []\n",
    "    for i, word in enumerate(vocab2list):\n",
    "        if word in vocab1:\n",
    "            perm1.append(vocab1[word])\n",
    "            perm2.append(i)\n",
    "        else:\n",
    "            unincluded.append(word)\n",
    "\n",
    "    print(unincluded)\n",
    "    return perm1, perm2\n",
    "\n",
    "embeds = model.get_input_embeddings()\n",
    "new_embeds = torch.nn.Embedding(embeds.num_embeddings, embeds.embedding_dim)\n",
    "for p in new_embeds.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "new_embeds.weight.data.copy_(embeds.weight)\n",
    "print(model.device)\n",
    "config.new_n_embd = new_embeds.embedding_dim\n",
    "config.new_vocab_size = new_embeds.num_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca8897ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_ = AutoModelForSequenceClassification.from_pretrained(params[6], config=config)\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(params[6], config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be9b56db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<pad>', '</s>', '<unk>', 'madeupword0000', 'madeupword0001', 'madeupword0002', '<mask>']\n",
      "computing vecmap\n",
      "torch.Size([1280, 768]) torch.Size([768, 1280])\n"
     ]
    }
   ],
   "source": [
    "perm, perm_ = vocab_permutation(tokenizer.vocab, tokenizer_.vocab)\n",
    "old_embeds = model_.get_input_embeddings()\n",
    "vecmap = learn_vecmap(new_embeds.weight[perm], old_embeds.weight[perm_])\n",
    "new_embeds = torch.nn.Sequential(new_embeds, vecmap)\n",
    "model_.set_input_embeddings(new_embeds)\n",
    "model = model_\n",
    "\n",
    "# else:\n",
    "#     model =  AutoModelForSequenceClassification.from_pretrained(params[6], config=config)\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n",
    "# # model =  AutoModelForSequenceClassification.from_pretrained(params[6], config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e619252",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_path = '/home/hyeryungson/mucoco/models_bak/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds'\n",
    "# state_dict\n",
    "mod = torch.load(os.path.join(mod_path, 'results/checkpoint-76000/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f680c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b8976f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(model.device)\n",
    "os.makedirs(params[7], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08ad0051",
   "metadata": {},
   "outputs": [],
   "source": [
    "## optimizer \n",
    "# need to separate parameters into 2 groups to apply different weight decay values (AdamW)\n",
    "# weight decay only applied to parameter weights (not to bias, layernorm weight/bias)\n",
    "# code source: https://discuss.huggingface.co/t/adamw-pytorch-vs-huggingface/30562\n",
    "# why?: weight decay = for overfitting https://forums.fast.ai/t/is-weight-decay-applied-to-the-bias-term/73212\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n",
    "optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.01,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "optimizer = optim.AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=1e-05,\n",
    "            eps=1e-08,\n",
    "            weight_decay=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb1e3149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim state_dict\n",
    "opt = torch.load(os.path.join(mod_path, 'results/checkpoint-76000/optimizer.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56ebaaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.load_state_dict(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b579a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cda3ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sch = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=600, num_training_steps=194720, last_epoch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "568ec98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## learning rate scheduler\n",
    "# \"linear\"\n",
    "sch = torch.load(os.path.join(mod_path, 'results/checkpoint-76000/scheduler.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5eabc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sch.load_state_dict(sch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2745cf41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.optim.lr_scheduler.LambdaLR at 0x7f5d2acdd3d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74d3d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in opt['state']:\n",
    "#     print(i, opt['state'][i]['exp_avg'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3f8b7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (name, t) in enumerate(model.named_parameters()):\n",
    "#     if i == 0: \n",
    "#         continue\n",
    "#     if ('bias' in name) or ('LayerNorm' in name):\n",
    "#         continue\n",
    "#     print(i, name, ':::' , t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fea593a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f'{params[7]}/results',          # output directory\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    per_device_eval_batch_size=4,   # batch size for evaluation\n",
    "#     warmup_steps=600,\n",
    "#     weight_decay=0.01,               # strength of weight decay\n",
    "#     learning_rate=1e-5,\n",
    "    logging_dir=f'{params[7]}/logs',            # directory for storing logs\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=1,\n",
    "    eval_steps=500,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "print(training_args.n_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6691fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9e50eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, lr_sch)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b51151f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-76000.\n",
      "You are resuming training from a checkpoint trained with 4.2.1 of Transformers but your current version is 4.26.1. This is not recommended and could yield to errors or unwanted behaviors.\n",
      "***** Running training *****\n",
      "  Num examples = 311564\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 194720\n",
      "  Number of trainable parameters = 87026690\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 3\n",
      "  Continuing training from global step 76000\n",
      "  Will skip the first 3 epochs then the first 70336 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7b65bb6dec415ab9b0723fc8b1e791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70336 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhayleyson\u001b[0m (\u001b[33mski-ml\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hyeryungson/mucoco/wandb/run-20230317_014409-9jrnv73v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ski-ml/huggingface/runs/9jrnv73v' target=\"_blank\">daily-oath-6</a></strong> to <a href='https://wandb.ai/ski-ml/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ski-ml/huggingface' target=\"_blank\">https://wandb.ai/ski-ml/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ski-ml/huggingface/runs/9jrnv73v' target=\"_blank\">https://wandb.ai/ski-ml/huggingface/runs/9jrnv73v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find an RNG file, if you are resuming a training that was launched in a distributed fashion, reproducibility is not guaranteed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110739' max='194720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110739/194720 3:49:16 < 9:14:18, 2.53 it/s, Epoch 5.69/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>0.297577</td>\n",
       "      <td>0.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>0.117400</td>\n",
       "      <td>0.304370</td>\n",
       "      <td>0.930750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>0.134600</td>\n",
       "      <td>0.299758</td>\n",
       "      <td>0.930500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.277251</td>\n",
       "      <td>0.933750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>0.114900</td>\n",
       "      <td>0.335345</td>\n",
       "      <td>0.930750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>0.116500</td>\n",
       "      <td>0.298580</td>\n",
       "      <td>0.932750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79500</td>\n",
       "      <td>0.123800</td>\n",
       "      <td>0.290136</td>\n",
       "      <td>0.933500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.318558</td>\n",
       "      <td>0.928250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80500</td>\n",
       "      <td>0.106700</td>\n",
       "      <td>0.306804</td>\n",
       "      <td>0.935500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.314432</td>\n",
       "      <td>0.932750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81500</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.331032</td>\n",
       "      <td>0.932750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.092500</td>\n",
       "      <td>0.375799</td>\n",
       "      <td>0.928250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82500</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>0.326321</td>\n",
       "      <td>0.933500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>0.130600</td>\n",
       "      <td>0.278964</td>\n",
       "      <td>0.935750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83500</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.310229</td>\n",
       "      <td>0.935750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.099300</td>\n",
       "      <td>0.290608</td>\n",
       "      <td>0.937750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84500</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.296856</td>\n",
       "      <td>0.932500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.313763</td>\n",
       "      <td>0.934750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85500</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>0.294505</td>\n",
       "      <td>0.931750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.290731</td>\n",
       "      <td>0.936250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86500</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.350507</td>\n",
       "      <td>0.927750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.316984</td>\n",
       "      <td>0.930750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87500</td>\n",
       "      <td>0.117800</td>\n",
       "      <td>0.316057</td>\n",
       "      <td>0.932250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.335416</td>\n",
       "      <td>0.930750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88500</td>\n",
       "      <td>0.132900</td>\n",
       "      <td>0.313113</td>\n",
       "      <td>0.929250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.291506</td>\n",
       "      <td>0.934500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89500</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>0.303618</td>\n",
       "      <td>0.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.313173</td>\n",
       "      <td>0.934750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90500</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.362208</td>\n",
       "      <td>0.933000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>0.123800</td>\n",
       "      <td>0.324224</td>\n",
       "      <td>0.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91500</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>0.308474</td>\n",
       "      <td>0.936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.316394</td>\n",
       "      <td>0.936500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92500</td>\n",
       "      <td>0.097900</td>\n",
       "      <td>0.337160</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>0.134600</td>\n",
       "      <td>0.257549</td>\n",
       "      <td>0.936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93500</td>\n",
       "      <td>0.113300</td>\n",
       "      <td>0.327387</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.098500</td>\n",
       "      <td>0.335063</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94500</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.304959</td>\n",
       "      <td>0.931750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>0.111100</td>\n",
       "      <td>0.337936</td>\n",
       "      <td>0.931500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95500</td>\n",
       "      <td>0.095500</td>\n",
       "      <td>0.339109</td>\n",
       "      <td>0.932500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>0.346517</td>\n",
       "      <td>0.930500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96500</td>\n",
       "      <td>0.123200</td>\n",
       "      <td>0.309106</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97000</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.276770</td>\n",
       "      <td>0.934750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97500</td>\n",
       "      <td>0.088500</td>\n",
       "      <td>0.336576</td>\n",
       "      <td>0.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.095900</td>\n",
       "      <td>0.338934</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98500</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.328056</td>\n",
       "      <td>0.934750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>0.101800</td>\n",
       "      <td>0.375191</td>\n",
       "      <td>0.926250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99500</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.342556</td>\n",
       "      <td>0.929250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.350984</td>\n",
       "      <td>0.934250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100500</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.354859</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101000</td>\n",
       "      <td>0.128600</td>\n",
       "      <td>0.318557</td>\n",
       "      <td>0.933500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101500</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.340083</td>\n",
       "      <td>0.933750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.068600</td>\n",
       "      <td>0.371110</td>\n",
       "      <td>0.934250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102500</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.339460</td>\n",
       "      <td>0.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103000</td>\n",
       "      <td>0.099200</td>\n",
       "      <td>0.328843</td>\n",
       "      <td>0.933000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103500</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.367707</td>\n",
       "      <td>0.934500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.350238</td>\n",
       "      <td>0.931750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104500</td>\n",
       "      <td>0.111600</td>\n",
       "      <td>0.321084</td>\n",
       "      <td>0.933250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>0.115400</td>\n",
       "      <td>0.301113</td>\n",
       "      <td>0.935500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105500</td>\n",
       "      <td>0.079600</td>\n",
       "      <td>0.372744</td>\n",
       "      <td>0.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.140900</td>\n",
       "      <td>0.328414</td>\n",
       "      <td>0.933000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106500</td>\n",
       "      <td>0.096300</td>\n",
       "      <td>0.322304</td>\n",
       "      <td>0.934500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107000</td>\n",
       "      <td>0.095500</td>\n",
       "      <td>0.313174</td>\n",
       "      <td>0.930500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107500</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>0.305754</td>\n",
       "      <td>0.934500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.325638</td>\n",
       "      <td>0.939250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108500</td>\n",
       "      <td>0.119600</td>\n",
       "      <td>0.345942</td>\n",
       "      <td>0.926500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109000</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>0.321677</td>\n",
       "      <td>0.934750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109500</td>\n",
       "      <td>0.117800</td>\n",
       "      <td>0.336564</td>\n",
       "      <td>0.931000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.084700</td>\n",
       "      <td>0.345593</td>\n",
       "      <td>0.936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110500</td>\n",
       "      <td>0.102500</td>\n",
       "      <td>0.338288</td>\n",
       "      <td>0.935000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-76500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-76500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-76500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-78000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-78000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-78000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-77500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-78500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-78500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-78500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-78000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-79000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-79000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-79000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-78500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-79500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-79500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-79500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-79000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-80000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-80000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-80000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-79500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-80500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-80500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-80500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-80000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-81000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-81000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-81000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-80500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-81500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-81500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-81500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-81000] due to args.save_total_limit\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-82000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-82000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-82000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-81500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-82500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-82500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-82500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-82000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-83000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-83000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-83000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-82500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-83500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-83500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-83500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-83000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-84000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-84000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-84000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-83500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-84500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-84500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-84500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-84000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-85000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-85000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-85000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-84500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-85500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-85500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-85500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-85000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-86000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-86000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-86000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-85500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-86500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-86500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-86500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-86000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-87000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-87000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-87000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-86500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-87500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-87500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-87500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-87000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-88000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-88000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-88000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-87500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-88500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-88500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-88500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-88000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-89000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-89000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-89000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-88500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-89500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-89500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-89500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-89000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-90000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-90000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-90000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-89500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-90500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-90500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-90500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-90000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-91000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-91000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-91000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-90500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-91500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-91500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-91500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-91000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-92000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-92000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-92000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-91500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-92500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-92500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-92500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-92000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-93000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-93000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-93000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-92500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-93500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-93500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-93500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-93000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-94000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-94000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-94000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-93500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-94500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-94500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-94500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-94000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-95000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-95000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-95000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-94500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-95500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-95500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-95500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-95000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-96000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-96000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-96000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-95500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-109000\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-109000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-109000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-108500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-109500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-109500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-109500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-109000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-110000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-110000/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-110000/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-109500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-110500\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-110500/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-110500/pytorch_model.bin\n",
      "Deleting older checkpoint [models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-110000] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51dab988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeryungson/anaconda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 311564\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 194720\n",
      "  Number of trainable parameters = 87026690\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:1129\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1128\u001b[0m wi \u001b[38;5;241m=\u001b[39m _WandbInit()\n\u001b[0;32m-> 1129\u001b[0m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m wi\u001b[38;5;241m.\u001b[39msettings\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:164\u001b[0m, in \u001b[0;36m_WandbInit.setup\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprinter\u001b[38;5;241m.\u001b[39mdisplay(line, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wl \u001b[38;5;241m=\u001b[39m \u001b[43mwandb_setup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Make sure we have a logger setup (might be an early logger)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py:307\u001b[0m, in \u001b[0;36msetup\u001b[0;34m(settings)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(settings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_WandbSetup\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 307\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py:302\u001b[0m, in \u001b[0;36m_setup\u001b[0;34m(settings, _reset)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m wl \u001b[38;5;241m=\u001b[39m \u001b[43m_WandbSetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wl\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py:288\u001b[0m, in \u001b[0;36m_WandbSetup.__init__\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m _WandbSetup\u001b[38;5;241m.\u001b[39m_instance \u001b[38;5;241m=\u001b[39m \u001b[43m_WandbSetup__WandbSetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py:100\u001b[0m, in \u001b[0;36m_WandbSetup__WandbSetup.__init__\u001b[0;34m(self, pid, settings, environ)\u001b[0m\n\u001b[1;32m     98\u001b[0m _set_logger(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_early_logger)\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_early_logger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# self._settings.freeze()\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py:128\u001b[0m, in \u001b[0;36m_WandbSetup__WandbSetup._settings_setup\u001b[0;34m(self, settings, early_logger)\u001b[0m\n\u001b[1;32m    126\u001b[0m     s\u001b[38;5;241m.\u001b[39m_apply_setup(settings, _logger\u001b[38;5;241m=\u001b[39mearly_logger)\n\u001b[0;32m--> 128\u001b[0m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_settings_from_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s\u001b[38;5;241m.\u001b[39m_cli_only_mode:\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/wandb/sdk/wandb_settings.py:1390\u001b[0m, in \u001b[0;36mSettings._infer_settings_from_environment\u001b[0;34m(self, _logger)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jupyter \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotebook_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotebook_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1390\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjupyter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m     settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_jupyter_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/wandb/jupyter.py:228\u001b[0m, in \u001b[0;36mnotebook_metadata\u001b[0;34m(silent)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    223\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: ipynb[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    225\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: ipynb[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    226\u001b[0m         }\n\u001b[0;32m--> 228\u001b[0m jupyter_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mnotebook_metadata_from_jupyter_servers_and_kernel_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jupyter_metadata:\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/wandb/jupyter.py:160\u001b[0m, in \u001b[0;36mnotebook_metadata_from_jupyter_servers_and_kernel_id\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnotebook_metadata_from_jupyter_servers_and_kernel_id\u001b[39m():\n\u001b[0;32m--> 160\u001b[0m     servers, kernel_id \u001b[38;5;241m=\u001b[39m \u001b[43mjupyter_servers_and_kernel_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m servers:\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/wandb/jupyter.py:257\u001b[0m, in \u001b[0;36mjupyter_servers_and_kernel_id\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m serverapp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 257\u001b[0m     servers\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mlist\u001b[39m(\u001b[43mserverapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_running_servers\u001b[49m()))\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m notebookapp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/importlib/util.py:245\u001b[0m, in \u001b[0;36m_LazyModule.__getattribute__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    244\u001b[0m         attrs_updated[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__spec__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# If exec_module() was used directly there is no guarantee the module\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# object was put into sys.modules.\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/jupyter_server/serverapp.py:112\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjupyter_server\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigManager\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjupyter_server\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilemanager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    113\u001b[0m     AsyncFileContentsManager,\n\u001b[1;32m    114\u001b[0m     FileContentsManager,\n\u001b[1;32m    115\u001b[0m )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjupyter_server\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlargefilemanager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LargeFileManager\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/jupyter_server/services/contents/filemanager.py:13\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnbformat\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01manyio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mto_thread\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_sync\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjupyter_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpaths\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exists, is_file_hidden, is_hidden\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/anyio/__init__.py:87\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     84\u001b[0m     BrokenResourceError, BrokenWorkerProcess, BusyResourceError, ClosedResourceError,\n\u001b[1;32m     85\u001b[0m     DelimiterNotFound, EndOfStream, ExceptionGroup, IncompleteRead, TypedAttributeLookupError,\n\u001b[1;32m     86\u001b[0m     WouldBlock)\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fileio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncFile, Path, open_file, wrap_file\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_resources\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m aclose_forcefully\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:846\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:941\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1040\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                         \u001b[38;5;66;03m# the instantiated 🤗 Transformers model to be trained\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \u001b[38;5;66;03m# training arguments, defined above\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining finished\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1540\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1542\u001b[0m )\n\u001b[0;32m-> 1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/transformers/trainer.py:1720\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step\n\u001b[1;32m   1718\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m-> 1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;66;03m# Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mignore_data_skip:\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/transformers/trainer_callback.py:353\u001b[0m, in \u001b[0;36mCallbackHandler.on_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[1;32m    352\u001b[0m     control\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_begin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/transformers/trainer_callback.py:397\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 397\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/transformers/integrations.py:747\u001b[0m, in \u001b[0;36mWandbCallback.on_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m     args\u001b[38;5;241m.\u001b[39mrun_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialized:\n\u001b[0;32m--> 747\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/transformers/integrations.py:721\u001b[0m, in \u001b[0;36mWandbCallback.setup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m         init_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mrun_name\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 721\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWANDB_PROJECT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuggingface\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# add config parameters (run may have been created manually)\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mupdate(combined_dict, allow_val_change\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:1153\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1153\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m logger\n\u001b[1;32m   1154\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterrupted\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "db563b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{params[7]}/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a26ce98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/tmp\n",
      "Configuration saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/tmp/config.json\n",
      "Model weights saved in models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/tmp/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(output_dir=f\"{params[7]}/tmp\") \n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5872b24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_dataset 311564 311564 {0, 1}\n",
      "create_dataset 4000 4000 {0, 1}\n",
      "create_dataset 4000 4000 {0, 1}\n",
      "None\n",
      "None\n",
      "look above for padding\n",
      "Adding special tokens\n",
      "tokenizer loaded\n",
      "[26287, 5398, 6641, 5057, 318, 379, 16677, 4, 357, 11275, 83, 14, 12519, 352, 13, 22, 1058, 352, 1267, 530, 286, 262, 4511, 287, 262, 995, 0, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256] 346\n",
      "datasets loaded and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-large and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<pad>', '</s>', '<unk>', 'madeupword0000', 'madeupword0001', 'madeupword0002', '<mask>']\n",
      "computing vecmap\n",
      "torch.Size([1280, 768]) torch.Size([768, 1280])\n",
      "cuda:0\n",
      "2\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeryungson/anaconda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 311564\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 97360\n",
      "  Number of trainable parameters = 87026690\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhayleyson\u001b[0m (\u001b[33mski-ml\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hyeryungson/mucoco/wandb/run-20230316_105352-bqwtyykf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ski-ml/huggingface/runs/bqwtyykf' target=\"_blank\">dry-flower-2</a></strong> to <a href='https://wandb.ai/ski-ml/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ski-ml/huggingface' target=\"_blank\">https://wandb.ai/ski-ml/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ski-ml/huggingface/runs/bqwtyykf' target=\"_blank\">https://wandb.ai/ski-ml/huggingface/runs/bqwtyykf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeryungson/anaconda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='138' max='97360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  138/97360 02:08 < 25:25:30, 1.06 it/s, Epoch 0.01/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 316\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28mprint\u001b[39m(training_args\u001b[38;5;241m.\u001b[39mn_gpu)\n\u001b[1;32m    306\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    307\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                         \u001b[38;5;66;03m# the instantiated 🤗 Transformers model to be trained\u001b[39;00m\n\u001b[1;32m    308\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \u001b[38;5;66;03m# training arguments, defined above\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    311\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m    312\u001b[0m )\n\u001b[0;32m--> 316\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining finished\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams[\u001b[38;5;241m7\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/checkpoint_best\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1540\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1542\u001b[0m )\n\u001b[0;32m-> 1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/transformers/trainer.py:1791\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1791\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1794\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1797\u001b[0m ):\n\u001b[1;32m   1798\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1799\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/transformers/trainer.py:2557\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2555\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2556\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2557\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/torch/tensor.py:245\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    238\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    239\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    244\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 245\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/lib/python3.9/site-packages/torch/autograd/__init__.py:145\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 145\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"running evaluation now\")\n",
    "\n",
    "metrics = trainer.evaluate(val_dataset)\n",
    "print(\"validation\", metrics)\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"test\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from torch import nn\n",
    "import torch\n",
    "import scipy\n",
    "# from scipy.special import softmax\n",
    "\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "###############################################################################################################\n",
    "from glob import glob \n",
    "import yaml\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, AddedToken\n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "###############################################################################################################\n",
    "\n",
    "import os\n",
    "os.chdir('/home/hyeryungson/mucoco/notebooks/energy-model-retrain')\n",
    "from load_ckpt import define_model\n",
    "from customTrainer import CustomTrainer\n",
    "os.chdir('/home/hyeryungson/mucoco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = scipy.special.softmax(logits, axis=-1)\n",
    "    log_predictions = np.log(predictions)\n",
    "    all_losses = np.sum(labels * log_predictions, axis=-1)\n",
    "    loss = -np.mean(all_losses, axis=0)\n",
    "    return {\"eval_loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define arguments\n",
    "params = ['', 'data/toxicity/jigsaw-unintended-bias-in-toxicity-classification',\n",
    " '0,1',\n",
    " 'train',\n",
    " 'dev',\n",
    " 'test',\n",
    " 'roberta-base',\n",
    " 'models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds',\n",
    " 'gpt2-roberta',\n",
    " 'full',\n",
    " 'gpt2-large',\n",
    " 'freeze-vecmap',\n",
    " 'dontbinarize',\n",
    " 'jsonl']\n",
    "\n",
    "time_limit = 1\n",
    "resume_yn=True\n",
    "num_gpu = torch.cuda.device_count()\n",
    "os.makedirs(params[7], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pjt_id: se53xibc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhayleyson\u001b[0m (\u001b[33mski-ml\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hyeryungson/mucoco/wandb/run-20230503_015510-se53xibc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/ski-ml/huggingface/runs/se53xibc' target=\"_blank\">fearless-serenity-22</a></strong> to <a href='https://wandb.ai/ski-ml/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ski-ml/huggingface' target=\"_blank\">https://wandb.ai/ski-ml/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ski-ml/huggingface/runs/se53xibc' target=\"_blank\">https://wandb.ai/ski-ml/huggingface/runs/se53xibc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path to yaml file wandb/run-20230503_015510-se53xibc/files/config.yaml\n",
      "models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-9/\n",
      "None\n",
      "None\n",
      "look above for padding\n",
      "Adding special tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-large and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<pad>', '</s>', '<unk>', 'madeupword0000', 'madeupword0001', 'madeupword0002', '<mask>']\n",
      "computing vecmap\n",
      "torch.Size([1280, 768]) torch.Size([768, 1280])\n"
     ]
    }
   ],
   "source": [
    "if resume_yn:\n",
    "    # get the latest pjt_id for now\n",
    "    pjt_id=sorted(glob(f'wandb/run-*'), reverse=True)[0].split('-')[-1]\n",
    "    print(f'pjt_id: {pjt_id}')\n",
    "    \n",
    "    # for resume in wandb\n",
    "    wandb.init(project=\"huggingface\", resume=\"must\", id=pjt_id)\n",
    "    wandb_path=sorted(glob(f'wandb/run-*{pjt_id}'), reverse=True)[0]\n",
    "    config_path=os.path.join(wandb_path, 'files/config.yaml')\n",
    "    print('path to yaml file', config_path)\n",
    "    with open(config_path, 'r') as stream:\n",
    "        past_run_config = yaml.safe_load(stream)\n",
    "        \n",
    "    # set the ckpt with highest step number as ckpt path\n",
    "    ckpt_dir=sorted(glob(f'{params[7]}/results/*/'), reverse=True)[0]\n",
    "    print(ckpt_dir)\n",
    "    # load model from ckpt\n",
    "    model, config, tokenizer = define_model(mod_path=os.path.join(ckpt_dir, \"pytorch_model.bin\"), load_weights=True)\n",
    "else:\n",
    "    model, config, tokenizer = define_model(mod_path=None, load_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_json('/home/hyeryungson/mucoco/data/toxicity/jigsaw-unintended-bias-in-toxicity-classification/fine-grained/train.jsonl', lines=True)\n",
    "dev_data = pd.read_json('/home/hyeryungson/mucoco/data/toxicity/jigsaw-unintended-bias-in-toxicity-classification/fine-grained/dev.jsonl', lines=True)\n",
    "test_data = pd.read_json('/home/hyeryungson/mucoco/data/toxicity/jigsaw-unintended-bias-in-toxicity-classification/fine-grained/test.jsonl', lines=True)\n",
    "\n",
    "train_texts, train_labels = train_data['text'].tolist(), train_data['toxicity'].tolist()\n",
    "val_texts, val_labels = dev_data['text'].tolist(), dev_data['toxicity'].tolist()\n",
    "test_texts, test_labels = test_data['text'].tolist(), test_data['toxicity'].tolist()\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor([1 - self.labels[idx], self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = Dataset(train_encodings, train_labels)\n",
    "val_dataset = Dataset(val_encodings, val_labels)\n",
    "test_dataset = Dataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f'{params[7]}/results',          # output directory\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    per_device_eval_batch_size=4,   # batch size for evaluation\n",
    "    warmup_steps=600, # commented out for resume\n",
    "    weight_decay=0.01,               # strength of weight decay # commented out for resume\n",
    "    learning_rate=1e-5, # commented out for resume\n",
    "    logging_dir=f'{params[7]}/logs',            # directory for storing logs\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=1,\n",
    "    eval_steps=500,\n",
    "    # metric_for_best_model=\"accuracy\",\n",
    "    # greater_is_better=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    gradient_accumulation_steps=4,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "print(training_args.n_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/results/checkpoint-9/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    # optimizers=(optimizer, lr_sch)\n",
    ")\n",
    "\n",
    "if resume_yn:\n",
    "    # resume_from_checkpoint = sorted(glob(os.path.join(params[7] + '/results/*')), key=lambda x: int(x.split('-')[-1]), reverse=True)\n",
    "    resume_from_checkpoint = ckpt_dir\n",
    "else:\n",
    "    resume_from_checkpoint = None \n",
    "print(resume_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeryungson/anaconda/envs/mucoco/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba15a53f6d294f7794ea085654be7543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m train_result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mtrain(resume_from_checkpoint\u001b[39m=\u001b[39mresume_from_checkpoint, time_limit\u001b[39m=\u001b[39mtime_limit)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtraining finished\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m os\u001b[39m.\u001b[39;49mmakedirs(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mparams[\u001b[39m7\u001b[39;49m]\u001b[39m}\u001b[39;49;00m\u001b[39m/checkpoint_best\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m trainer\u001b[39m.\u001b[39msave_model(output_dir\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mparams[\u001b[39m7\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m/checkpoint_best\u001b[39m\u001b[39m\"\u001b[39m) \n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmodel saved\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda/envs/mucoco/lib/python3.8/os.py:223\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     mkdir(name, mode)\n\u001b[1;32m    224\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best'"
     ]
    }
   ],
   "source": [
    "\n",
    "# try: \n",
    "print(\"start training\")\n",
    "train_result = trainer.train(resume_from_checkpoint=resume_from_checkpoint, time_limit=time_limit)\n",
    "print(\"training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Waiting for W&B process to finish... (success).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.makedirs(f\"{params[7]}/checkpoint_best\")\n",
    "\n",
    "trainer.save_model(output_dir=f\"{params[7]}/checkpoint_best\") \n",
    "print(\"model saved\")\n",
    "\n",
    "print(\"running evaluation now\")\n",
    "\n",
    "metrics = trainer.evaluate(val_dataset)\n",
    "print(\"validation\", metrics)\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"test\", metrics)\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     # if e.args[0] == \"TIMEOUT\":\n",
    "#     #     print(e.args[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

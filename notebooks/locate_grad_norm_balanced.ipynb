{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "locate tokens with highest gradient norm. Is the sign important? \n",
    "- if gradient is positive: having a value i\n",
    "- if gradient is negative: having a value in that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hyeryungson/mucoco\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'notebooks.load_ckpt_w_attn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m Softmax\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnotebooks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mload_ckpt_w_attn\u001b[39;00m \u001b[39mimport\u001b[39;00m define_model\n\u001b[1;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39margparse\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'notebooks.load_ckpt_w_attn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.nn import Softmax\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from notebooks.energy_model_retrain.load_ckpt import define_model\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터 로드\n",
    "samples=pd.read_csv('./notebooks/results/samples.csv')\n",
    "# label이 1인 데이터만 사용\n",
    "sample1=samples.loc[samples['label']==1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "look above for padding\n",
      "Adding special tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-large and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<pad>', '</s>', '<unk>', 'madeupword0000', 'madeupword0001', 'madeupword0002', '<mask>']\n",
      "computing vecmap\n",
      "torch.Size([1280, 768]) torch.Size([768, 1280])\n"
     ]
    }
   ],
   "source": [
    "# load trained model\n",
    "ckpt_path='/home/hyeryungson/mucoco/models_bak_contd/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best/pytorch_model.bin'\n",
    "model, config, tokenizer = define_model(ckpt_path, output_attentions=True, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([50256, 50257], ['<|endoftext|>', '<mask>'], 50257)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 논문에서는 gpt-2의 tokenizer를 사용하였으므로, mask token이 기존에는 없었음\n",
    "tokenizer.all_special_ids, tokenizer.all_special_tokens, tokenizer.vocab_size\n",
    "\n",
    "# tokenizer에 mask token 추가\n",
    "SPECIAL_TOKENS = {\"mask_token\": \"<mask>\"}\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "\n",
    "tokenizer.all_special_ids, tokenizer.all_special_tokens, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = sample1['text'].tolist()[0:10]\n",
    "batch = tokenizer(test_sent, padding=True, return_tensors=\"pt\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm more concerned about the mind control being practiced by facists like tRump!\",\n",
       " \"Right wing terror apologist. You're a sick one.\",\n",
       " \"a way to offset the taxpayer is to fire all the border guards.  We clearly don't need them.  They ain't enforcing anything except coddling these so called (very healthy and rich looking) refugees from the United States of America.  The joke is on us.  Sad.  Jughead should be grossly ashamed of his pathetic leadership. A sad day for Canada indeed.\",\n",
       " 'The man from Kenya Born in Kenya, the Republicans & Democrats let this man destroy America not one step up and tried to stop, Look at America after 7 yrs. 7 mths. of this jerk running around do nothing, make America look weak.',\n",
       " 'Someone who doesn\\'t know the difference between weather and climate and who thinks that the science  upon which all of this is based comes from meteorologists has no business referring to other people as \"idiots.\"',\n",
       " \"The sheep are swallowing it up. You'd have to be a complete fool to believe a person would commit such a heinous act and conveniently leave his wallet in the cab of the truck.\",\n",
       " 'your an idiot',\n",
       " \"Making assumptions because you can't deal with the opinions of others, I see.  Typical of people like you.\\nIf it's not accusations of Russian trolls, it's Fox or Breitbart or Rush.  Anything to make yourself feel better by making assumptions about people you don't know.\",\n",
       " \"DUH - guess what - the total history of the church and its sacraments does depend upon how the church supports its sacraments via scripture - the core of the church's understanding.  What you state is not only inaccurate but heretical.  All the sacraments have a foundation in scripture - how these were pastorally implemented, etc. evolved but always grounded in our story - scripture.   \\nIt has nothing to do with *rigor* - your statement is just plan ignorant and wrong.\",\n",
       " \"That's exactly what's wrong with this way of thinking. It's not unlike what is going on Russia where people think all homosexuals are pedophiles.\\n\\nSexuality is a spectrum and it doesn't always have to do with sexual ACTIVITY. The fact that you immediately equate transgender people with being perverted and predatory shows your ignorance. It's insulting, and low.\\n\\nThis isn't a debate about wether or not we should allow sexual predators into bathrooms, it's about accepting that some people actually ARE transgender and have the right to live a lifestyle that matches their sexual identity. This is a debate about the government not having the right to look down people's pants and decide where they belong. It's about individual freedom and civil liberty. \\n\\nYour boys don't sound transgender, and children of the same gender are just as capable of any sort of sexual assault. Your logic would state every child should have an individual bathroom if that's your concern.\"]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.input_ids[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_output = model.forward(\n",
    "batch[\"input_ids\"].cuda(),\n",
    "attention_mask=batch[\"attention_mask\"].cuda(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.8512,  3.0555],\n",
       "        [-3.7042,  4.5156],\n",
       "        [-3.6532,  4.4367],\n",
       "        [-3.7493,  4.5800],\n",
       "        [-3.6698,  4.4535],\n",
       "        [-3.5594,  4.2812],\n",
       "        [-3.6983,  4.4980],\n",
       "        [-2.4656,  2.5589],\n",
       "        [-3.5377,  4.2471],\n",
       "        [-3.0818,  3.4175]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_output['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 196, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_output['hidden_states'][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7137e-03, 9.9729e-01],\n",
       "        [2.6920e-04, 9.9973e-01],\n",
       "        [3.0652e-04, 9.9969e-01],\n",
       "        [2.4127e-04, 9.9976e-01],\n",
       "        [2.9646e-04, 9.9970e-01],\n",
       "        [3.9329e-04, 9.9961e-01],\n",
       "        [2.7561e-04, 9.9972e-01],\n",
       "        [6.5325e-03, 9.9347e-01],\n",
       "        [4.1584e-04, 9.9958e-01],\n",
       "        [1.5022e-03, 9.9850e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax=torch.nn.Softmax(dim=-1)\n",
    "softmax(classifier_output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.0.weight torch.Size([50257, 1280])\n",
      "roberta.embeddings.word_embeddings.1.weight torch.Size([768, 1280])\n",
      "roberta.embeddings.position_embeddings.weight torch.Size([514, 768])\n",
      "roberta.embeddings.token_type_embeddings.weight torch.Size([1, 768])\n",
      "roberta.embeddings.LayerNorm.weight torch.Size([768])\n",
      "roberta.embeddings.LayerNorm.bias torch.Size([768])\n",
      "roberta.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "roberta.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "roberta.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "roberta.encoder.layer.0.attention.self.key.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# embedding layer\n",
    "for n, p in list(model.named_parameters())[:10]:\n",
    "    print(n, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 768 hidden state size\n",
    "### 196 number of tokens in a sequence\n",
    "### 10 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loss? # output itself?\n",
    "# loss = F.cross_entropy(classifier_output[0], torch.ones(len(test_sent)).long().cuda())\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = classifier_output['hidden_states'][0]\n",
    "layer.retain_grad()\n",
    "classifier_output['logits'][0][1].backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 196, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = torch.norm(layer.grad, dim=-1)\n",
    "norm = torch.where(norm > 0, norm, torch.full_like(norm, 1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 196])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.9708e-01, 2.2867e-01, 1.8651e-01,  ..., 1.0000e-10, 1.0000e-10,\n",
       "         1.0000e-10],\n",
       "        [1.0000e-10, 1.0000e-10, 1.0000e-10,  ..., 1.0000e-10, 1.0000e-10,\n",
       "         1.0000e-10],\n",
       "        [1.0000e-10, 1.0000e-10, 1.0000e-10,  ..., 1.0000e-10, 1.0000e-10,\n",
       "         1.0000e-10],\n",
       "        ...,\n",
       "        [1.0000e-10, 1.0000e-10, 1.0000e-10,  ..., 1.0000e-10, 1.0000e-10,\n",
       "         1.0000e-10],\n",
       "        [1.0000e-10, 1.0000e-10, 1.0000e-10,  ..., 1.0000e-10, 1.0000e-10,\n",
       "         1.0000e-10],\n",
       "        [1.0000e-10, 1.0000e-10, 1.0000e-10,  ..., 1.0000e-10, 1.0000e-10,\n",
       "         1.0000e-10]], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_edit_tokens = torch.topk(norm, 5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[1.1212e+00, 7.3591e-01, 5.1177e-01, 5.0955e-01, 5.0683e-01],\n",
       "        [1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10],\n",
       "        [1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10],\n",
       "        [1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10],\n",
       "        [1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10],\n",
       "        [1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10],\n",
       "        [1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10],\n",
       "        [1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10],\n",
       "        [1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10],\n",
       "        [1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10]],\n",
       "       device='cuda:0'),\n",
       "indices=tensor([[11, 12,  6,  7, 13],\n",
       "        [ 1,  0,  2,  4,  3],\n",
       "        [ 1,  0,  2,  4,  3],\n",
       "        [ 1,  0,  2,  4,  3],\n",
       "        [ 1,  0,  2,  4,  3],\n",
       "        [ 1,  0,  2,  4,  3],\n",
       "        [ 1,  0,  2,  4,  3],\n",
       "        [ 1,  0,  2,  4,  3],\n",
       "        [ 1,  0,  2,  4,  3],\n",
       "        [ 1,  0,  2,  4,  3]], device='cuda:0'))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_edit_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1777, 1023, 2000, 1630,  588])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.input_ids[0][to_edit_tokens.indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = batch.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(batch.input_ids)): \n",
    "    batch.input_ids[i][to_edit_tokens.indices[i]] = 50257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm more concerned about the mind control being practiced by facists like tRump!\n",
      "I'm more concerned about the<mask><mask> being practiced by<mask><mask><mask> tRump!<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "--\n",
      "Right wing terror apologist. You're a sick one.\n",
      "<mask><mask><mask><mask><mask>. You're a sick one.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "--\n",
      "a way to offset the taxpayer is to fire all the border guards.  We clearly don't need them.  They ain't enforcing anything except coddling these so called (very healthy and rich looking) refugees from the United States of America.  The joke is on us.  Sad.  Jughead should be grossly ashamed of his pathetic leadership. A sad day for Canada indeed.\n",
      "<mask><mask><mask><mask><mask> taxpayer is to fire all the border guards.  We clearly don't need them.  They ain't enforcing anything except coddling these so called (very healthy and rich looking) refugees from the United States of America.  The joke is on us.  Sad.  Jughead should be grossly ashamed of his pathetic leadership. A sad day for Canada indeed.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "--\n",
      "The man from Kenya Born in Kenya, the Republicans & Democrats let this man destroy America not one step up and tried to stop, Look at America after 7 yrs. 7 mths. of this jerk running around do nothing, make America look weak.\n",
      "<mask><mask><mask><mask><mask> in Kenya, the Republicans & Democrats let this man destroy America not one step up and tried to stop, Look at America after 7 yrs. 7 mths. of this jerk running around do nothing, make America look weak.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "--\n",
      "Someone who doesn't know the difference between weather and climate and who thinks that the science  upon which all of this is based comes from meteorologists has no business referring to other people as \"idiots.\"\n",
      "<mask><mask><mask><mask><mask> the difference between weather and climate and who thinks that the science  upon which all of this is based comes from meteorologists has no business referring to other people as \"idiots.\"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "--\n",
      "The sheep are swallowing it up. You'd have to be a complete fool to believe a person would commit such a heinous act and conveniently leave his wallet in the cab of the truck.\n",
      "<mask><mask><mask><mask><mask> up. You'd have to be a complete fool to believe a person would commit such a heinous act and conveniently leave his wallet in the cab of the truck.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "--\n",
      "your an idiot\n",
      "<mask><mask><mask><mask><mask><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "--\n",
      "Making assumptions because you can't deal with the opinions of others, I see.  Typical of people like you.\n",
      "If it's not accusations of Russian trolls, it's Fox or Breitbart or Rush.  Anything to make yourself feel better by making assumptions about people you don't know.\n",
      "<mask><mask><mask><mask><mask>'t deal with the opinions of others, I see.  Typical of people like you.\n",
      "If it's not accusations of Russian trolls, it's Fox or Breitbart or Rush.  Anything to make yourself feel better by making assumptions about people you don't know.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "--\n",
      "DUH - guess what - the total history of the church and its sacraments does depend upon how the church supports its sacraments via scripture - the core of the church's understanding.  What you state is not only inaccurate but heretical.  All the sacraments have a foundation in scripture - how these were pastorally implemented, etc. evolved but always grounded in our story - scripture.   \n",
      "It has nothing to do with *rigor* - your statement is just plan ignorant and wrong.\n",
      "<mask><mask><mask><mask><mask> what - the total history of the church and its sacraments does depend upon how the church supports its sacraments via scripture - the core of the church's understanding.  What you state is not only inaccurate but heretical.  All the sacraments have a foundation in scripture - how these were pastorally implemented, etc. evolved but always grounded in our story - scripture.   \n",
      "It has nothing to do with *rigor* - your statement is just plan ignorant and wrong.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "--\n",
      "That's exactly what's wrong with this way of thinking. It's not unlike what is going on Russia where people think all homosexuals are pedophiles.\n",
      "\n",
      "Sexuality is a spectrum and it doesn't always have to do with sexual ACTIVITY. The fact that you immediately equate transgender people with being perverted and predatory shows your ignorance. It's insulting, and low.\n",
      "\n",
      "This isn't a debate about wether or not we should allow sexual predators into bathrooms, it's about accepting that some people actually ARE transgender and have the right to live a lifestyle that matches their sexual identity. This is a debate about the government not having the right to look down people's pants and decide where they belong. It's about individual freedom and civil liberty. \n",
      "\n",
      "Your boys don't sound transgender, and children of the same gender are just as capable of any sort of sexual assault. Your logic would state every child should have an individual bathroom if that's your concern.\n",
      "<mask><mask><mask><mask><mask> wrong with this way of thinking. It's not unlike what is going on Russia where people think all homosexuals are pedophiles.\n",
      "\n",
      "Sexuality is a spectrum and it doesn't always have to do with sexual ACTIVITY. The fact that you immediately equate transgender people with being perverted and predatory shows your ignorance. It's insulting, and low.\n",
      "\n",
      "This isn't a debate about wether or not we should allow sexual predators into bathrooms, it's about accepting that some people actually ARE transgender and have the right to live a lifestyle that matches their sexual identity. This is a debate about the government not having the right to look down people's pants and decide where they belong. It's about individual freedom and civil liberty. \n",
      "\n",
      "Your boys don't sound transgender, and children of the same gender are just as capable of any sort of sexual assault. Your logic would state every child should have an individual bathroom if that's your concern.\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(batch.input_ids)): \n",
    "    print(test_sent[i])\n",
    "    print(tokenizer.decode(batch.input_ids[i]))\n",
    "    print('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8539, 11028,  3246,  7451,  2471])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.input_ids[1][to_edit_tokens.indices[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  835,    64,   284,   262, 11677])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.input_ids[2][to_edit_tokens.indices[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.input_ids[0][to_edit_tokens.indices[0]] = 50257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm more concerned about the mind control being practiced by facists like tRump!<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm more concerned about the<mask><mask> being practiced by<mask><mask><mask> tRump!<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "K = 1\n",
    "rbt_config = RobertaConfig.from_pretrained(args.rbt_path, cache_dir=None)\n",
    "rbt_tknzr = RobertaTokenizer.from_pretrained(args.rbt_path, do_lower_case=False)\n",
    "rbt_model = RobertaForMTL.from_pretrained(args.rbt_path, config=rbt_config,\n",
    "                                            task_names=['bertscore', 'maskedlm', 'cls'])\n",
    "rbt_model.to(device)\n",
    "oreo = OREO(rbt_model=rbt_model, rbt_tknzr=rbt_tknzr, k=K, device=device)\n",
    "data = pd.read_csv(args.infile, sep='\\n', quoting=csv.QUOTE_NONE, header=None)\n",
    "results = []\n",
    "\n",
    "for orgs in tqdm(batchify(data[0].tolist(), args.batch_size), total=len(data[0])/args.batch_size, desc='loading input'):\n",
    "    torch.cuda.empty_cache()\n",
    "    cand_inps_sents = orgs.copy()\n",
    "    edit_track = [[] for _ in range(len(orgs))]\n",
    "\n",
    "    for step in range(args.iter_step):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        inps = [rbt_tknzr.tokenize(sent, add_prefix_space=True)[:50] for sent in cand_inps_sents]\n",
    "\n",
    "        if args.attribute == 'formality':\n",
    "            abbr_pos = oreo.select_abbr_span(inps)\n",
    "\n",
    "        cand_inps, cand_lens, _, _ = collate_no_tokenize(inps, rbt_tknzr.convert_tokens_to_ids, device=device)\n",
    "        bsz, seqlen = cand_inps.size()\n",
    "        oreo.model.output_hidden_states = True\n",
    "        attr_val_org, hid_states_org = oreo.cal_attr(cand_inps, hook_hid_grad=True)\n",
    "        oreo.model.output_hidden_states = False\n",
    "\n",
    "        loss = F.cross_entropy(attr_val_org, torch.ones(bsz).long().to(device))\n",
    "        loss.backward()\n",
    "\n",
    "        attr_val = F.softmax(attr_val_org, dim=1).cpu()\n",
    "        del attr_val_org\n",
    "        attr_val_mask = torch.where(attr_val[:, 1] > args.cls_thld, torch.zeros(bsz),\n",
    "                                    torch.ones(bsz)).bool().tolist()\n",
    "\n",
    "        attr_scores = attr_val[:, 1].tolist()\n",
    "        for i, (sent, attr_score) in enumerate(zip(cand_inps_sents, attr_scores)):\n",
    "            ex = {'sent': sent, 'score': attr_score}\n",
    "            edit_track[i].append(ex)\n",
    "\n",
    "        ent_mask = get_entity_mask(cand_inps_sents, cand_inps, rbt_tknzr, add_prefix_space=True)\n",
    "        grad_mask = (cand_inps.ne(oreo.pad_idx)) * (cand_inps.ne(oreo.bos_idx)) * \\\n",
    "                    (cand_inps.ne(oreo.eos_idx)) * ent_mask\n",
    "\n",
    "        del cand_inps, ent_mask\n",
    "\n",
    "        for i, state in enumerate(hid_states_org):\n",
    "            norm = torch.norm(state.grad, dim=-1).unsqueeze(2)\n",
    "            norm = torch.where(norm > 0, norm, torch.full_like(norm, 1e-10))\n",
    "            if i == 0:\n",
    "                max_span_len = torch.floor((cand_lens - 2) * args.max_mask_ratio)\n",
    "                max_span_len = torch.where(max_span_len < 1, torch.ones_like(max_span_len), max_span_len)\n",
    "                emb_ngram_top1 = get_ngram_topk(norm.squeeze(-1), grad_mask, args.C, max_span_len, args.fixed_span_len)\n",
    "                break\n",
    "\n",
    "        del grad_mask, hid_states_org\n",
    "        oreo.model.zero_grad()\n",
    "\n",
    "        if args.attribute == 'formality':\n",
    "            ins_pos_l = [item if item else emb_ngram_top1[i] for i, item in enumerate(abbr_pos)]\n",
    "        elif args.attribute == 'simplicity':\n",
    "            ins_pos_l = emb_ngram_top1\n",
    "\n",
    "        cand_inps, cand_lens, _, ins_pos = collate_inp_mask_after_span(inps, rbt_tknzr.convert_tokens_to_ids,\n",
    "                                                                        ins_pos_l,\n",
    "                                                                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_topk(inps_grad, mask, C, max_span_len, fixed_span_len=3):\n",
    "    inps_grad = torch.where(mask.eq(1), inps_grad, torch.full_like(inps_grad, -1e16))\n",
    "    bsz, seqlen = inps_grad.size()\n",
    "    \n",
    "    ngrams_b = [[] for _ in range(bsz)]\n",
    "    for ngram in range(1, fixed_span_len + 1):\n",
    "        ngram_grad = 0.\n",
    "        for i in range(ngram):\n",
    "            ngram_grad += inps_grad[:, i: i + (seqlen - ngram + 1)]\n",
    "        \n",
    "        value, position = torch.topk(ngram_grad / (ngram + C), 1, dim=1)\n",
    "        for bat, (val, pos) in enumerate(zip(value.cpu().tolist(), position.cpu().tolist())):\n",
    "            for v, p in zip(val, pos):\n",
    "                if v > 0:\n",
    "                    ngrams_b[bat].append((v, list(range(p, p + ngram))))\n",
    "                else:\n",
    "                    assert ValueError, \"Value of topk grad cannot be negative\"\n",
    "\n",
    "    sorted_ngrams_b = []\n",
    "    for bat, seq in enumerate(ngrams_b):\n",
    "        seq = [val_idx for val_idx in seq if len(val_idx[1]) <= max_span_len[bat]]\n",
    "        sorted_ngrams_b.append(sorted(seq, key=lambda x: x[0], reverse=True)[0][1])\n",
    "\n",
    "    return sorted_ngrams_b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hyeryungson/mucoco\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.nn import Softmax\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from notebooks.utils.load_ckpt import define_model\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터 로드\n",
    "samples=pd.read_csv('./notebooks/results/test_mucoco+add_preds.csv')\n",
    "# label이 1인 데이터만 사용\n",
    "sample1=samples.loc[samples['toxicity']>0.5].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-large and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE:  cuda\n"
     ]
    }
   ],
   "source": [
    "# load trained model\n",
    "ckpt_path='/home/hyeryungson/mucoco/models/models_balanced/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best/pytorch_model.bin'\n",
    "# ckpt_path='/home/hyeryungson/mucoco/models_bak_contd/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best/pytorch_model.bin'\n",
    "model, tokenizer = define_model(ckpt_path, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([50256], ['<|endoftext|>'], 50257)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 논문에서는 gpt-2의 tokenizer를 사용하였으므로, mask token이 기존에는 없었음\n",
    "# tokenizer.all_special_ids, tokenizer.all_special_tokens, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # tokenizer에 mask token 추가\n",
    "# SPECIAL_TOKENS = {\"mask_token\": \"<mask>\"}\n",
    "# tokenizer.add_special_tokens(SPECIAL_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([50256, 50257], ['<|endoftext|>', '<mask>'], 50257)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 논문에서는 gpt-2의 tokenizer를 사용하였으므로, mask token이 기존에는 없었음\n",
    "# tokenizer.all_special_ids, tokenizer.all_special_tokens, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify if the code is correct\n",
    "test_sent = sample1['text'].tolist()[0:10]\n",
    "batch = tokenizer(test_sent, padding=True, return_tensors=\"pt\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ha ha, HILLARY LOST. All you hillary fools need to be rounded up and put in prison along with your leader. You are all anti--American scum. Perhaps we should dump you in Mexico along with all the illegal trash you let in....if you love 'em so much, go live with 'em. But you are not welcome here, you hate filled Marxist morons.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cls token이 따로 없는데 잘 학습이 된게 맞을까? -> 상관없다고 하심 (교수님)\n",
    "tokenizer.decode(batch['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# forward\n",
    "classifier_output = classifier.forward(\n",
    "    batch[\"input_ids\"].cuda(),\n",
    "    attention_mask=batch[\"attention_mask\"].cuda(),\n",
    ")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get attentions\n",
    "attentions = classifier_output[\"attentions\"]\n",
    "# attention_mask에서 1의 개수를 셈\n",
    "lengths = [i.tolist().count(1) for i in batch[\"attention_mask\"]]\n",
    "# 보고자 하는 attention layer 만 가져옴\n",
    "attentions = attentions[\n",
    "    10 # originally 10\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_attns = attentions.max(1)[0][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "locate_ixes=[]\n",
    "for i, attn in enumerate(cls_attns):\n",
    "    # attention_mask가 1인 곳 까지의 attention을 보고, start of sentence와 end of sentence에 해당하는 token을 제거하고, softmax를 취한다.\n",
    "    # current_attn = attn[: lengths[i]][1:-1].softmax(-1)\n",
    "    current_attn = attn[: lengths[i]].softmax(-1) # <- current tokenizer does not add <s> and </s> to the sentence.\n",
    "    # 이 값의 평균을 구한다.\n",
    "    avg_value = current_attn.view(-1).mean().item()\n",
    "    # 이 값 중에 평균보다 큰 값을 지니는 위치를 찾는다. (+1 because we skipped the first token)\n",
    "    # top_masks = ((current_attn > avg_value).nonzero().view(-1)) + 1\n",
    "    top_masks = ((current_attn > avg_value).nonzero().view(-1))\n",
    "    torch.cuda.empty_cache()\n",
    "    top_masks = top_masks.cpu().tolist()\n",
    "    # attention 값이 평균보다 큰 토큰의 수가 6 또는 문장 전체 토큰 수의 1/3 보다 크면  \n",
    "    if len(top_masks) > min((lengths[i] - 2) // 3, 6):\n",
    "        # 그냥 attention 값 기준 top k 개 (k = 6 또는 토큰 수/3)를 뽑는다.\n",
    "        top_masks = (\n",
    "            # current_attn.topk(min((lengths[i] - 2) // 3, 6))[1] + 1\n",
    "            current_attn.topk(min((lengths[i] - 2) // 3, 6))[1]\n",
    "        )\n",
    "        top_masks = top_masks.cpu().tolist()\n",
    "    # 현재 문장의 input id를 가져온다.\n",
    "    current_sent = batch[\"input_ids\"][i][: lengths[i]]\n",
    "    count = 0\n",
    "    top_masks_final = []\n",
    "    # top_masks에 뽑힌 index를 돌면서\n",
    "    for index in top_masks:\n",
    "        # mask해야 할 토큰이 and, of, or, so 에 해당하지 않으면\n",
    "        if tokenizer.decode(current_sent[index]) not in [ ## maybe add more!\n",
    "            \" and\",\n",
    "            \" of\",\n",
    "            \" or\",\n",
    "            \" so\",\n",
    "        ]:\n",
    "            # token을 mask 한다.\n",
    "            # current_sent[index] = mask_token\n",
    "            top_masks_final.append(index)\n",
    "            # count 수를 늘린다.\n",
    "            count += 1\n",
    "        else:\n",
    "            # 만약에 and, of, or, so 에 해당하면 아무것도 하지 않는다.\n",
    "            pass\n",
    "    locate_ixes.append(top_masks_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[78, 75, 77, 79, 76, 80],\n",
       " [2, 0, 3, 1],\n",
       " [1, 2],\n",
       " [16, 0, 17, 10, 7, 8],\n",
       " [0, 2, 10, 13],\n",
       " [52, 19, 54, 17, 47, 53],\n",
       " [39, 40, 36, 34, 41, 30],\n",
       " [0, 13, 22, 25, 26, 27],\n",
       " [31, 30, 0, 29, 2, 28],\n",
       " [137, 135, 136, 133, 1, 0]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locate_ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = TargetEmbeddings(\n",
    "                        embed_dim=primary_embed_dim,\n",
    "                        embed_lut=embed_luts[0],\n",
    "                        sent_length=sent_length,\n",
    "                        batch_size=batch_size,\n",
    "                        device=device,\n",
    "                        st=args.st,\n",
    "                        init_value=init_value,\n",
    "                        random_init=args.init == \"random\",\n",
    "                        sampling_strategy=args.sampling_strategy,\n",
    "                        sampling_strategy_k=args.sampling_strategy_k,\n",
    "                        embed_scales=embed_scales,\n",
    "                        metric=args.metric,\n",
    "                        same_embed=args.same_embeds,\n",
    "                        final_bias=final_bias,\n",
    "                        eos_token_id=primary_tokenizer.eos_token_id\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RoBERTa\n",
      "Loaded RoBERTa classifier in 0.000164031982421875s\n",
      "generating masks\n",
      "Extracting SLOT tokens\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:10<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:08<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:08<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:08<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:08<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:08<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:08<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:08<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:08<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:08<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:08<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:08<00:00,  5.33it/s]\n"
     ]
    }
   ],
   "source": [
    "_time = time.time()\n",
    "print(\"Loading RoBERTa\")\n",
    "classifier = model\n",
    "print(f\"Loaded RoBERTa classifier in {time.time() - _time}s\")\n",
    "\n",
    "# mask_token에 해당하는 id 설정\n",
    "mask_token = tokenizer.encode(\"<mask>\", add_special_tokens=False)[0]\n",
    "output_sents=dict()\n",
    "print(\"generating masks\")\n",
    "print(\"Extracting SLOT tokens\")\n",
    "# mask 대상 텍스트 로드\n",
    "file = sample1['text'].tolist()\n",
    "# 레이어 별로 attention 값을 기준으로 mask 해보기 위해서 outer for-loop 추가\n",
    "for layer_num in range(0, 12):\n",
    "    print(layer_num)\n",
    "    output_sents[layer_num] = []\n",
    "    # 텍스트를 32개씩 batch로 처리할 예정임\n",
    "    for i in tqdm(range(0, len(file), 32)):\n",
    "        # get batch\n",
    "        input_lines = file[i : i + 32]\n",
    "        # tokenize\n",
    "        batch = tokenizer(\n",
    "            input_lines, padding=True, return_tensors=\"pt\", truncation=True\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        # forward\n",
    "        classifier_output = classifier.forward(\n",
    "            batch[\"input_ids\"].cuda(),\n",
    "            attention_mask=batch[\"attention_mask\"].cuda(),\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        # get attentions\n",
    "        attentions = classifier_output[\"attentions\"]\n",
    "        # attention_mask에서 1의 개수를 셈\n",
    "        lengths = [i.tolist().count(1) for i in batch[\"attention_mask\"]]\n",
    "        # 보고자 하는 attention layer 만 가져옴\n",
    "        attentions = attentions[\n",
    "            layer_num # originally 10\n",
    "        ]  # 10 is chosen because it is the magical layer number of the grand elves\n",
    "        # attentions.max(1)[0]: axis 1 에 대해서 max 값을 가져온다. (sequence에 대해서 여러개의 attention heads 중에서 가장 큰 값을 가져온다.) -> [:, 0] 그리고 나서 cls token의 attention을 가져온다.\n",
    "        cls_attns = attentions.max(1)[0][:, 0]\n",
    "        # batch에 있는 각 example에 대해서\n",
    "        for i, attn in enumerate(cls_attns):\n",
    "            # attention_mask가 1인 곳 까지의 attention을 보고, start of sentence와 end of sentence에 해당하는 token을 제거하고, softmax를 취한다.\n",
    "            # current_attn = attn[: lengths[i]][1:-1].softmax(-1)\n",
    "            current_attn = attn[: lengths[i]].softmax(-1) # <- current tokenizer does not add <s> and </s> to the sentence.\n",
    "            # 이 값의 평균을 구한다.\n",
    "            avg_value = current_attn.view(-1).mean().item()\n",
    "            # 이 값 중에 평균보다 큰 값을 지니는 위치를 찾는다. (+1 because we skipped the first token)\n",
    "            # top_masks = ((current_attn > avg_value).nonzero().view(-1)) + 1\n",
    "            top_masks = ((current_attn > avg_value).nonzero().view(-1))\n",
    "            torch.cuda.empty_cache()\n",
    "            top_masks = top_masks.cpu().tolist()\n",
    "            # attention 값이 평균보다 큰 토큰의 수가 6 또는 문장 전체 토큰 수의 1/3 보다 크면  \n",
    "            if len(top_masks) > min((lengths[i] - 2) // 3, 6):\n",
    "                # 그냥 attention 값 기준 top k 개 (k = 6 또는 토큰 수/3)를 뽑는다.\n",
    "                top_masks = (\n",
    "                    # current_attn.topk(min((lengths[i] - 2) // 3, 6))[1] + 1\n",
    "                    current_attn.topk(min((lengths[i] - 2) // 3, 6))[1]\n",
    "                )\n",
    "                top_masks = top_masks.cpu().tolist()\n",
    "            # 현재 문장의 input id를 가져온다.\n",
    "            current_sent = batch[\"input_ids\"][i][: lengths[i]]\n",
    "            count = 0\n",
    "            # top_masks에 뽑힌 index를 돌면서\n",
    "            for index in top_masks:\n",
    "                # mask해야 할 토큰이 and, of, or, so 에 해당하지 않으면\n",
    "                if tokenizer.decode(current_sent[index]) not in [ ## maybe add more!\n",
    "                    \" and\",\n",
    "                    \" of\",\n",
    "                    \" or\",\n",
    "                    \" so\",\n",
    "                ]:\n",
    "                    # token을 mask 한다.\n",
    "                    current_sent[index] = mask_token\n",
    "                    # count 수를 늘린다.\n",
    "                    count += 1\n",
    "                else:\n",
    "                    # 만약에 and, of, or, so 에 해당하면 아무것도 하지 않는다.\n",
    "                    pass\n",
    "            # sent = (\n",
    "            #     tokenizer.decode(current_sent)[3:-4]\n",
    "            #     .replace(\"<mask>\", \" <mask>\")\n",
    "            #     .strip()\n",
    "            # )\n",
    "            sent = (\n",
    "                # masking을 마친 input id를 decode 한다.\n",
    "                tokenizer.decode(current_sent)\n",
    "                .replace(\"<mask>\", \" <mask>\")\n",
    "                .strip()\n",
    "            )\n",
    "            # mask 된 곳이 없으면, 마지막에 추가한다.\n",
    "            if \"<mask>\" not in sent:\n",
    "                sent = sent + \" <mask>.\"\n",
    "            \n",
    "            # 처리된 문장을 output_sents에 추가한다.\n",
    "            output_sents[layer_num].append(sent)\n",
    "\n",
    "# with open(args.out_file + \".intermediate_sentences.txt\", \"w\") as f:\n",
    "#     f.write(\"\\n\".join(output_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample1['attn_L11'] = output_sents\n",
    "# sample1['attn_L12'] = output_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 12):\n",
    "    sample1[f\"attn_L{i+1}\"] = output_sents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['notebooks/results/attn_based_locate_binary_230524.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# joblib.dump(sample1, \"notebooks/results/attn_based_locate_balanced_230524.pkl\")\n",
    "joblib.dump(sample1, \"notebooks/results/attn_based_locate_binary_230524.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'attn_L11'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda/envs/mucoco/lib/python3.8/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda/envs/mucoco/lib/python3.8/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda/envs/mucoco/lib/python3.8/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'attn_L11'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample1\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtoxicity\u001b[39m\u001b[39m'\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39miloc[\u001b[39m1\u001b[39m, ][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m], sample1\u001b[39m.\u001b[39;49msort_values(by\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtoxicity\u001b[39;49m\u001b[39m'\u001b[39;49m, ascending\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\u001b[39m.\u001b[39;49miloc[\u001b[39m1\u001b[39;49m, ][\u001b[39m'\u001b[39;49m\u001b[39mattn_L11\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "File \u001b[0;32m~/anaconda/envs/mucoco/lib/python3.8/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/envs/mucoco/lib/python3.8/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/anaconda/envs/mucoco/lib/python3.8/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'attn_L11'"
     ]
    }
   ],
   "source": [
    "sample1.sort_values(by='toxicity', ascending=False).iloc[1, ]['text'], sample1.sort_values(by='toxicity', ascending=False).iloc[1, ]['attn_L11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[original]\n",
      "Ha ha, HILLARY LOST. All you hillary fools need to be rounded up and put in prison along with your leader. You are all anti--American scum. Perhaps we should dump you in Mexico along with all the illegal trash you let in....if you love 'em so much, go live with 'em. But you are not welcome here, you hate filled Marxist morons.\n",
      "[masked]\n",
      "Ha ha, HILLARY LOST. All you hillary fools need to be rounded up and put in prison along with your leader. You are all anti--American scum. Perhaps we should dump you in Mexico along with all the illegal trash you let in....if you love 'em so much, go live with 'em. But you are not welcome here, <mask> <mask> <mask> <mask> <mask> <mask>.\n",
      "[original]\n",
      "Two dumbs makes one dead.  Happens all too frequently.\n",
      "[masked]\n",
      "<mask> <mask> <mask> <mask> one dead.  Happens all too frequently.\n",
      "[original]\n",
      "Trash site.  Just like Hillary Clinton.\n",
      "[masked]\n",
      "Tr <mask> <mask>.  Just like Hillary Clinton.\n",
      "[original]\n",
      "Chomp, chomp ... SLURP! Pigs at the trough. More slop from the public, please!\n",
      "[masked]\n",
      "<mask>omp, chomp... SL <mask> <mask>! <mask> at the trough. More <mask> <mask> from the public, please!\n",
      "[original]\n",
      "how about capital for the buyers. they are as stupid as this jerk\n",
      "[masked]\n",
      "<mask> about <mask> for the buyers. they are as <mask> as this <mask>\n",
      "[original]\n",
      "Just an initial impression; I'd say Papadopoulos was blowing alot of smoke up someone's ass about the whole affair. \n",
      "The email was omitted because it appears that  alot of what Papadopoulos has being saying is a big steaming load of cow dung.\n",
      "[masked]\n",
      "Just an initial impression; I'd say Papadopoulos was blowing alot of smoke up <mask>'s <mask> about the whole affair. \n",
      "The email was omitted because it appears that  alot of what Papadopoulos has being saying is a <mask> steaming load of <mask> <mask> <mask>.\n",
      "[original]\n",
      "You said it nicer than I did. I find this type of person absolutely gross. They want to say whatever they want, great - I'm free to tell them how disgusting, vile, and stupid they are.\n",
      "[masked]\n",
      "You said it nicer than I did. I find this type of person absolutely gross. They want to say whatever they want, great - I'm free <mask> tell them how <mask>, <mask>, and <mask> <mask> <mask>.\n",
      "[original]\n",
      "Pffft!  Sarah takes to Facebook to show off her cheek injections and over the hill duck lips.  Sarah Palin is an embarrassment.\n",
      "[masked]\n",
      "<mask>ffft!  Sarah takes to Facebook to show off her <mask> injections and over the hill duck lips. <mask> Sarah Palin <mask> <mask> <mask>.\n",
      "[original]\n",
      "Klastri is so right:  Trump is just a buffoon entertainer.  He is the master of lies and manipulation.  He is a joke.\n",
      "[masked]\n",
      "<mask>last <mask> is so right:  Trump is just a buffoon entertainer.  He is the master of lies and manipulation.  <mask> <mask> <mask> <mask>.\n",
      "[original]\n",
      "THANK YOU. I'm so sick and tired of people acting like the military is filled with precious snowflakes that will be oh so very #triggered the minute that someone isn't genuflecting before the flag, the anthem, or the notion of invading yet another country for capitalist interests.\n",
      "\n",
      "\n",
      "How on Earth did this dumb association get made between such pageantry and people in the military without it getting laughed out of the room on the first day? It doesn't even pass the laugh test....the idea that because someone at some event involving ENTERTAINMENT doesn't do the proper rituals at the proper time would be insulting to those in the military? Ridiculous.\n",
      "[masked]\n",
      "<mask> <mask> YOU. I'm so sick and tired of people acting like the military is filled with precious snowflakes that will be oh so very #triggered the minute that someone isn't genuflecting before the flag, the anthem, or the notion of invading yet another country for capitalist interests.\n",
      "\n",
      "\n",
      "How on Earth did this dumb association get made between such pageantry and people in the military without it getting laughed out of the room on the first day? It doesn't even pass the laugh test....the idea that because someone at some event involving ENTERTAINMENT doesn't do the proper rituals at the proper time would be insulting to those in the <mask>? <mask> <mask> <mask>.\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(sample1['text'].tolist()[:10], output_sents[10][:10]):\n",
    "    print(\"[original]\")\n",
    "    print(i)\n",
    "    print(\"[masked]\")\n",
    "    print(j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

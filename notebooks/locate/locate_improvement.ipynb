{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/hyeryung_son/mucoco\")\n",
    "\n",
    "import torch\n",
    "import string\n",
    "\n",
    "from notebooks.utils.load_ckpt import define_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation + '\\n '\n",
    "punctuations = list(punctuations)\n",
    "punctuations.remove('-')\n",
    "\n",
    "\n",
    "# model 의 forward 함수에서 정의를 output_attentions=True를 넘길 수 있게 되어 있다.\n",
    "def locate(model, tokenizer, batch, max_num_tokens = 6, num_layer=10):\n",
    "    # torch.cuda.empty_cache()\n",
    "    # forward\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        classifier_output = model.forward(**batch, output_attentions=True)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    # get attentions\n",
    "    attentions = classifier_output[\"attentions\"]\n",
    "    # attention_mask에서 1의 개수를 셈\n",
    "    # lengths = [i.tolist().count(1) for i in batch[\"attention_mask\"]]\n",
    "    lengths = [len(x) for x in batch[\"input_ids\"]]\n",
    "    # 보고자 하는 attention layer 만 가져옴\n",
    "    attentions = attentions[\n",
    "        num_layer # originally 10\n",
    "    ]\n",
    "    cls_attns = attentions.max(1)[0][:, 0]\n",
    "    \n",
    "    stopwords = [\" and\", \" of\", \" or\", \" so\"] + punctuations\n",
    "    stopwords_ids = [tokenizer.encode(word)[0] for word in stopwords]\n",
    "\n",
    "    locate_ixes=[]\n",
    "    for i, attn in enumerate(cls_attns):\n",
    "        \n",
    "        current_sent = batch[\"input_ids\"][i][: lengths[i]]\n",
    "        print(\"current_sent\", current_sent)\n",
    "        no_punc_indices = torch.where(~torch.isin(current_sent, torch.tensor(stopwords_ids).to(torch.device('cuda'))))[0]\n",
    "        print(\"no_punc_indices\", no_punc_indices)\n",
    "        \n",
    "        # current tokenizer does not add <s> and </s> to the sentence.\n",
    "        current_attn = attn[: lengths[i]].softmax(-1) \n",
    "        print(\"current_attn\", current_attn)\n",
    "        current_attn = current_attn[no_punc_indices]\n",
    "        print(\"current_attn\", current_attn)\n",
    "        \n",
    "        # 이 값의 평균을 구한다.\n",
    "        avg_value = current_attn.view(-1).mean().item()\n",
    "        print(\"avg_value\", avg_value)\n",
    "        # 이 값 중에 평균보다 큰 값을 지니는 위치를 찾는다.\n",
    "        # fixed to reflect that sometimes the sequence length is 1.\n",
    "        top_masks = ((current_attn >= avg_value).nonzero().view(-1)) \n",
    "        torch.cuda.empty_cache()\n",
    "        top_masks = top_masks.cpu().tolist()\n",
    "        print(\"top_masks\", top_masks)\n",
    "        \n",
    "        # attention 값이 평균보다 큰 토큰의 수가 k개 또는 문장 전체 토큰 수의 1/3 보다 크면  \n",
    "        if len(top_masks) > min((lengths[i]) // 3, max_num_tokens):\n",
    "            # 그냥 attention 값 기준 k 개 또는 토큰 수/3 중 작은 수를 뽑는다.\n",
    "            top_masks = (\n",
    "                current_attn.topk(max(min((lengths[i]) // 3, max_num_tokens), 1))[1]\n",
    "            )\n",
    "            top_masks = top_masks.cpu().tolist()\n",
    "            print(\"top k top_masks\", top_masks)\n",
    "        top_masks_final = no_punc_indices[top_masks]\n",
    "        print(\"top_masks_final\", top_masks_final)\n",
    "        \n",
    "        # ToDo: word의 일부만 locate 한 경우, word 전체를 locate 한다.\n",
    "        j, k = 0, 0\n",
    "        grouped_tokens = []\n",
    "        grouped_tokens_for_word = []\n",
    "        words = tokenizer.decode(current_sent).strip().split()\n",
    "        print(\"words\", words)\n",
    "        while j < len(current_sent):\n",
    "            if (tokenizer.decode(current_sent[j]).strip() not in punctuations) and (tokenizer.decode(current_sent[j]) not in ['\\n', ' ']):\n",
    "                print(\"tokenizer.decode(current_sent[j])\", tokenizer.decode(current_sent[j]))\n",
    "                while k < len(words):\n",
    "                    if tokenizer.decode(current_sent[j]).strip() in words[k]:\n",
    "                        grouped_tokens_for_word.append(j)\n",
    "                        break\n",
    "                    else:\n",
    "                        grouped_tokens.append(grouped_tokens_for_word)\n",
    "                        grouped_tokens_for_word = []\n",
    "                        k += 1\n",
    "            j += 1\n",
    "        grouped_tokens.append(grouped_tokens_for_word)\n",
    "        print(\"grouped_tokens\", grouped_tokens)\n",
    "        \n",
    "        top_masks_final.sort()\n",
    "        top_masks_final_final = []\n",
    "        for index in top_masks_final:\n",
    "            print(\"index\", index)\n",
    "            if index not in top_masks_final_final:\n",
    "                word = [grouped_ixes for grouped_ixes in grouped_tokens if index in grouped_ixes]\n",
    "                print(\"word\", word)\n",
    "                if len(word) > 0:\n",
    "                    word = word[0]\n",
    "                else:\n",
    "                    print(f\"!!! {index} not in the grouped_ixes {grouped_tokens}\")\n",
    "                    print(f\"!!! tokenizer.decode(index): {tokenizer.decode(index)}\")\n",
    "                top_masks_final_final.extend(word)\n",
    "        locate_ixes.append(list(set(top_masks_final_final)))\n",
    "\n",
    "            \n",
    "    return locate_ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-large and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE:  cuda\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer =define_model('/home/hyeryung_son/mucoco/models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(\"You're a fucking moron.\", return_tensors=\"pt\").to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_sent tensor([1639,  821,  257, 9372, 2146,  261,   13], device='cuda:0')\n",
      "no_punc_indices tensor([0, 1, 2, 3, 4, 5], device='cuda:0')\n",
      "current_attn tensor([0.1384, 0.1600, 0.1325, 0.1467, 0.1485, 0.1400, 0.1340],\n",
      "       device='cuda:0')\n",
      "current_attn tensor([0.1384, 0.1600, 0.1325, 0.1467, 0.1485, 0.1400], device='cuda:0')\n",
      "avg_value 0.14433614909648895\n",
      "top_masks [1, 3, 4]\n",
      "top k top_masks [1]\n",
      "top_masks_final tensor([1], device='cuda:0')\n",
      "words [\"You're\", 'a', 'fucking', 'moron.']\n",
      "tokenizer.decode(current_sent[j]) You\n",
      "tokenizer.decode(current_sent[j]) 're\n",
      "tokenizer.decode(current_sent[j])  a\n",
      "tokenizer.decode(current_sent[j])  fucking\n",
      "tokenizer.decode(current_sent[j])  mor\n",
      "tokenizer.decode(current_sent[j]) on\n",
      "grouped_tokens [[0, 1], [2], [3], [4, 5]]\n",
      "index tensor(1, device='cuda:0')\n",
      "word [[0, 1]]\n",
      "indices [[0, 1]]\n"
     ]
    }
   ],
   "source": [
    "indices = locate(model, tokenizer, batch, max_num_tokens=1)\n",
    "print(\"indices\", indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You    True    're    True     a    False     fucking    False     mor    False    on    False    .    False    "
     ]
    }
   ],
   "source": [
    "for i, tok in enumerate(batch['input_ids'][0]):\n",
    "    print(tokenizer.decode(tok), end='    ')\n",
    "    print(i in indices[0], end='    ')\n",
    "    # if i in indices[0]:\n",
    "    #     print(\"<mask>\", end='    ')\n",
    "    # else:\n",
    "    #     print(tokenizer.decode(tok), end='    ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hyeryungson/mucoco\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.nn import Softmax\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from notebooks.load_ckpt_w_attn import define_model\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터 로드\n",
    "samples=pd.read_csv('./notebooks/results/samples.csv')\n",
    "# label이 1인 데이터만 사용\n",
    "sample1=samples.loc[samples['label']==1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "look above for padding\n",
      "Adding special tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-large and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<pad>', '</s>', '<unk>', 'madeupword0000', 'madeupword0001', 'madeupword0002', '<mask>']\n",
      "computing vecmap\n",
      "torch.Size([1280, 768]) torch.Size([768, 1280])\n"
     ]
    }
   ],
   "source": [
    "# load trained model\n",
    "ckpt_path='/home/hyeryungson/mucoco/models_bak_contd/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best/pytorch_model.bin'\n",
    "model, config, tokenizer = define_model(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([50256], ['<|endoftext|>'], 50257)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 논문에서는 gpt-2의 tokenizer를 사용하였으므로, mask token이 기존에는 없었음\n",
    "tokenizer.all_special_ids, tokenizer.all_special_tokens, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer에 mask token 추가\n",
    "SPECIAL_TOKENS = {\"mask_token\": \"<mask>\"}\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([50256, 50257], ['<|endoftext|>', '<mask>'], 50257)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 논문에서는 gpt-2의 tokenizer를 사용하였으므로, mask token이 기존에는 없었음\n",
    "tokenizer.all_special_ids, tokenizer.all_special_tokens, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify if the code is correct\n",
    "test_sent = sample1['text'].tolist()[0:10]\n",
    "batch = tokenizer(test_sent, padding=True, return_tensors=\"pt\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm more concerned about the mind control being practiced by facists like tRump!<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cls token이 따로 없는데 잘 학습이 된게 맞을까? -> 상관없다고 하심 (교수님)\n",
    "tokenizer.decode(batch['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RoBERTa\n",
      "Loaded RoBERTa classifier in 8.58306884765625e-05s\n",
      "generating masks\n",
      "Extracting SLOT tokens\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:04<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:04<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:05<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:05<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:05<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:05<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:05<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:05<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:05<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:05<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:04<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:04<00:00,  4.84it/s]\n"
     ]
    }
   ],
   "source": [
    "_time = time.time()\n",
    "print(\"Loading RoBERTa\")\n",
    "classifier = model\n",
    "print(f\"Loaded RoBERTa classifier in {time.time() - _time}s\")\n",
    "\n",
    "# mask_token에 해당하는 id 설정\n",
    "mask_token = tokenizer.encode(\"<mask>\", add_special_tokens=False)[0]\n",
    "output_sents=dict()\n",
    "print(\"generating masks\")\n",
    "print(\"Extracting SLOT tokens\")\n",
    "# mask 대상 텍스트 로드\n",
    "file = sample1['text'].tolist()\n",
    "# 레이어 별로 attention 값을 기준으로 mask 해보기 위해서 outer for-loop 추가\n",
    "for layer_num in range(0, 12):\n",
    "    print(layer_num)\n",
    "    output_sents[layer_num] = []\n",
    "    # 텍스트를 32개씩 batch로 처리할 예정임\n",
    "    for i in tqdm(range(0, len(file), 32)):\n",
    "        # get batch\n",
    "        input_lines = file[i : i + 32]\n",
    "        # tokenize\n",
    "        batch = tokenizer(\n",
    "            input_lines, padding=True, return_tensors=\"pt\", truncation=True\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        # forward\n",
    "        classifier_output = classifier.forward(\n",
    "            batch[\"input_ids\"].cuda(),\n",
    "            attention_mask=batch[\"attention_mask\"].cuda(),\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        # get attentions\n",
    "        attentions = classifier_output[\"attentions\"]\n",
    "        # attention_mask에서 1의 개수를 셈\n",
    "        lengths = [i.tolist().count(1) for i in batch[\"attention_mask\"]]\n",
    "        # 보고자 하는 attention layer 만 가져옴\n",
    "        attentions = attentions[\n",
    "            layer_num # originally 10\n",
    "        ]  # 10 is chosen because it is the magical layer number of the grand elves\n",
    "        # attentions.max(1)[0]: axis 1 에 대해서 max 값을 가져온다. (sequence에 대해서 여러개의 attention heads 중에서 가장 큰 값을 가져온다.) -> [:, 0] 그리고 나서 cls token의 attention을 가져온다.\n",
    "        cls_attns = attentions.max(1)[0][:, 0]\n",
    "        # batch에 있는 각 example에 대해서\n",
    "        for i, attn in enumerate(cls_attns):\n",
    "            # attention_mask가 1인 곳 까지의 attention을 보고, start of sentence와 end of sentence에 해당하는 token을 제거하고, softmax를 취한다.\n",
    "            # current_attn = attn[: lengths[i]][1:-1].softmax(-1)\n",
    "            current_attn = attn[: lengths[i]].softmax(-1) # <- current tokenizer does not add <s> and </s> to the sentence.\n",
    "            # 이 값의 평균을 구한다.\n",
    "            avg_value = current_attn.view(-1).mean().item()\n",
    "            # 이 값 중에 평균보다 큰 값을 지니는 위치를 찾는다. (+1 because we skipped the first token)\n",
    "            # top_masks = ((current_attn > avg_value).nonzero().view(-1)) + 1\n",
    "            top_masks = ((current_attn > avg_value).nonzero().view(-1))\n",
    "            torch.cuda.empty_cache()\n",
    "            top_masks = top_masks.cpu().tolist()\n",
    "            # attention 값이 평균보다 큰 토큰의 수가 6 또는 문장 전체 토큰 수의 1/3 보다 크면  \n",
    "            if len(top_masks) > min((lengths[i] - 2) // 3, 6):\n",
    "                # 그냥 attention 값 기준 top k 개 (k = 6 또는 토큰 수/3)를 뽑는다.\n",
    "                top_masks = (\n",
    "                    # current_attn.topk(min((lengths[i] - 2) // 3, 6))[1] + 1\n",
    "                    current_attn.topk(min((lengths[i] - 2) // 3, 6))[1]\n",
    "                )\n",
    "                top_masks = top_masks.cpu().tolist()\n",
    "            # 현재 문장의 input id를 가져온다.\n",
    "            current_sent = batch[\"input_ids\"][i][: lengths[i]]\n",
    "            count = 0\n",
    "            # top_masks에 뽑힌 index를 돌면서\n",
    "            for index in top_masks:\n",
    "                # mask해야 할 토큰이 and, of, or, so 에 해당하지 않으면\n",
    "                if tokenizer.decode(current_sent[index]) not in [ ## maybe add more!\n",
    "                    \" and\",\n",
    "                    \" of\",\n",
    "                    \" or\",\n",
    "                    \" so\",\n",
    "                ]:\n",
    "                    # token을 mask 한다.\n",
    "                    current_sent[index] = mask_token\n",
    "                    # count 수를 늘린다.\n",
    "                    count += 1\n",
    "                else:\n",
    "                    # 만약에 and, of, or, so 에 해당하면 아무것도 하지 않는다.\n",
    "                    pass\n",
    "            # sent = (\n",
    "            #     tokenizer.decode(current_sent)[3:-4]\n",
    "            #     .replace(\"<mask>\", \" <mask>\")\n",
    "            #     .strip()\n",
    "            # )\n",
    "            sent = (\n",
    "                # masking을 마친 input id를 decode 한다.\n",
    "                tokenizer.decode(current_sent)\n",
    "                .replace(\"<mask>\", \" <mask>\")\n",
    "                .strip()\n",
    "            )\n",
    "            # mask 된 곳이 없으면, 마지막에 추가한다.\n",
    "            if \"<mask>\" not in sent:\n",
    "                sent = sent + \" <mask>.\"\n",
    "            \n",
    "            # 처리된 문장을 output_sents에 추가한다.\n",
    "            output_sents[layer_num].append(sent)\n",
    "\n",
    "# with open(args.out_file + \".intermediate_sentences.txt\", \"w\") as f:\n",
    "#     f.write(\"\\n\".join(output_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample1['attn_L11'] = output_sents\n",
    "# sample1['attn_L12'] = output_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 12):\n",
    "    sample1[f\"attn_L{i+1}\"] = output_sents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['notebooks/results/attn_based_locate_230411.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(sample1, \"notebooks/results/attn_based_locate_230411.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"You're a fucking moron.\", \"You're a fucking <mask>on.\")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1.sort_values(by='api_proba', ascending=False).iloc[1, ]['text'], sample1.sort_values(by='api_proba', ascending=False).iloc[1, ]['attn_L11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[original]\n",
      "I'm more concerned about the mind control being practiced by facists like tRump!\n",
      "[masked]\n",
      "I'm <mask> concerned about the <mask> control being practiced by <mask> <mask> <mask> tRump!\n",
      "[original]\n",
      "Right wing terror apologist. You're a sick one.\n",
      "[masked]\n",
      "Right wing terror apologist. You're <mask> <mask> <mask>.\n",
      "[original]\n",
      "a way to offset the taxpayer is to fire all the border guards.  We clearly don't need them.  They ain't enforcing anything except coddling these so called (very healthy and rich looking) refugees from the United States of America.  The joke is on us.  Sad.  Jughead should be grossly ashamed of his pathetic leadership. A sad day for Canada indeed.\n",
      "[masked]\n",
      "a way to offset the taxpayer is to fire all the border guards.  We clearly don't need them.  They ain't enforcing anything except coddling these so called (very healthy and rich looking) refugees from the United States of America.  The joke is on us.  Sad.  Jug <mask> <mask> be grossly <mask> of <mask> <mask> leadership. A sad day for Canada indeed.\n",
      "[original]\n",
      "The man from Kenya Born in Kenya, the Republicans & Democrats let this man destroy America not one step up and tried to stop, Look at America after 7 yrs. 7 mths. of this jerk running around do nothing, make America look weak.\n",
      "[masked]\n",
      "The <mask> from Kenya Born in Kenya, the <mask> & Democrats let this <mask> destroy America not one step up and tried to stop, Look at America after 7 yrs. 7 mths. of <mask> <mask> running around do nothing, make America look <mask>.\n",
      "[original]\n",
      "Someone who doesn't know the difference between weather and climate and who thinks that the science  upon which all of this is based comes from meteorologists has no business referring to other people as \"idiots.\"\n",
      "[masked]\n",
      "Someone who doesn't know the difference between weather and climate and who thinks that the science  upon which all of this is based comes from meteorologists has no business referring to other <mask> <mask> <mask> <mask> <mask> <mask>\n",
      "[original]\n",
      "The sheep are swallowing it up. You'd have to be a complete fool to believe a person would commit such a heinous act and conveniently leave his wallet in the cab of the truck.\n",
      "[masked]\n",
      "The sheep are swallowing it <mask>. <mask> <mask> <mask> to be <mask> complete <mask> to believe a person would commit such a heinous act and conveniently leave his wallet in the cab of the truck.\n",
      "[original]\n",
      "your an idiot\n",
      "[masked]\n",
      "your an idiot <mask>.\n",
      "[original]\n",
      "Making assumptions because you can't deal with the opinions of others, I see.  Typical of people like you.\n",
      "If it's not accusations of Russian trolls, it's Fox or Breitbart or Rush.  Anything to make yourself feel better by making assumptions about people you don't know.\n",
      "[masked]\n",
      "Making assumptions because you can't deal with the opinions of others, I see.  Typical of people <mask> you.\n",
      "If it's not accusations of <mask> trolls, it's Fox or Breitbart or Rush.  Anything to make yourself feel better by making assumptions about people <mask> <mask> <mask> <mask>.\n",
      "[original]\n",
      "DUH - guess what - the total history of the church and its sacraments does depend upon how the church supports its sacraments via scripture - the core of the church's understanding.  What you state is not only inaccurate but heretical.  All the sacraments have a foundation in scripture - how these were pastorally implemented, etc. evolved but always grounded in our story - scripture.   \n",
      "It has nothing to do with *rigor* - your statement is just plan ignorant and wrong.\n",
      "[masked]\n",
      "D <mask>H - guess what - the total history of the church and its sacraments does depend upon how the church supports its sacraments via scripture - the core of the church's understanding.  What you state is not only inaccurate but heretical.  All the sacraments have a foundation in scripture - how these were pastorally implemented, etc. evolved but always grounded in our story - scripture.   \n",
      "It has nothing to do with *rigor* - your statement is <mask> <mask> <mask> and <mask>.\n",
      "[original]\n",
      "That's exactly what's wrong with this way of thinking. It's not unlike what is going on Russia where people think all homosexuals are pedophiles.\n",
      "\n",
      "Sexuality is a spectrum and it doesn't always have to do with sexual ACTIVITY. The fact that you immediately equate transgender people with being perverted and predatory shows your ignorance. It's insulting, and low.\n",
      "\n",
      "This isn't a debate about wether or not we should allow sexual predators into bathrooms, it's about accepting that some people actually ARE transgender and have the right to live a lifestyle that matches their sexual identity. This is a debate about the government not having the right to look down people's pants and decide where they belong. It's about individual freedom and civil liberty. \n",
      "\n",
      "Your boys don't sound transgender, and children of the same gender are just as capable of any sort of sexual assault. Your logic would state every child should have an individual bathroom if that's your concern.\n",
      "[masked]\n",
      "That's exactly what's wrong with this way of thinking. It's not unlike what is going on Russia where people think <mask> homosexuals <mask> <mask> <mask>.\n",
      "\n",
      "Sexuality is a spectrum and it doesn't always have to do with sexual ACTIVITY. The fact that you immediately equate transgender people with being perverted and predatory shows your ignorance. It's insulting, and low.\n",
      "\n",
      "This isn't a debate about wether or not we should allow sexual predators into bathrooms, it's about accepting that some people actually ARE transgender and have the right to live a lifestyle that matches their sexual identity. This is a debate about the government not having the right to look down people's pants and decide where they belong. It's about individual freedom and civil liberty. \n",
      "\n",
      " <mask> boys don't sound transgender, and children of the same gender are just as capable of any sort of sexual assault. Your logic would state every child should have an individual bathroom if that's your concern.\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(sample1['text'].tolist()[:10], output_sents[10][:10]):\n",
    "    print(\"[original]\")\n",
    "    print(i)\n",
    "    print(\"[masked]\")\n",
    "    print(j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

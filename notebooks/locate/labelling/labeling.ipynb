{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "07/15/23\n",
    "Code to prepare for asking lab members to label\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "rdir = '/data/hyeryung'\n",
    "\n",
    "if os.getcwd() != f'{rdir}/mucoco':\n",
    "    print(f\"change dir to '{rdir}/mucoco'\")\n",
    "    os.chdir(f'{rdir}/mucoco')\n",
    "\n",
    "from notebooks.utils.load_ckpt import define_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all = pd.read_json('data/toxicity/jigsaw-unintended-bias-in-toxicity-classification/fine-grained/test.jsonl',\n",
    "                    lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num test samples 3998\n"
     ]
    }
   ],
   "source": [
    "print(\"num test samples\", test_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking duplicates... 0 duplicated texts\n"
     ]
    }
   ],
   "source": [
    "print(f\"checking duplicates... {test_all.duplicated(subset=['text']).sum()} duplicated texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_all.loc[test_all['toxicity']>=0.5].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num toxic test samples 1960\n"
     ]
    }
   ],
   "source": [
    "print(\"num toxic test samples\", test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxicity_bin\n",
       "[0.5, 0.6)    659\n",
       "[0.6, 0.7)    498\n",
       "[0.7, 0.8)    397\n",
       "[0.8, 0.9)    300\n",
       "[0.9, 1.0)     51\n",
       "[1.0, 1.1)     55\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check distribution by toxicity\n",
    "test = test.reset_index(drop=True)\n",
    "test['toxicity_bin'] = pd.cut(test['toxicity'], np.arange(0.5, 1.1, 0.1), right=False)#, labels=[\"[0.5, 0.6)\"])\n",
    "test_=test.copy()\n",
    "test.groupby('toxicity_bin').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어떻게 하는게 가장 좋을까? 흠.. \n",
    "# 고려해야 할점.. \n",
    "# - 어떤 UI 가 라벨링하기 좋을까? -> 일단 그것부터 생각?\n",
    "# - toxicity 값은 보여줄까? -> 일단 샘플을 다 뽑아놓고, 필터링해서 제공하면 되지 않을까? 일단 컬럼에 포함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교수님과 얘기했던 UI = 일단은 word 단위로 라벨을 받아보자.\n",
    "test['text_words'] = test.text.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_bin</th>\n",
       "      <th>text_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You aren't that bright. Citizens were picking ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[0.5, 0.6)</td>\n",
       "      <td>[You, aren't, that, bright., Citizens, were, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>References are constantly being made  that ind...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>[0.8, 0.9)</td>\n",
       "      <td>[References, are, constantly, being, made, tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"...these are the people..\"  Those REAL people...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>[0.8, 0.9)</td>\n",
       "      <td>[\"...these, are, the, people..\", Those, REAL, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This article is especially hilarious in light ...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[0.6, 0.7)</td>\n",
       "      <td>[This, article, is, especially, hilarious, in,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolutely not!  We should never learn to live...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[0.5, 0.6)</td>\n",
       "      <td>[Absolutely, not!, We, should, never, learn, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>I don’t see it happening because of all the pr...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>[0.7, 0.8)</td>\n",
       "      <td>[I, don’t, see, it, happening, because, of, al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>Every time a lib opens his mouth more garbage ...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[0.6, 0.7)</td>\n",
       "      <td>[Every, time, a, lib, opens, his, mouth, more,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>the dealers don't have a hard time making mone...</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>[0.7, 0.8)</td>\n",
       "      <td>[the, dealers, don't, have, a, hard, time, mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>My opinion is Trump keeps opening his mouth an...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[0.6, 0.7)</td>\n",
       "      <td>[My, opinion, is, Trump, keeps, opening, his, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>\"... and claiming the booking process cost $10...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[0.5, 0.6)</td>\n",
       "      <td>[\"..., and, claiming, the, booking, process, c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1960 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  toxicity  \\\n",
       "0     You aren't that bright. Citizens were picking ...  0.500000   \n",
       "1     References are constantly being made  that ind...  0.800000   \n",
       "2     \"...these are the people..\"  Those REAL people...  0.833333   \n",
       "3     This article is especially hilarious in light ...  0.600000   \n",
       "4     Absolutely not!  We should never learn to live...  0.500000   \n",
       "...                                                 ...       ...   \n",
       "1955  I don’t see it happening because of all the pr...  0.750000   \n",
       "1956  Every time a lib opens his mouth more garbage ...  0.600000   \n",
       "1957  the dealers don't have a hard time making mone...  0.738462   \n",
       "1958  My opinion is Trump keeps opening his mouth an...  0.600000   \n",
       "1959  \"... and claiming the booking process cost $10...  0.500000   \n",
       "\n",
       "     toxicity_bin                                         text_words  \n",
       "0      [0.5, 0.6)  [You, aren't, that, bright., Citizens, were, p...  \n",
       "1      [0.8, 0.9)  [References, are, constantly, being, made, tha...  \n",
       "2      [0.8, 0.9)  [\"...these, are, the, people..\", Those, REAL, ...  \n",
       "3      [0.6, 0.7)  [This, article, is, especially, hilarious, in,...  \n",
       "4      [0.5, 0.6)  [Absolutely, not!, We, should, never, learn, t...  \n",
       "...           ...                                                ...  \n",
       "1955   [0.7, 0.8)  [I, don’t, see, it, happening, because, of, al...  \n",
       "1956   [0.6, 0.7)  [Every, time, a, lib, opens, his, mouth, more,...  \n",
       "1957   [0.7, 0.8)  [the, dealers, don't, have, a, hard, time, mak...  \n",
       "1958   [0.6, 0.7)  [My, opinion, is, Trump, keeps, opening, his, ...  \n",
       "1959   [0.5, 0.6)  [\"..., and, claiming, the, booking, process, c...  \n",
       "\n",
       "[1960 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(test['text_words'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = pd.DataFrame(test['text_words'].apply(len)).rename(columns={'text_words': 'num_words'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_labelling = pd.concat([test[['text', 'toxicity']], lens, words], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_labelling.to_excel('notebooks/locate/labelling/test_geq_0.5.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_labelling = pd.read_excel('notebooks/locate/labelling/test_geq_0.5.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_labelling[\"type\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_labelling[\"text_index\"] = df_for_labelling.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "blanks_for_labelling = df_for_labelling.copy()\n",
    "blanks_for_labelling[\"type\"] = 1\n",
    "blanks_for_labelling.loc[:, 0:195] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_labelling = pd.concat([df_for_labelling, blanks_for_labelling], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_labelling = df_for_labelling.sort_values(by=['text_index', 'type'], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_labelling = df_for_labelling.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_labelling['type'].replace({0: 'text', 1: 'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([      'text',   'toxicity',  'num_words',            0,            1,\n",
       "                  2,            3,            4,            5,            6,\n",
       "       ...\n",
       "                188,          189,          190,          191,          192,\n",
       "                193,          194,          195,       'type', 'text_index'],\n",
       "      dtype='object', length=201)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_labelling.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_labelling.to_excel('notebooks/locate/labelling/test_geq_0.5_v0.2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old code (06.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('data/toxicity/jigsaw-unintended-bias-in-toxicity-classification/fine-grained/test.jsonl',\n",
    "                    lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.loc[test['toxicity']>=0.5].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select toxic 0.5~1.0\n",
    "test = test.reset_index(drop=True)\n",
    "test['toxicity_bin'] = pd.cut(test['toxicity'], np.arange(0.5, 1.1, 0.1), labels=np.arange(6))\n",
    "test_=test.copy()\n",
    "\n",
    "## randomly select from each bin\n",
    "tot_count=150\n",
    "bin_count=6\n",
    "test = test.groupby('toxicity_bin').sample(int(tot_count/bin_count), random_state=11)\n",
    "\n",
    "## add 4 examples from toxicity 1\n",
    "test = pd.concat([test, test_.loc[((~test_.index.isin(test.index)) & (test_.toxicity>0.99))].sample(4, random_state=11)], axis=0)\n",
    "\n",
    "test.to_json('./notebooks/results/labelling_samples_test_geq0.5_150.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-large and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE:  cuda\n"
     ]
    }
   ],
   "source": [
    "model,config,tokenizer=define_model('/home/hyeryungson/mucoco/models_bak_contd/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best/pytorch_model.bin',\n",
    "                                 output_attentions=True, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized=tokenizer(test['text'].tolist(),padding=True, return_tensors=\"pt\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model input size와 같아야할거같은데!\n",
    "tokenized_ids=tokenized['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(tokenized_ids.shape[0]):\n",
    "    tmp_sent = tokenized_ids[i]\n",
    "    tmp_toks = [tokenizer.decode(x) for x in tmp_sent]\n",
    "    res.append(tmp_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting from index 4.0\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('./notebooks/results/toxic_labels.txt'):\n",
    "    with open('./notebooks/results/toxic_labels.txt','r') as f:\n",
    "        contents = f.readlines()\n",
    "    last_index = len(contents)/2\n",
    "    print(f'starting from index {last_index}')\n",
    "else:\n",
    "    fptr = open('./notebooks/results/toxic_labels.txt', 'w')\n",
    "    fptr.close()\n",
    "    last_index=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.525 ||||||||||| as long as you cannot admit who your enemies are you are losing fast. these guys want you dead yet they get modern missiles from your army. WTF!!!!<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "0 ------ as\n",
      "1 ------  long\n",
      "2 ------  as\n",
      "3 ------  you\n",
      "4 ------  cannot\n",
      "5 ------  admit\n",
      "6 ------  who\n",
      "7 ------  your\n",
      "8 ------  enemies\n",
      "9 ------  are\n",
      "10 ------  you\n",
      "11 ------  are\n",
      "12 ------  losing\n",
      "13 ------  fast\n",
      "14 ------ .\n",
      "15 ------  these\n",
      "16 ------  guys\n",
      "17 ------  want\n",
      "18 ------  you\n",
      "19 ------  dead\n",
      "20 ------  yet\n",
      "21 ------  they\n",
      "22 ------  get\n",
      "23 ------  modern\n",
      "24 ------  missiles\n",
      "25 ------  from\n",
      "26 ------  your\n",
      "27 ------  army\n",
      "28 ------ .\n",
      "29 ------  W\n",
      "30 ------ TF\n",
      "31 ------ !!!!\n",
      "List you typed: \n",
      "0.525 ||||||||||| Bahaha. \n",
      "\n",
      "That is super funny.\n",
      "\n",
      "Seriously, though. If I ever wanted to kill myself, I'd climb your ego, and jump to your IQ.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "0 ------ Bah\n",
      "1 ------ aha\n",
      "2 ------ .\n",
      "3 ------  \n",
      "4 ------ \n",
      "\n",
      "5 ------ \n",
      "\n",
      "6 ------ That\n",
      "7 ------  is\n",
      "8 ------  super\n",
      "9 ------  funny\n",
      "10 ------ .\n",
      "11 ------ \n",
      "\n",
      "12 ------ \n",
      "\n",
      "13 ------ Seriously\n",
      "14 ------ ,\n",
      "15 ------  though\n",
      "16 ------ .\n",
      "17 ------  If\n",
      "18 ------  I\n",
      "19 ------  ever\n",
      "20 ------  wanted\n",
      "21 ------  to\n",
      "22 ------  kill\n",
      "23 ------  myself\n",
      "24 ------ ,\n",
      "25 ------  I\n",
      "26 ------ 'd\n",
      "27 ------  climb\n",
      "28 ------  your\n",
      "29 ------  ego\n",
      "30 ------ ,\n",
      "31 ------  and\n",
      "32 ------  jump\n",
      "33 ------  to\n",
      "34 ------  your\n",
      "35 ------  IQ\n",
      "36 ------ .\n"
     ]
    }
   ],
   "source": [
    "for i in range(last_index, tokenized_ids.shape[0]):\n",
    "    try:\n",
    "        print(f\"{test['toxicity'].tolist()[i]} ||||||||||| {''.join(res[i])}\")\n",
    "        for j in range(len(res[i])):\n",
    "            if res[i][j] != '<|endoftext|>':\n",
    "                print(f\"{j} ------ {res[i][j]}\")\n",
    "        toxic_tokens=input(\"input comma separated list of tokens that are toxic >>\")\n",
    "        if '-' in toxic_tokens:\n",
    "            start, end = toxic_tokens.split('-')    \n",
    "            toxic_tokens = ','.join([str(x) for x in range(int(start.strip()), int(end.strip())+1,1)])\n",
    "        print(f\"List you typed: {toxic_tokens}\")\n",
    "        # toxic_tokens=toxic_tokens.split(\",\")\n",
    "        fptr = open('./notebooks/results/toxic_labels.txt', 'a')\n",
    "        fptr.write(''.join(res[i]) + '\\n')\n",
    "        fptr.write(toxic_tokens + '\\n')\n",
    "        fptr.close()\n",
    "    except KeyboardInterrupt:\n",
    "        fptr = open('./notebooks/results/toxic_labels.txt', 'a')\n",
    "        fptr.write(i + '\\n')\n",
    "        fptr.close()\n",
    "        last_index = i    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mucoco",
   "language": "python",
   "name": "mucoco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

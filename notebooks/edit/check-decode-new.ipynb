{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "025a4494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "os.chdir('/data/hyeryung/mucoco/')\n",
    "from mucoco.utils import TargetProbability, TargetEmbeddings, TargetSimplex, Lambda, Optimizer, get_epsilon\n",
    "import mucoco.losses as lossbuilder\n",
    "import mucoco.options as options\n",
    "import mucoco.utils as utils\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb1b9f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import mucoco.utils\n",
    "importlib.reload(mucoco.utils)\n",
    "from mucoco.utils import TargetProbability, TargetEmbeddings, TargetSimplex, Lambda, Optimizer, get_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f46c07c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To control logging level for various modules used in the application:\n",
    "# from here: https://github.com/huggingface/transformers/issues/3050\n",
    "def set_global_logging_level(level=logging.ERROR, prefices=[\"\"]):\n",
    "    \"\"\"\n",
    "    Override logging levels of different modules based on their name as a prefix.\n",
    "    It needs to be invoked after the modules have been loaded so that their loggers have been initialized.\n",
    "\n",
    "    Args:\n",
    "        - level: desired level. e.g. logging.INFO. Optional. Default is logging.ERROR\n",
    "        - prefices: list of one or more str prefices to match (e.g. [\"transformers\", \"torch\"]). Optional.\n",
    "          Default is `[\"\"]` to match all active loggers.\n",
    "          The match is a case-sensitive `module_name.startswith(prefix)`\n",
    "    \"\"\"\n",
    "    prefix_re = re.compile(fr'^(?:{ \"|\".join(prefices) })')\n",
    "    for name in logging.root.manager.loggerDict:\n",
    "        if re.match(prefix_re, name):\n",
    "            logging.getLogger(name).setLevel(level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0539bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "args = joblib.load('./args_decoding.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00a2f374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(AR_temperature=1.0, AR_top_k=0, AR_top_p=0.96, adam_betas='(0.9, 0.999)', adam_eps=1e-08, additional_data='none', allow_diff_vocab=True, always_mucoco='false', batch_size=1, beam=False, beam_size=1, betas='0.8:0.2', bos=True, cache_dir='hf_cache', coeff_pattern='constant', coeff_steps=200, cpu=False, custom_epsilons='none', damp=False, dampness=0.1, data='data//control-prompts/nontoxic_prompts-1.jsonl', datastyle='jsonl', debug=False, debug_gradients='false', decay_method=None, decay_steps=1, decode_temperature=0.1, dynamic_lambda_update=True, dynamic_lr_update=True, early_stop_steps=40, embedgd_begin_temperature=0.5, embedgd_decay_method=None, embedgd_do_sample='false', embedgd_final_temperature=0.05, embedgd_grad_distance='l2', embedgd_gumbel_noise_max=0.0, embedgd_lr_pattern='constant', embedgd_momentum=0.0, embedgd_noise_variance=1.0, embedgd_temperature_reduction_steps=50, embedgd_top_k=0, embedgd_top_p=1.0, end_idx=-1, eos=True, epsilon=0.2, epsilon_cooldown_steps='1', epsilon_decay_functions='linear', epsilon_warmup_steps='0', epsilons='-5', evaluation_metrics='fluency', expgd_do_sample=False, expgd_gumbel_noise_max=0.0, expgd_momentum=0.0, expgd_mw=1, expgd_top_k=0, expgd_top_p=1.0, final_bias=False, fp16_source='pytorch', gold_loss_epsilons='none', half_lr=False, init='target', jsonl_primary_key='prompt', jsonl_secondary_key='text', jsonl_tertiary_key=None, jsonl_tokenized='false', keyword_tau=2.0, keyword_topk=1, keywords='none', kweight=5.0, label_id='0:0', lambda_lr=1.0, lambda_update=50, length_diff='0', length_normalize=False, linear_scale='false', log_interval=25, loss='gpt2:classification_no_prefix', loss_type='dotplusplus', lossabbr='pyx:toxicity', lr=0.25, lr_decay=1.0, lr_update_size=0.01, match_with='reference', max_allowed_length=200, max_grad_norm=0.0, max_length=20, max_lr=0.45, max_output_length=20, max_prefix_length=50, metric='l2', min_epsilons='-5', min_lr=None, model='gpt2-large:models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best', model_dtype='fp32', model_types='AutoModelForCausalLM:RobertaCustomForSequenceClassification', num_examples=100, num_samples=25, only_mucoco='false', optim='embedgd', optim_steps=200, outfile='outputs/toxicity/edit-test/outputs.txt', output_style='jsonl', prefix_length=0, random_example='true', repetition_penalty=0.0, restarts=0, results_path=None, same_embeds=True, sampling_strategy='greedy', sampling_strategy_k='none', scale_loss='none', seed=42, selection_criterion='mrr_allsat', semantic_methods='bertscore', sgd_momentum=0.0, sgd_nesterov=False, show_all_outputs=False, show_warnings=False, st=False, start_decay_steps=1, start_idx=0, suffix_length=0, suffix_source=None, target_tokenize_different=False, target_type='embeds', time=False, tokenizer='gpt2-large:models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best', topic_target='none', topic_word_lists='none', use_context='false', warmup_end_lr=None, warmup_init_lr=None, warmup_steps=1, weight_decay=0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca1b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cli_main():\n",
    "    parser = options.get_parser()\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a962612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_output(tokens, eos_token_id, return_tensors=False, allow_first_eos=False, skip_special_tokens=[], prompt=None, sentence_complete=False, lossfn=None):\n",
    "    # print(tokens)\n",
    "    if sentence_complete:\n",
    "        tokens = sentence_completion(prompt, tokens, lossfn)\n",
    "    new_tokens = []\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok == eos_token_id and (not allow_first_eos or i > 0):\n",
    "            break\n",
    "        \n",
    "        if (tok not in skip_special_tokens):\n",
    "            new_tokens.append(tok)\n",
    "        \n",
    "    if return_tensors:\n",
    "        return torch.LongTensor([new_tokens])\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33e34bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=logging.ERROR,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "logger = logging.getLogger(\"mucoco\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "logger.info(args)\n",
    "\n",
    "if args.outfile is not None:\n",
    "    outf = open(args.outfile, \"w\")\n",
    "    outallsatf = open(args.outfile + \".allsat\", \"w\")\n",
    "    outf2 = open(args.outfile + \".hayley\", \"w\")\n",
    "\n",
    "# Fix seed\n",
    "if args.seed is not None:\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "use_cuda = torch.cuda.is_available() and not args.cpu\n",
    "logger.info(\n",
    "    \"loading model(s) from {} and tokenizer(s) from {}\".format(\n",
    "        args.model, args.tokenizer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af8540d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_paths ['gpt2-large', 'models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best']\n"
     ]
    }
   ],
   "source": [
    "name2tokenizer = {}\n",
    "name2model = {}\n",
    "name2config = {}\n",
    "loss2modelname = {}\n",
    "loss2tokenizer = {}\n",
    "embed_luts = []\n",
    "embed_scales = []\n",
    "\n",
    "betas = []\n",
    "model_paths = args.model.split(\":\")\n",
    "tokenizer_paths = args.tokenizer.split(\":\")\n",
    "print('tokenizer_paths', tokenizer_paths)\n",
    "cur_lr = args.lr\n",
    "args.jsonl_tokenized = args.jsonl_tokenized == \"true\"\n",
    "\n",
    "if args.model_types is not None:\n",
    "    model_types = args.model_types.split(\":\")\n",
    "else:\n",
    "    model_types = [AutoModel for _ in model_paths]\n",
    "\n",
    "losses = args.loss.split(\":\")\n",
    "if args.lossabbr is not None:\n",
    "    lossabbr = args.lossabbr.split(\":\")\n",
    "else:\n",
    "    lossabbr = [x for x in losses]\n",
    "\n",
    "if args.label_id is None or args.label_id == \"none\":\n",
    "    label_ids = [1 for _ in losses]\n",
    "else:\n",
    "    label_ids = [int(i) for i in args.label_id.split(\":\")]\n",
    "\n",
    "if args.keywords is None or args.keywords == \"none\":\n",
    "    keywords = [\"the\" for _ in losses]\n",
    "elif args.keywords in [\"_roc_\", \"_commongen_\", \"_commongenunique_\"]:\n",
    "    keywords = [\"\" for _ in losses] # will be different for each input\n",
    "else:\n",
    "    keywords = args.keywords.split(\":\")\n",
    "    if len(keywords) == 1:\n",
    "        keywords = [f\"_topic_:{args.keywords[0]}\" for _ in losses] #when keyword isn't used but topic is passed\n",
    "\n",
    "if \"allsat\" in args.selection_criterion: \n",
    "    # with this flag, the output which minimized the primary objective while satisfying all objectives is selected. In case all constraints are not satisfied (e.g when constraints are competing or optimization fails), this will predict the default output (Using an autoregressive decoding setup: beam search in this case)\n",
    "    betas = [1.0] + [0.0 for _ in range(len(losses)-1)]\n",
    "elif (args.selection_criterion == \"weighted_sum\" and args.betas is not None) or args.selection_criterion == \"last\":\n",
    "    # this setup will select the best outputs according to the weights betas for each of the losses (even though they are not satisfied)\n",
    "    betas = [float(beta) for beta in args.betas.split(\":\")]\n",
    "else:\n",
    "    raise ValueError(\"correct selection_criterion or betas needs to be specified\")\n",
    "\n",
    "assert len(betas) == len(losses) and len(losses) == len(model_paths) and len(model_paths) == len(model_types) and len(betas) == len(lossabbr)\n",
    "assert np.abs(sum(betas) - 1.0) < 1e-6, f\"sum of betas is {sum(betas)} != 1.0\"\n",
    "\n",
    "prev_vocab_size = None\n",
    "vocab_size = None\n",
    "primary_vocab_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea524e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embedgd'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "838530a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2-large',\n",
       " 'models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf4501cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name2model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce61e3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################################################################################\n",
    "#Load the models and tokenizers\n",
    "##################################################################################################################################################################\n",
    "for i, model_path in enumerate(model_paths):\n",
    "    if model_path not in name2model: #making sure we are not loading the model twice in case some constraints use the same model. \n",
    "        name2tokenizer[model_path] = AutoTokenizer.from_pretrained(tokenizer_paths[i], cache_dir=args.cache_dir,  use_fast=True)\n",
    "        name2config[model_path] = AutoConfig.from_pretrained(model_path, cache_dir=args.cache_dir)\n",
    "\n",
    "        if model_types[i] == \"sentence-transformer\":\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(SentenceTransformer(model_path))\n",
    "        elif \"Custom\" in model_types[i]:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(getattr(utils, model_types[i]).from_pretrained(model_path, config=name2config[model_path], cache_dir=args.cache_dir))\n",
    "        else:\n",
    "            name2model[model_path] = lossbuilder.ModelWrapper(getattr(transformers, model_types[i]).from_pretrained(model_path, config=name2config[model_path], cache_dir=args.cache_dir))\n",
    "\n",
    "        if not args.show_warnings:\n",
    "            # print(logging.root.manager.loggerDict)\n",
    "            # input()\n",
    "            set_global_logging_level(logging.ERROR, [name2model[model_path].__module__])\n",
    "            # logging.getLogger(name2model[model_path].__class__.__name__).setLevel(logging.ERROR) \n",
    "\n",
    "        name2model[model_path].eval()\n",
    "        embed_lut_ = name2model[model_path].get_input_embeddings()\n",
    "        if isinstance(embed_lut_, torch.nn.Sequential):\n",
    "            new_vocab_size = embed_lut_[0].num_embeddings\n",
    "        else:\n",
    "            new_vocab_size = embed_lut_.num_embeddings\n",
    "        if prev_vocab_size is None:\n",
    "            vocab_size=new_vocab_size\n",
    "        if new_vocab_size != prev_vocab_size and prev_vocab_size is not None:\n",
    "            if not args.allow_diff_vocab:\n",
    "                raise ValueError(f\"all models should have the same vocabulary {new_vocab_size} != {vocab_size}\")\n",
    "            else:\n",
    "                logger.warning(\"all models don't have the same vocabulary and we are still proceeding\")\n",
    "        prev_vocab_size = vocab_size\n",
    "\n",
    "    if args.target_tokenize_different: # for seq2seq models where target tokenizer is different than the source tokenizer\n",
    "        embed_luts.append(name2model[model_path].get_decoder().get_input_embeddings())\n",
    "    else:\n",
    "        input_embeds = name2model[model_path].get_input_embeddings()\n",
    "        if isinstance(input_embeds, torch.nn.Sequential):\n",
    "            input_embeds = input_embeds[0]\n",
    "        embed_luts.append(input_embeds)\n",
    "\n",
    "    if args.target_type == \"embeds\":\n",
    "        embed_luts[-1].requires_grad=False\n",
    "\n",
    "    if i == 0:\n",
    "        primary_vocab_size = vocab_size\n",
    "        primary_embed_dim = embed_luts[-1].embedding_dim\n",
    "\n",
    "    if getattr(name2model[model_path], \"get_decoder\", None) is None: #this is for MarianMT models which have a weird embedding_scale parameter\n",
    "        embed_scales.append(1.0)\n",
    "    else:\n",
    "        embed_scales.append(getattr(name2model[model_path].get_decoder(), \"embed_scale\", 1.0))\n",
    "\n",
    "if use_cuda:\n",
    "    for name, model in name2model.items():\n",
    "        model.cuda()\n",
    "    logger.info(\"model(s) moved to GPU\")\n",
    "##################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e651031f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpt2-large': <mucoco.losses.model_wrapper.ModelWrapper object at 0x7fb069d03340>, 'models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best': <mucoco.losses.model_wrapper.ModelWrapper object at 0x7fafdffaef40>}\n",
      "{'gpt2-large': GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2-large\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      ", 'models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best': RobertaConfig {\n",
      "  \"_name_or_path\": \"models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"new_n_embd\": 1280,\n",
      "  \"new_vocab_size\": 50257,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "}\n",
      "{'gpt2-large': GPT2TokenizerFast(name_or_path='gpt2-large', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True), 'models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best': GPT2TokenizerFast(name_or_path='models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best', vocab_size=50257, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)}\n"
     ]
    }
   ],
   "source": [
    "print(name2model)\n",
    "print(name2config)\n",
    "print(name2tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4a89523",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################################################################\n",
    "#first loss is the primary loss, others are constraints\n",
    "##################################################################################################################################################################\n",
    "lossfns = []\n",
    "for i, loss in enumerate(losses):\n",
    "    lossfns.append(lossbuilder.build_loss(loss, name2model[model_paths[i]], name2tokenizer[model_paths[i]], args))\n",
    "    loss2modelname[loss] = model_paths[i]\n",
    "    loss2tokenizer[loss] = name2tokenizer[model_paths[i]]\n",
    "primary_tokenizer = loss2tokenizer[losses[0]]\n",
    "\n",
    "logger.info(\"tokenizer(s), model(s) and loss function(s) loaded\")\n",
    "\n",
    "if args.model_dtype == \"fp16\": #while this is supported it doesn't work that well yet. Not recommended\n",
    "    for name, model in name2model.items():\n",
    "        model.half()\n",
    "    logger.info(\"changed everything to fp16\")\n",
    "\n",
    "#constraint thresholds. In the paper, we recommend to start with a high threshold value which is usually satisfied by default or easily satisfied and then decrease it gradually, otherwise weird adversarial solutions come up. This code supports different kinds of schedules for decreasing this threshold (usually just step or linear suffices). If no schedule is specified, it just remains the same as the original. \n",
    "if args.epsilons is not None and args.epsilons != \"none\":\n",
    "    epsilons = [float(eps) for eps in args.epsilons.split(\":\")]\n",
    "    if args.min_epsilons is not None:\n",
    "        min_epsilons = [float(eps) for eps in args.min_epsilons.split(\":\")]\n",
    "        epsilon_warmup_steps = [int(steps) for steps in args.epsilon_warmup_steps.split(\":\")]\n",
    "        epsilon_cooldown_steps = [int(steps) for steps in args.epsilon_cooldown_steps.split(\":\")]\n",
    "        epsilon_decay_functions = [f for f in args.epsilon_decay_functions.split(\":\")]\n",
    "    else:\n",
    "        min_epsilons = [float(eps) for eps in args.epsilons.split(\":\")]\n",
    "        epsilon_warmup_steps = [1 for eps in min_epsilons]\n",
    "        epsilon_cooldown_steps = [2 for eps in min_epsilons]\n",
    "        epsilon_decay_functions = [\"none\" for eps in min_epsilons]\n",
    "    min_epsilons = [eps + getattr(lossfns[i+1], \"epsilon_additive\", 0)  for i, eps in enumerate(min_epsilons)]\n",
    "else:\n",
    "    epsilons = []\n",
    "    min_epsilons = []\n",
    "    decay_function = []\n",
    "    epsilon_warmup_steps = []\n",
    "    epsilon_cooldown_steps = []\n",
    "##################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9e4f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################################################################\n",
    "# assert args.data is not None or args.additional_data is not None, \"no data path has been provided\"\n",
    "source_dataset = None\n",
    "target_dataset = None\n",
    "additional_dataset = None\n",
    "args.use_context = args.use_context == \"true\"\n",
    "if args.data is not None:\n",
    "    data_paths = args.data.split(\":\")\n",
    "    if len(data_paths) == 1:\n",
    "        source_data = data_paths[0]\n",
    "        target_data = data_paths[0] #not used\n",
    "        context_data = data_paths[0] # not used\n",
    "        # args.use_context = False\n",
    "    elif len(data_paths) == 2:\n",
    "        source_data = data_paths[0]\n",
    "        target_data = data_paths[1] # useful for debugging\n",
    "        context_data = data_paths[1] #not used here\n",
    "        # args.use_context = False\n",
    "    else:\n",
    "        source_data = data_paths[0]\n",
    "        target_data = data_paths[1] # useful for debugging\n",
    "        context_data = data_paths[2] # tsv file\n",
    "\n",
    "    additional_data = args.additional_data\n",
    "    if additional_data is None or additional_data == \"none\":\n",
    "        additional_data = source_data # additional data was used in STRAP (Krishna et al 2020) when x is paraphrased to z, then the model is used to generate y in the target style. If there's no additional_data, it defaults to the source text\n",
    "elif args.additional_data is not None and additional_data != \"none\":\n",
    "    source_data = args.additional_data\n",
    "    target_data = args.additional_data\n",
    "    additional_data = args.additional_data\n",
    "else:\n",
    "    source_dataset = sys.stdin\n",
    "    start_idx = 0\n",
    "    end_idx = 1000000 # a very high number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "316af2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data//control-prompts/nontoxic_prompts-1.jsonl\n",
      "data//control-prompts/nontoxic_prompts-1.jsonl\n",
      "data//control-prompts/nontoxic_prompts-1.jsonl\n",
      "data//control-prompts/nontoxic_prompts-1.jsonl\n",
      "data//control-prompts/nontoxic_prompts-1.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(args.data); print(source_data); print(target_data); print(context_data); print(additional_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa897136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "if source_dataset is None:\n",
    "    logger.info(\"Loading the dataset ...\")\n",
    "    if args.datastyle == \"text\":\n",
    "        source_dataset = [l.strip() for l in open(source_data)]\n",
    "        target_dataset = [l.strip() for l in open(target_data)]\n",
    "        context_dataset = []\n",
    "        import csv\n",
    "        with open(context_data) as csvfile: #there can be multiple contexts, for example for paraphrasing, so we allow for a list of contexts for every input\n",
    "            reader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "            for row in reader:\n",
    "                context_dataset.append(row)\n",
    "        additional_dataset = [l.strip() for l in open(additional_data)]\n",
    "    elif args.datastyle == \"jsonl\": #for some prompts datasets\n",
    "        source_dataset = [json.loads(l)[args.jsonl_primary_key] for l in open(source_data)]\n",
    "        target_dataset = [json.loads(l)[args.jsonl_primary_key] for l in open(target_data)]\n",
    "        additional_dataset = [json.loads(l)[args.jsonl_primary_key] for l in open(additional_data)]\n",
    "        if args.jsonl_secondary_key is not None and args.jsonl_secondary_key != \"none\":\n",
    "            source_dataset = [x[args.jsonl_secondary_key] for x in source_dataset]\n",
    "            target_dataset = [x[args.jsonl_secondary_key] for x in target_dataset]\n",
    "            additional_dataset = [x[args.jsonl_secondary_key] for x in additional_dataset]\n",
    "\n",
    "        context_dataset = [None] * len(source_dataset)\n",
    "        if args.use_context:\n",
    "            context_dataset = [json.loads(l)[args.jsonl_primary_key] for l in open(context_data)]\n",
    "            if args.jsonl_secondary_key is not None and args.jsonl_secondary_key != \"none\":\n",
    "                context_dataset = [x[args.jsonl_secondary_key] for x in context_dataset]\n",
    "    elif args.datastyle == \"single-jsonl\": #one jsonl file has all the information\n",
    "        source_dataset = [json.loads(l)[args.jsonl_primary_key] for l in open(source_data)]\n",
    "        target_dataset = [json.loads(l)[args.jsonl_secondary_key] for l in open(target_data)]\n",
    "        additional_dataset = [json.loads(l)[args.jsonl_secondary_key] for l in open(additional_data)]\n",
    "\n",
    "        context_dataset = [None] * len(source_dataset)\n",
    "        if args.use_context:\n",
    "            context_dataset = [[json.loads(l)[args.jsonl_secondary_key]] for l in open(context_data)] #meaningful\n",
    "    start_idx = args.start_idx\n",
    "    end_idx = (len(source_dataset) + args.end_idx) % len(source_dataset) # also works with negative end_idx\n",
    "\n",
    "    logger.info(\"Data loaded\")\n",
    "##################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "664e38fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DNA to be used to try to track down perpetrator']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cad3e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DNA to be used to try to track down perpetrator']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6fbec12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DNA to be used to try to track down perpetrator']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f51f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e1e32bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 True\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################################################################################\n",
    "# Doing prediction + constrained decoding\n",
    "##################################################################################################################################################################\n",
    "source_batch, target_batch, additional_batch, for_predicted_source_batch, predicted_batch, context_batch = [], [], [], [], [], []\n",
    "batch_size = args.batch_size # higher than 1 batch size does not work at the moment. It won't fit in a single GPU anyway \n",
    "\n",
    "device = \"cuda\" if use_cuda else \"cpu\"\n",
    "c = 0\n",
    "\n",
    "losslists = [[] for _ in range(len(losses))]\n",
    "predictedlosslists = [[] for _ in range(len(losses))]\n",
    "source_primarylosslist = [] \n",
    "# allparetosets = []\n",
    "all_stepcounts = []\n",
    "avg_time = 0\n",
    "\n",
    "#data loading is very simple and probably can be sped up\n",
    "\n",
    "if args.gold_loss_epsilons is not None and args.gold_loss_epsilons != \"none\":\n",
    "    args.gold_loss_epsilons = args.gold_loss_epsilons.lower().split(\":\")\n",
    "    assert len(args.gold_loss_epsilons) == len(losses)-1\n",
    "else:\n",
    "    args.gold_loss_epsilons = [\"false\" for _ in range(len(losses)-1)]\n",
    "\n",
    "# for source_text, target_text, additional_text in zip(source_dataset, target_dataset, additional_dataset):\n",
    "example_p = 1.0\n",
    "args.random_example = args.random_example == \"true\"\n",
    "if args.num_examples > 0 and target_dataset is not None:\n",
    "    example_p = args.num_examples*1.0/len(source_dataset)\n",
    "print(example_p, args.random_example)\n",
    "print(start_idx, end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fd0c1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd6b811a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DNA to be used to try to track down perpetrator']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "638bcfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id = 0; source_text = source_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c31731c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 doing it! do_this_example\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################################################################################\n",
    "##################################################################################################################################################################\n",
    "##################################################################################################################################################################\n",
    "# start looping over prompts\n",
    "##################################################################################################################################################################\n",
    "##################################################################################################################################################################\n",
    "##################################################################################################################################################################\n",
    "#for text_id, source_text in enumerate(source_dataset):\n",
    "\n",
    "# ITERATING OVER PROMPTS. DO FOLLOWING FOR EACH OF PROMPT.\n",
    "if text_id < start_idx or text_id > end_idx:\n",
    "    pass\n",
    "    #continue\n",
    "\n",
    "if args.num_examples > 0 and c > 0 and c == args.num_examples: #stop after processing num_examples if it is set \n",
    "    print(f\"done {c}\")\n",
    "    #break\n",
    "\n",
    "do_this_example = np.random.rand() <= example_p\n",
    "if not do_this_example:\n",
    "    pass\n",
    "    #continue\n",
    "\n",
    "print(text_id, \"doing it! do_this_example\")\n",
    "\n",
    "c += 1\n",
    "\n",
    "new_kweight = args.kweight\n",
    "if target_dataset is not None:\n",
    "    target_text = target_dataset[text_id]\n",
    "    additional_text = additional_dataset[text_id]\n",
    "    context_texts = context_dataset[text_id]\n",
    "    # print(context_texts)\n",
    "    # input()\n",
    "else:\n",
    "    args.jsonl_tokenized = False\n",
    "    items = source_text.split(\"::\")\n",
    "    source_text = items[0].rstrip()\n",
    "    target_text = items[1].rstrip()\n",
    "    if target_text == \"-\":\n",
    "        args.init = \"zeros\"\n",
    "    elif target_text == \"--\":\n",
    "        args.init = \"target\"\n",
    "    else:\n",
    "        args.init = \"targettarget\"\n",
    "    additional_text = items[2]\n",
    "\n",
    "    if len(items) > 3:\n",
    "        args.max_output_length = int(items[3])\n",
    "        args.max_length = int(items[3])\n",
    "    if len(items) > 4:\n",
    "        args.use_context = True\n",
    "        context_texts = [items[4]].rstrip()                               \n",
    "    else:\n",
    "        args.use_context = False\n",
    "        context_texts = []\n",
    "\n",
    "    if len(items) > 5:\n",
    "        new_kweight = float(items[5])\n",
    "    # if len(items) > 4:\n",
    "    #     target_text = items[4].rstrip()\n",
    "\n",
    "    print(args.use_context, context_texts)\n",
    "\n",
    "if args.keywords == \"_roc_\":\n",
    "    keywords = [\"none\"] + additional_text.split(\", \")\n",
    "    # input(keywords)\n",
    "if args.keywords == \"_rocunique_\":\n",
    "    keywords = [\"none\"] + additional_text.split(\", \") + + ['none']\n",
    "    # input(keywords)\n",
    "elif args.keywords == \"_commongen_\":\n",
    "    print(additional_text)\n",
    "    keywords = [\"none\"] + json.loads(additional_text)['concept_set'].split(\"#\")\n",
    "elif args.keywords == \"_commongenunique_\":\n",
    "    keywords = [\"none\"] + json.loads(additional_text)['concept_set'].split(\"#\") + ['none']\n",
    "    # input(keywords)\n",
    "\n",
    "\n",
    "# early_skip=\"n\"\n",
    "# if args.debug:\n",
    "#     early_skip = input(f\"skip this example? {source_text} [yes(y)/maybe(m)/no(n)]\")\n",
    "#     if early_skip == \"y\":\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b552ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.keywords none\n",
      "keywords ['the', 'the']\n",
      "target_text DNA to be used to try to track down perpetrator\n",
      "args.init target\n"
     ]
    }
   ],
   "source": [
    "print(\"args.keywords\", args.keywords); print(\"keywords\", keywords); print(\"target_text\", target_text); print(\"args.init\", args.init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34c88f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################################################################\n",
    "# encode source and context text\n",
    "##################################################################################################################################################################\n",
    "if not args.jsonl_tokenized:\n",
    "    if source_text == \"\":\n",
    "        source_text = primary_tokenizer.bos_token\n",
    "    source_indices = primary_tokenizer.encode(source_text, return_tensors=\"pt\").to(device)\n",
    "    source_indices_write = source_indices[0].tolist()\n",
    "    # if source_indices\n",
    "    additional_indices = primary_tokenizer.encode(additional_text, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "\n",
    "    eos_token_id = primary_tokenizer.eos_token_id\n",
    "    bos_token_id = primary_tokenizer.bos_token_id\n",
    "    context_indices = None\n",
    "    if args.target_tokenize_different:\n",
    "        with primary_tokenizer.as_target_tokenizer():\n",
    "            eos_token_id=primary_tokenizer.eos_token_id\n",
    "            bos_token_id = primary_tokenizer.bos_token_id\n",
    "            if args.use_context:\n",
    "                context_indices = primary_tokenizer.encode(context_texts[0], return_tensors=\"pt\", add_special_tokens=False).to(device).unsqueeze(1)\n",
    "    elif args.use_context:\n",
    "        context_indices = primary_tokenizer.encode(context_texts[0], return_tensors=\"pt\", add_special_tokens=False).to(device).unsqueeze(1)\n",
    "\n",
    "    if not args.target_tokenize_different and \"Seq2SeqLM\" in model_paths[0]:\n",
    "        logger.warning(\"you are using a seq2seq model for your primary loss but not tokenizing the target sentences with a different target tokenizer.\")\n",
    "\n",
    "    #for_predicted_source_indices are used to compute the primary loss wrt source as target. Useful for debugging style transfer models. \n",
    "    if args.target_tokenize_different:\n",
    "        with primary_tokenizer.as_target_tokenizer():\n",
    "            for_predicted_source_indices = primary_tokenizer.encode(source_text, return_tensors=\"pt\").to(device)\n",
    "            target_indices = primary_tokenizer.encode(target_text, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "    else:\n",
    "        for_predicted_source_indices = source_indices\n",
    "        target_indices = primary_tokenizer.encode(target_text, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "\n",
    "else:\n",
    "    source_indices_write = source_text # to write to file\n",
    "    source_indices = source_text\n",
    "    target_indices = target_text\n",
    "    additional_indices = additional_text\n",
    "    context_indices = context_texts\n",
    "    if len(source_indices) == 0:\n",
    "        source_indices.append(primary_tokenizer.bos_token_id)\n",
    "\n",
    "    source_indices = torch.LongTensor([source_indices]).to(device)\n",
    "    additional_indices = torch.LongTensor([additional_indices]).to(device)\n",
    "\n",
    "    #unused\n",
    "    context_indices = None\n",
    "    if args.use_context:\n",
    "        context_indices = torch.LongTensor([context_indices]).to(device).to(device).unsqueeze(1)\n",
    "    #end unused\n",
    "\n",
    "    #for_predicted_source_indices are used to compute the primary loss wrt source as target. Useful for debugging style transfer models. \n",
    "    for_predicted_source_indices = source_indices\n",
    "    target_indices = torch.LongTensor([target_indices]).to(device)\n",
    "\n",
    "    bos_token_id = primary_tokenizer.bos_token_id\n",
    "    eos_token_id = primary_tokenizer.eos_token_id\n",
    "    if args.target_tokenize_different:\n",
    "        with primary_tokenizer.as_target_tokenizer():\n",
    "            bos_token_id = primary_tokenizer.bos_token_id\n",
    "            eos_token_id = primary_tokenizer.eos_token_id\n",
    "\n",
    "    source_text = primary_tokenizer.decode(source_indices[0].tolist())\n",
    "\n",
    "source_batch.append(source_indices)\n",
    "target_batch.append(target_indices)\n",
    "for_predicted_source_batch.append(for_predicted_source_indices)\n",
    "additional_batch.append(additional_indices)\n",
    "context_batch.append(context_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32b3c8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_batch [tensor([[28886,   284,   307,   973,   284,  1949,   284,  2610,   866, 33599]],\n",
      "       device='cuda:0')]\n",
      "target_batch [tensor([[28886,   284,   307,   973,   284,  1949,   284,  2610,   866, 33599]],\n",
      "       device='cuda:0')]\n",
      "for_predicted_source_batch [tensor([[28886,   284,   307,   973,   284,  1949,   284,  2610,   866, 33599]],\n",
      "       device='cuda:0')]\n",
      "additional_batch [tensor([[28886,   284,   307,   973,   284,  1949,   284,  2610,   866, 33599]],\n",
      "       device='cuda:0')]\n",
      "context_batch [None]\n"
     ]
    }
   ],
   "source": [
    "print(\"source_batch\", source_batch); print(\"target_batch\", target_batch); print(\"for_predicted_source_batch\", for_predicted_source_batch); print(\"additional_batch\", additional_batch); print(\"context_batch\", context_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "755d52ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5529399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(source_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "747d4d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(args.use_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "976e34d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(source_batch) == batch_size: #this is just one for now, greater than 1 batch size will not work\n",
    "\n",
    "source_batch = torch.cat(source_batch, dim=0).to(device)\n",
    "target_batch = torch.cat(target_batch, dim=0).to(device)\n",
    "additional_batch = torch.cat(additional_batch, dim=0).to(device)\n",
    "for_predicted_source_batch = torch.cat(for_predicted_source_batch, dim=0).to(device)  \n",
    "\n",
    "# print(\"what\", args.use_context)\n",
    "if args.use_context:\n",
    "    context_batch = torch.cat(context_batch, dim=0).to(device)\n",
    "    print(context_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d5522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "302e4fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/share/virtualenvs/mucoco-29Dmb6mE/lib/python3.8/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for sexual crimes in this nation, making it much easier to identify the semen for what it is:\n",
      "'s real brain – would have to be developed for Australia or, in the United States, under FBI\n",
      " regions, there has been some doubt over whether the results can be used as evidence of a link to\n",
      " Ian Murdock who was on the run and unable to be found since 1983.\n",
      "\n",
      "But now\n",
      ".\n",
      "\n",
      "Disabilities activists also blamed the nature of the crime, which appeared random, adding that\n",
      ", and you've probably already heard that there's still more of a chance that Smith could have killed\n",
      " Janice Eddington. The DNA is excluded because it could not be proven that Eddington\n",
      "(s) of the crime.\n",
      "\n",
      "Things we think are possibly due to the presence of HIV\n",
      " or victims. It's all part of the unwinnable war on drugs and illegal immigration.\n",
      "\n",
      ".\"\n",
      "\n",
      "Vandalous groom-rapist Raffey Mack's DNA was not supplied to\n",
      " of murder Bucky Lessard 'But if the case is clear-cut, have you even been\n",
      " of the Jody case.\n",
      "\n",
      "Long-term community outreach\n",
      "\n",
      "Several community-based groups\n",
      "'s DNA. She was cleared on Monday after the DNA test result was confirmed in laboratory reports. Dr\n",
      ", admits he is a firm believer in the official version of the attack.\n",
      "\n",
      "For over a\n",
      ". Neighbors who knew Gaetje were staying at the house at the time of the attack.\n",
      " or to retrieve child.\"\n",
      "\n",
      "While the Appealing Inclusion Criteria for DNA Monitoring is an\n",
      " of the assault and help solve this case.\n",
      "\n",
      "\"I have committed to take this fight into\n",
      ".But what if blood from the crime scene is not on one of the believed victims? This is\n",
      ", authorities say.\n",
      "\n",
      "Graham was only 16 when he shot 39-year-old Gregory Reid\n",
      ". The DNA profiling found no matches with the newly released DNA at large. Search warrant was issued.\n",
      ".\n",
      "\n",
      "The AP reported the charges in full on Tuesday.\n",
      "\n",
      "___\n",
      "\n",
      "4:\n",
      "\n",
      "\n",
      "A list of 240 people in NSW has been released to help identify a man who mowed\n",
      ". On the other hand, we expect that keeping records of the medium of either the. translation or\n",
      ", but for now, the fact that the decay rate of the DNA is higher than previous estimates has\n",
      " -- until instead it was somehow suspected to be showing up as a lover of her killer.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################################################################################\n",
    "# generating AR samples\n",
    "##################################################################################################################################################################\n",
    "predicted_batches = [] #each sample x restart becomes a tensor\n",
    "for batchidx in range(source_batch.size(0)): #batch size is 1\n",
    "    with torch.no_grad():\n",
    "        starttime = time.time()\n",
    "        AR_predicted_all =\\\n",
    "            lossfns[0].generate(\n",
    "                input_ids=source_batch[batchidx].unsqueeze(0),\n",
    "                additional_ids=additional_batch[batchidx].unsqueeze(0),\n",
    "                num_return_sequences=(args.restarts + 1)*args.num_samples) # 25 for nontoxic\n",
    "        #some bug about length\n",
    "\n",
    "        # AR_predicted_indices_all = []\n",
    "        AR_prediction_all = []\n",
    "        # clean output for predicted token ids\n",
    "        for sample_idx in range(len(AR_predicted_all)):\n",
    "            AR_predicted_indices =\\\n",
    "                clean_output(AR_predicted_all[sample_idx].tolist(), # remove eos token, etc.\n",
    "                    eos_token_id=eos_token_id,\n",
    "                    return_tensors=True, allow_first_eos=losses[0] == \"bart\",\n",
    "                    skip_special_tokens=[bos_token_id, eos_token_id])\n",
    "            # AR_predicted_indices_all.append(AR_predicted_indices)\n",
    "\n",
    "            if args.target_tokenize_different:\n",
    "                with primary_tokenizer.as_target_tokenizer():\n",
    "                    AR_prediction = primary_tokenizer.decode(AR_predicted_indices[0].tolist())\n",
    "            else:\n",
    "                AR_prediction = primary_tokenizer.decode(AR_predicted_indices[0].tolist())\n",
    "            AR_prediction_all.append(AR_prediction)\n",
    "            print(AR_prediction)\n",
    "\n",
    "            # predicted_batch.append(AR_predicted_indices)\n",
    "            predicted_batches.append(AR_predicted_indices.to(device))\n",
    "        if args.time:\n",
    "            print(time.time()-starttime)\n",
    "##################################################################################################################################################################\n",
    "# 25 initial outputs per prompt\n",
    "# hayley_result={\"prompt\": source_text}\n",
    "##################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c12f434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.restarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28617dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embeds'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.target_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4880495a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb88903d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.restarts 0\n"
     ]
    }
   ],
   "source": [
    "broken_skip = False\n",
    "print(\"args.restarts\", args.restarts)\n",
    "##################################################################################################################################################################\n",
    "# for each prompt loop over 25 samples\n",
    "##################################################################################################################################################################\n",
    "# for sample_idx in range(10): # just save 2 samples per prompt\n",
    "# for sample_idx in range(args.num_samples): # 25 for nontoxic\n",
    "#     for restart_idx in range(args.restarts + 1): # 0 for nontoxic. restart the optimization if the constraints are not satisfied\n",
    "sample_idx= 0\n",
    "restart_idx= 0\n",
    "predicted_batch = predicted_batches[sample_idx * (args.restarts + 1) + restart_idx]\n",
    "AR_prediction = AR_prediction_all[sample_idx * (args.restarts + 1) + restart_idx]\n",
    "hayley_result={\"prompt\": source_text}\n",
    "hayley_result.update({\"sample_id\": sample_idx, \"original_text\": AR_prediction})\n",
    "\n",
    "##TODO: in case of always_mucoco=false and num_restarts > 0, comb through the restarts and skip if constraints are satisfied\n",
    "\n",
    "skip=False\n",
    "predicted_allsat=False\n",
    "lengthwise_best_prediction = [None] * batch_size\n",
    "\n",
    "if args.debug:\n",
    "    print(\"AR output:\", source_text, additional_text, predicted_batch)\n",
    "\n",
    "# losses of the autoregressive output: we should perform atleast as well as this. If we don't, we predict this output\n",
    "# Also, if the autoregressive output already satisfies the constraints, we skip mucoco unless, args.always_mucoco is true\n",
    "predicted_labels = {}\n",
    "total_predicted_loss = 0.0\n",
    "predicted_allsat=True\n",
    "predictedlosses = []\n",
    "\n",
    "for lossid in range(len(losses)):\n",
    "    lossname = losses[lossid]\n",
    "\n",
    "    predicted_loss, predicted_lo =\\\n",
    "        lossfns[lossid].compute_gold_loss(\n",
    "            (source_batch, target_batch), \n",
    "            additional_batch=additional_batch, \n",
    "            context_batch=context_batch,\n",
    "            use_context=args.use_context,\n",
    "            label_id=label_ids[lossid],\n",
    "            keyword=keywords[lossid],\n",
    "            kweight=new_kweight)\n",
    "\n",
    "    predictedlosses.append(predicted_loss.data.cpu())\n",
    "    predicted_loss = predicted_loss.sum().item()\n",
    "    hayley_result.update({f\"original_loss{lossid}\": predicted_loss})\n",
    "    total_predicted_loss += betas[lossid] * predicted_loss\n",
    "\n",
    "    if lossid > 0:\n",
    "        predicted_allsat = predicted_allsat and (predicted_loss <= min_epsilons[lossid-1])\n",
    "\n",
    "    if \"label_prediction\" in predicted_lo:\n",
    "        predicted_labels[lossid] = predicted_lo['label_prediction']\n",
    "    else:\n",
    "        predicted_labels[lossid] = \"NA\"\n",
    "\n",
    "    if lossid > 0 and args.gold_loss_epsilons[lossid-1] == \"true\": #use the predicted loss as the threshold, mucoco has to beat it then\n",
    "        min_epsilons[lossid - 1] = predicted_loss + getattr(lossfns[lossid], \"epsilon_additive\", 0)\n",
    "        epsilons[lossid - 1] = predicted_loss + getattr(lossfns[lossid], \"epsilon_additive\", 0) ##TODO check \n",
    "\n",
    "predictedlosslists.append(predictedlosses)\n",
    "\n",
    "\n",
    "if args.only_mucoco == \"false\":\n",
    "    lengthwise_best_prediction = [(AR_prediction, total_predicted_loss, predicted_allsat, predicted_batch[0].tolist(), -1)]\n",
    "skip = predicted_allsat\n",
    "\n",
    "definite_skip = False\n",
    "ask_skip = \"\"\n",
    "if args.debug and early_skip==\"m\": \n",
    "    print(f\"new example: {source_text}\\nautoregressive output: {AR_prediction}\")\n",
    "    for lossid in range(len(losses)):\n",
    "        print(f\"{lossabbr[lossid]} for desired label_id({label_ids[lossid]}): {predictedlosslists[-1][lossid]}; predicted label: {predicted_labels[lossid]}\")\n",
    "    if predicted_allsat:\n",
    "        print(f\"autoregressive output already satisfies the constraints\")\n",
    "    ask_skip = input(f\"skip this example? [y/n]\")\n",
    "    definite_skip = ask_skip == \"y\"\n",
    "\n",
    "elif skip and predicted_allsat and (args.always_mucoco == \"false\"):\n",
    "    definite_skip = True\n",
    "\n",
    "if args.debug:\n",
    "    print('definite_skip', definite_skip, skip, predicted_allsat, args.always_mucoco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cea5a78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definite_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ededd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e63c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e40656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3323925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         if not definite_skip:\n",
    "    # print(args.max_length)\n",
    "if (args.max_length is None or args.max_length == -1) and args.init not in [\"source\", \"target\"]: \n",
    "    #since we don't know the about length, we search in a (-length_diff, length_diff) window and predict the best performing one.\n",
    "    predicted_length = predicted_batch.size(1)\n",
    "    length_range = [predicted_length + int(diff) for diff in args.length_diff.split(\":\")]\n",
    "    length_range = [x for x in length_range if x <= args.max_allowed_length and x >= 1]\n",
    "    if len(length_range) == 0:\n",
    "        length_range = [args.max_allowed_length]\n",
    "    length_range = sorted(list(set(length_range)))\n",
    "elif args.init == \"targettarget\":\n",
    "    length_range = [target_batch.size(1)]\n",
    "elif args.init == \"target\":\n",
    "    length_range = [predicted_batch.size(1)]\n",
    "elif args.init == \"source\":\n",
    "    length_range = [source.size(1)]\n",
    "else: \n",
    "    #another way to use this approach is train models which also compute loss on <pad> token and then predict the entire sentence including pad, it has shown to work in some of our experiments\n",
    "    length_range = [args.max_length]           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d319eba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68365c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30e7b6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting a sentence length:  20\n"
     ]
    }
   ],
   "source": [
    "# for sent_length_ in length_range:\n",
    "sent_length_ = 20\n",
    "# prefix_length is used to indicate if instead of predicting the entire sentence via optimization, we want to fix a prefix (of specified length) and predict the remaining suffix. We use part of the beam search prediction as the prefix. \n",
    "if args.prefix_length > 0:\n",
    "    sent_length = sent_length_ - args.prefix_length\n",
    "    target_prefix = predicted_batch[:, :args.prefix_length]\n",
    "else:\n",
    "    sent_length = sent_length_\n",
    "    target_prefix = torch.empty((source_indices.size(0), 0)).long().to(device)\n",
    "\n",
    "if sent_length <= 0:\n",
    "    pass #continue\n",
    "if sent_length > args.max_allowed_length:\n",
    "    #max_allowed_length is just to make sure things don't go out of memory,\n",
    "    old_l = sent_length\n",
    "    sent_length = args.max_allowed_length\n",
    "    print(f\"changed output length to {sent_length} from {old_l} to avoid GPU overflow. This is a temporary solution\")\n",
    "else:\n",
    "    print(\"predicting a sentence length: \", sent_length)\n",
    "\n",
    "#                 if args.target_type == \"simplex\": # use V sized real vector for each token and apply softmax before output\n",
    "#                     outputs = TargetSimplex(\n",
    "#                         vocabsize=primary_vocab_size,\n",
    "#                         sent_length=sent_length,\n",
    "#                         batch_size=batch_size,\n",
    "#                         device=device,\n",
    "#                         temperature=args.decode_temperature,\n",
    "#                         st=args.st,\n",
    "#                         init_value=source_batch[:,1:-1] if args.init == \"source\" else None,\n",
    "#                         random_init=args.init == \"random\",\n",
    "#                         do_sample=args.expgd_do_sample,\n",
    "#                         top_p=args.expgd_top_p,\n",
    "#                         top_k=args.expgd_top_k,\n",
    "#                         embed_scales=embed_scales\n",
    "#                     )\n",
    "#                 elif args.target_type == \"probs\": # use V sized vector which sums to one for each token and apply softmax before output\n",
    "#                     init_value = None\n",
    "#                     break_after=False\n",
    "#                     if args.init == \"source\": #initialize the target with the source\n",
    "#                         init_value = source_batch\n",
    "#                         target_prefix = torch.empty((source_indices.size(0), 0)).long().to(device)\n",
    "#                         sent_length = init_value.size(1)\n",
    "#                         break_after=True\n",
    "#                         # print(source_batch, init_value, sent_length, init_value)\n",
    "#                     elif args.init == \"target\": #initialize the target with the autoregressive output\n",
    "#                         init_value = target_batch\n",
    "#                         target_prefix = torch.empty((source_indices.size(0), 0)).long().to(device)\n",
    "#                         sent_length = init_value.size(1)\n",
    "#                         break_after=True\n",
    "#                         # print(source_batch, init_value)\n",
    "\n",
    "#                     outputs = TargetProbability(\n",
    "#                         vocabsize=primary_vocab_size,\n",
    "#                         sent_length=sent_length,\n",
    "#                         batch_size=batch_size,\n",
    "#                         device=device,\n",
    "#                         st=args.st,\n",
    "#                         init_value=init_value,\n",
    "#                         random_init=args.init == \"random\",\n",
    "#                         do_sample=args.expgd_do_sample,\n",
    "#                         top_p=args.expgd_top_p,\n",
    "#                         top_k=args.expgd_top_k,\n",
    "#                         embed_scales=embed_scales,\n",
    "#                         max_steps=args.optim_steps\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316d77b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f30f2fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################################################################################\n",
    "# initialize embedding\n",
    "##################################################################################################################################################################\n",
    "#                 elif args.target_type == \"embeds\":\n",
    "init_value = None\n",
    "break_after=False\n",
    "if args.init == \"source\": #initialize the target with the source\n",
    "    init_value = embed_luts[0](source_batch)\n",
    "    target_prefix = torch.empty((source_indices.size(0), 0)).long().to(device)\n",
    "    sent_length = init_value.size(1)\n",
    "    break_after=True\n",
    "    # print(source_batch, init_value, sent_length, init_value)\n",
    "elif args.init == \"targettarget\": #initialize the target with given target\n",
    "    init_value = embed_luts[0](target_batch)\n",
    "    target_prefix = torch.empty((source_indices.size(0), 0)).long().to(device)\n",
    "    sent_length = init_value.size(1)\n",
    "    break_after=True \n",
    "    print(predicted_batch.size())   \n",
    "    print(sent_length)\n",
    "elif args.init == \"target\": #initialize the target with the autoregressive output\n",
    "    ##################################################################################################################################################################\n",
    "    init_value = embed_luts[0](predicted_batch)\n",
    "    ##################################################################################################################################################################\n",
    "    target_prefix = torch.empty((source_indices.size(0), 0)).long().to(device)\n",
    "    sent_length = init_value.size(1)\n",
    "    break_after=True \n",
    "    print(predicted_batch.size())   \n",
    "    print(sent_length)\n",
    "elif args.init == \"random_vocab\":\n",
    "    random_indices = torch.multinomial(torch.ones(primary_vocab_size,)/primary_vocab_size, num_samples=batch_size*sent_length, replacement=True).view(batch_size, sent_length).to(device)\n",
    "    init_value = embed_luts[0](random_indices)\n",
    "elif args.init == \"embedgd-zeros\":\n",
    "    if args.target_tokenize_different:\n",
    "        with primary_tokenizer.as_target_tokenizer():\n",
    "            indices = torch.empty((batch_size, sent_length)).long().fill_(primary_tokenizer.eos_token_id).to(device)\n",
    "    else:\n",
    "        indices = torch.empty((batch_size, sent_length)).long().fill_(primary_tokenizer.eos_token_id).to(device)\n",
    "    # print(primary_tokenizer.decode(indices[0]))\n",
    "    init_value = embed_luts[0](indices)\n",
    "elif args.init == \"zeros\":\n",
    "    indices = torch.zeros((batch_size, sent_length)).long().to(device)\n",
    "    init_value = embed_luts[0](indices)\n",
    "\n",
    "\n",
    "final_bias = None\n",
    "if args.final_bias:\n",
    "    final_bias = lossfns[0].model.final_logits_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18848a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  329,  3206,  6741,   287,   428,  3277,    11,  1642,   340,   881,\n",
       "          4577,   284,  5911,   262, 34972,   329,   644,   340,   318,    25]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd2c3825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.026, -0.012, -0.015,  ..., -0.040, -0.021, -0.011],\n",
       "         [-0.003, -0.022,  0.000,  ..., -0.065,  0.057, -0.013],\n",
       "         [ 0.031,  0.016, -0.021,  ..., -0.010,  0.037, -0.012],\n",
       "         ...,\n",
       "         [-0.045, -0.021, -0.016,  ...,  0.053,  0.012, -0.014],\n",
       "         [-0.032, -0.004, -0.030,  ...,  0.026,  0.031, -0.027],\n",
       "         [-0.050, -0.015, -0.020,  ...,  0.037, -0.020, -0.028]]],\n",
       "       device='cuda:0', grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_luts[0](predicted_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556cbfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7afa960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4224da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32715028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################################################################################\n",
    "outputs = TargetEmbeddings(\n",
    "    embed_dim=primary_embed_dim,\n",
    "    embed_lut=embed_luts[0],\n",
    "    sent_length=sent_length,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    st=args.st,\n",
    "    init_value=init_value,\n",
    "    random_init=args.init == \"random\",\n",
    "    sampling_strategy=args.sampling_strategy,\n",
    "    sampling_strategy_k=args.sampling_strategy_k,\n",
    "    embed_scales=embed_scales,\n",
    "    metric=args.metric,\n",
    "    same_embed=args.same_embeds,\n",
    "    final_bias=final_bias,\n",
    "    eos_token_id=primary_tokenizer.eos_token_id\n",
    ")\n",
    "##################################################################################################################################################################\n",
    "#                 else:\n",
    "#                     raise ValueError(\"Wrong target_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f39658ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TargetEmbeddings()"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ff41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd37c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008539f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84fbb6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(losses) > 1:\n",
    "    lambda_ = Lambda(count=len(epsilons))\n",
    "    if use_cuda:\n",
    "        lambda_.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bda7e8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2', 'classification_no_prefix']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54e1f618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lambda()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9d3b079d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(args.lr); print(args.lambda_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d454d58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Optimizer.from_opt(outputs, args)\n",
    "cur_lr = args.lr\n",
    "# print(optimizer._optimizer.param_groups)\n",
    "# input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fbaea7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 0.026, -0.012, -0.015,  ..., -0.040, -0.021, -0.011],\n",
      "         [-0.003, -0.022,  0.000,  ..., -0.065,  0.057, -0.013],\n",
      "         [ 0.031,  0.016, -0.021,  ..., -0.010,  0.037, -0.012],\n",
      "         ...,\n",
      "         [-0.045, -0.021, -0.016,  ...,  0.053,  0.012, -0.014],\n",
      "         [-0.032, -0.004, -0.030,  ...,  0.026,  0.031, -0.027],\n",
      "         [-0.050, -0.015, -0.020,  ...,  0.037, -0.020, -0.028]]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for group in optimizer._optimizer.param_groups:\n",
    "    for p in group['params']:\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f9d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23597ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee0661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3dff22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "252387fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda()\n"
     ]
    }
   ],
   "source": [
    "if len(losses) > 1:\n",
    "    old_optim = args.optim\n",
    "    args.optim = \"gradascent\"\n",
    "    old_lr = args.lr\n",
    "    args.lr = args.lambda_lr\n",
    "    optimizer_lambda = Optimizer.from_opt(lambda_, args)\n",
    "    print(lambda_)\n",
    "    args.optim = old_optim\n",
    "    args.lr = old_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129786c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a203341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d3379e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea569b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = [None] * batch_size\n",
    "best_allsat = [False] * batch_size\n",
    "best_repeat_count = [0] * batch_size\n",
    "best_losses = [[None] * batch_size for _ in range(len(losses))]\n",
    "best_step = -100\n",
    "\n",
    "best_pred_tokens = [None] * batch_size\n",
    "best_prediction_set = [set() for _ in range(batch_size)]\n",
    "best_pred_probs = [None] * batch_size\n",
    "best_index = [-1 for i in range(batch_size)]\n",
    "\n",
    "scaler = None\n",
    "if args.model_dtype == \"fp16\" and args.fp16_source == \"pytorch\":\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for lossid, lossname in enumerate(losses):\n",
    "    losslists[lossid].append([])\n",
    "\n",
    "broken = False\n",
    "prev_loss = None\n",
    "dynamic_lambda_update_prev_loss = None\n",
    "same_loss_count = 0\n",
    "dynamic_loss_update_same_loss_count = 0\n",
    "starttime = time.time()\n",
    "repeat_counts = [0] * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0764008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ffdc252b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embedgd'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5c41d384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.optim_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3fe779ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "step=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b736afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_for_backward = []\n",
    "logging_outputs = []\n",
    "\n",
    "# print(optimizer.new_predictions)\n",
    "# what does this do?\n",
    "pred_embeds, pred_tokens, pred_probs = outputs.forward_multiple(embed_luts, new_predictions=getattr(optimizer._optimizer, \"new_predictions\", None))  # forward\n",
    "\n",
    "# if not args.time and args.debug:\n",
    "def get_sent(tokens, tokenizer):\n",
    "    batch = []\n",
    "    if args.target_tokenize_different:\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            for toks in tokens:\n",
    "                batch.append(tokenizer.decode(clean_output(toks.tolist(), -1, allow_first_eos=losses[0] == \"bart\")))\n",
    "    else:\n",
    "        for toks in tokens:\n",
    "            batch.append(tokenizer.decode(clean_output(toks.tolist(), -1, allow_first_eos=losses[0] == \"bart\")))\n",
    "    return batch\n",
    "\n",
    "target_sents = get_sent(torch.cat([target_prefix, pred_tokens], dim=1), primary_tokenizer)\n",
    "if step % 10 == 0:\n",
    "    hayley_result.update({f\"step_{step}_text\": target_sents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2fdfbde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 1280])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_embeds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df1f7534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 1280])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_embeds[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01524344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 1280])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_embeds[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8fa35c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "02ce506d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fdd72c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.466, -1.495, -2.117,  ..., -2.410, -2.129, -1.576],\n",
       "         [-2.035, -1.984, -2.460,  ..., -2.536, -2.230, -2.046],\n",
       "         [-2.066, -2.057, -2.434,  ..., -2.452, -2.318, -2.122],\n",
       "         ...,\n",
       "         [-1.555, -1.561, -2.164,  ..., -2.414, -2.126, -1.647],\n",
       "         [-1.505, -1.505, -2.137,  ..., -2.443, -2.109, -1.599],\n",
       "         [-1.452, -1.533, -2.116,  ..., -2.464, -2.161, -1.585]]],\n",
       "       device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6260cc89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d8f56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7320af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b84f66c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(target_sents, end=\"\\n\")\n",
    "original_preds = None\n",
    "if len(pred_embeds) > 1:\n",
    "    original_preds = pred_embeds[1]\n",
    "\n",
    "# print(\"what\", args.use_context)\n",
    "for lossid, lossname in enumerate(losses):\n",
    "    lossvalue, logging_output =\\\n",
    "        lossfns[lossid].compute_loss(\n",
    "            [source_batch, target_prefix], \n",
    "            [pred_tokens, pred_embeds[0][lossid], pred_probs], \n",
    "            additional_batch=additional_batch, \n",
    "            context_batch=context_batch,\n",
    "            use_context=args.use_context,\n",
    "            embed_scale=embed_scales[lossid], \n",
    "            label_id=label_ids[lossid],\n",
    "            keyword=keywords[lossid],\n",
    "            original_preds=original_preds,\n",
    "            kweight=new_kweight,\n",
    "            step=step\n",
    "        )\n",
    "\n",
    "    losslists[lossid][-1].append(lossvalue.sum().item())  #for logging\n",
    "    # hayley_result.update({f\"step_{step}_loss{lossid}\": lossvalue.sum().item()})\n",
    "    losses_for_backward.append(lossvalue)  # for backward\n",
    "    logging_outputs.append(logging_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7213f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "403f4d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor([0.218]),\n",
       " 'max_length': 20,\n",
       " 'nsentences': 1,\n",
       " 'lm_logprobs': tensor([[-0.808, -0.590]])}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d1b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba078f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "439736c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad(set_to_none=True)\n",
    "outputs.zero_grad()\n",
    "if len(losses) > 1:\n",
    "    optimizer_lambda.zero_grad(set_to_none=True)\n",
    "    lambda_.zero_grad()\n",
    "\n",
    "for model in name2model.values():\n",
    "    model.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7084cfed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "67f7643c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mucoco.utils.optim.EmbedGD"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(optimizer._optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc08317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e82d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "276393fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.0\n"
     ]
    }
   ],
   "source": [
    "if args.linear_scale == \"true\": # no lagragian, plain old linear sum\n",
    "    # \n",
    "    # grads = []\n",
    "    # if args.debug and args.debug_gradients == \"true\":\n",
    "    #     for sid in range(len(losses_for_backward)):\n",
    "    #         optimizer.backward(losses_for_backward[sid], retain_graph=True, scaler=scaler)\n",
    "    #         grad = []\n",
    "    #         for p in outputs.parameters():\n",
    "    #             grad.append(p.grad.data)\n",
    "    #             param_norm = p.grad.data.norm(2, -1).sum(dim=0)\n",
    "    #             print(sid, \"for theta\", param_norm)\n",
    "    #         grads.append(grad[0])\n",
    "    #         optimizer.zero_grad(set_to_none=True)\n",
    "    #         outputs.zero_grad(set_to_none=True)\n",
    "    #         for modelname in loss2modelname.values():\n",
    "    #             name2model[modelname].zero_grad(set_to_none=True) \n",
    "    #     graddot = (grads[0] * grads[1]).sum(dim=-1)\n",
    "    #     print(graddot)\n",
    "    #     grads0norm = torch.nn.functional.normalize(grads[0], p=2, dim=-1)\n",
    "    #     grads1norm = torch.nn.functional.normalize(grads[1], p=2, dim=-1)\n",
    "    #     print((grads0norm * grads1norm).sum(dim=-1))\n",
    "        # input()\n",
    "    # else:\n",
    "    # total_loss = betas[0] * losses_for_backward[0]\n",
    "    total_loss = 0\n",
    "    cur_epsilons = [] # just for avoiding syntax errors, epsilons are useless in this setting\n",
    "    for sid in range(len(losses_for_backward)):\n",
    "        total_loss = total_loss + betas[sid] * losses_for_backward[sid]\n",
    "        cur_epsilons.append(0.0)\n",
    "\n",
    "    total_batchloss = total_loss.sum()\n",
    "    optimizer.backward(total_batchloss, retain_graph=False, scaler=scaler)\n",
    "else:\n",
    "    total_loss = 0.0\n",
    "    total_loss = losses_for_backward[0]\n",
    "    # total_loss_for_lambda = 0.0\n",
    "    cur_epsilons = []\n",
    "    # print(total_loss.item(), end=\", \")\n",
    "\n",
    "    constraint_values = []\n",
    "    for sid in range(1, len(losses_for_backward)): #the secondary losses or constraints\n",
    "        # print(sid-1, epsilons, min_epsilons, epsilon_warmup_steps, epsilon_cooldown_steps)\n",
    "        cur_epsilon = get_epsilon(step, epsilons[sid-1], min_epsilons[sid-1], epsilon_warmup_steps[sid-1], epsilon_cooldown_steps[sid-1], epsilon_decay_functions[sid-1])\n",
    "        print(cur_epsilon)\n",
    "        constraint_value = (cur_epsilon - losses_for_backward[sid]).detach()\n",
    "        damp = args.dampness * constraint_value\n",
    "        # lambda_.set_active(sid-1, constraint_value)\n",
    "        mask = lambda_.get_mask(sid-1, damp)\n",
    "        # mask = 1.0\n",
    "\n",
    "        closs_for_theta = lambda_.get_loss(sid - 1, damp * mask, (cur_epsilon - losses_for_backward[sid]))\n",
    "        total_loss = total_loss - closs_for_theta\n",
    "\n",
    "        cur_epsilons.append(cur_epsilon)                             \n",
    "        constraint_values.append(constraint_value.item())\n",
    "\n",
    "    total_batchloss = total_loss.sum()\n",
    "    optimizer.backward(total_batchloss, retain_graph=False, scaler=scaler) ### calculate gradient\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86594394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f622aaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.dampness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fbbd7482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], device='cuda:0')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_.get_mask(sid-1, damp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9c13f5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.538], device='cuda:0')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2b8e15f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.897], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closs_for_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "57dddb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.538], device='cuda:0')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damp * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "85892019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.382], device='cuda:0', grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_epsilon - losses_for_backward[sid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0044fa96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f4aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90fab75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb016f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb5a0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bc8681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f49eecba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5.0]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "96bfddc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5.3820013999938965]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraint_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dfe2f94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.538], device='cuda:0')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb7e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9134db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.debug and args.debug_gradients == \"true\":\n",
    "    total_norm = 0\n",
    "    gi=0\n",
    "    for p in outputs.parameters():\n",
    "        gi+=1\n",
    "        param_norm = p.grad.data.norm(2, -1).sum(dim=0)\n",
    "        # print(p.dtype)\n",
    "        print(\"for theta\", param_norm)\n",
    "    for p in lambda_.parameters():\n",
    "        print(\"for lambda\", p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4809582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for theta tensor([21.992, 29.774, 33.268, 21.684, 25.161, 26.622, 21.426, 21.295, 22.553,\n",
      "        22.249, 21.385, 22.593, 20.672, 28.503, 28.234, 24.527, 20.883, 22.111,\n",
      "        26.308, 20.601], device='cuda:0')\n",
      "params tensor([[[-0.432,  0.422, -0.260,  ...,  0.831, -0.792,  1.038],\n",
      "         [-0.237, -0.345, -0.015,  ..., -0.179, -0.038,  1.567],\n",
      "         [ 0.407, -0.034, -1.884,  ...,  0.821, -0.435,  0.598],\n",
      "         ...,\n",
      "         [-0.348, -0.123, -0.119,  ...,  0.593,  0.058, -0.328],\n",
      "         [-0.371, -0.242,  1.205,  ...,  0.032, -0.425, -0.253],\n",
      "         [-0.925, -0.313, -0.351,  ...,  0.098, -0.235, -0.768]]],\n",
      "       device='cuda:0')\n",
      "for lambda tensor([5.382], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "total_norm = 0\n",
    "gi=0\n",
    "for p in outputs.parameters():\n",
    "    gi+=1\n",
    "    param_norm = p.grad.data.norm(2, -1).sum(dim=0)\n",
    "    # print(p.dtype)\n",
    "    print(\"for theta\", param_norm)\n",
    "    print(\"params\", p.grad.data)\n",
    "for p in lambda_.parameters():\n",
    "    print(\"for lambda\", p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb8931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "33c1de41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b0cb5a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0a660ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'false'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.debug_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "909e058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_outputs[0].get('entropy', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0cdbae80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor([223.893]),\n",
       " 'max_length': 20,\n",
       " 'nsentences': 1,\n",
       " 'lm_logprobs': tensor([[[ 0.531, -0.228,  0.584,  ..., -1.097,  1.028, -1.424],\n",
       "          [-0.059,  0.304, -0.144,  ...,  0.240,  0.140, -1.365],\n",
       "          [-0.332,  0.826,  0.938,  ..., -0.588,  0.644, -0.524],\n",
       "          ...,\n",
       "          [ 0.370,  0.117,  0.138,  ..., -0.678, -0.071,  0.339],\n",
       "          [ 0.193,  0.112, -1.046,  ...,  0.127,  0.354,  0.382],\n",
       "          [ 0.928,  0.312,  0.345,  ..., -0.082,  0.230,  0.782]]]),\n",
       " 'entropy': None}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b7d6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "21a5ded0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.432,  0.422, -0.260,  ...,  0.831, -0.792,  1.038],\n",
      "         [-0.237, -0.345, -0.015,  ..., -0.179, -0.038,  1.567],\n",
      "         [ 0.407, -0.034, -1.884,  ...,  0.821, -0.435,  0.598],\n",
      "         ...,\n",
      "         [-0.348, -0.123, -0.119,  ...,  0.593,  0.058, -0.328],\n",
      "         [-0.371, -0.242,  1.205,  ...,  0.032, -0.425, -0.253],\n",
      "         [-0.925, -0.313, -0.351,  ...,  0.098, -0.235, -0.768]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "indices = [[0,1]] * len(source_batch)\n",
    "if logging_outputs[0].get('entropy', None) is not None:\n",
    "    optimizer.step(scaler=scaler, entropy=logging_outputs[0].get('entropy', None), indices=indices) ### backpropagate\n",
    "else:\n",
    "    optimizer.step(scaler=scaler, indices=indices) ### backpropagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1347a83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.432,  0.422, -0.260,  ...,  0.831, -0.792,  1.038],\n",
      "         [-0.237, -0.345, -0.015,  ..., -0.179, -0.038,  1.567],\n",
      "         [ 0.000, -0.000, -0.000,  ...,  0.000, -0.000,  0.000],\n",
      "         ...,\n",
      "         [-0.000, -0.000, -0.000,  ...,  0.000,  0.000, -0.000],\n",
      "         [-0.000, -0.000,  0.000,  ...,  0.000, -0.000, -0.000],\n",
      "         [-0.000, -0.000, -0.000,  ...,  0.000, -0.000, -0.000]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ddabb232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(optimizer._optimizer.lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f70899a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer._optimizer.lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b0d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4e69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ae96693e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.grad[0][19][1].data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ff638dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[-0.008, -0.023, -0.020,  ..., -0.013,  0.056, -0.032],\n",
      "         [-0.003, -0.022,  0.000,  ..., -0.065,  0.057, -0.013],\n",
      "         [ 0.050,  0.007,  0.013,  ..., -0.091,  0.082,  0.014],\n",
      "         ...,\n",
      "         [ 0.051,  0.063,  0.024,  ...,  0.024, -0.034,  0.023],\n",
      "         [-0.032, -0.004, -0.030,  ...,  0.026,  0.031, -0.027],\n",
      "         [-0.050, -0.015, -0.020,  ...,  0.037, -0.020, -0.028]]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in outputs.parameters():\n",
    "    gi+=1\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4cf60ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 0.005,  0.020, -0.013,  ...,  0.046,  0.004, -0.021],\n",
      "         [-0.003, -0.022,  0.000,  ..., -0.065,  0.057, -0.013],\n",
      "         [ 0.050,  0.007,  0.013,  ..., -0.091,  0.082,  0.014],\n",
      "         ...,\n",
      "         [ 0.051,  0.063,  0.024,  ...,  0.024, -0.034,  0.023],\n",
      "         [ 0.018,  0.024, -0.010,  ..., -0.062,  0.031,  0.091],\n",
      "         [-0.040,  0.072, -0.037,  ..., -0.015, -0.041,  0.019]]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "torch.Size([1, 20, 1280])\n"
     ]
    }
   ],
   "source": [
    "for p in outputs.parameters():\n",
    "    gi+=1\n",
    "    print(p)\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02dca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4e1839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a968f1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84b22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "                # input()\n",
    "\n",
    "        # text (어떤 변수? source_batch)를 태워서 locate 하기 (근데, 궁금한것은 source_batch와 target_prefix 는 다른건가?)\n",
    "        # index가 여기에서 뽑히는 거라고 생각하자.\n",
    "        indices = [[0,1]] * len(source_batch)\n",
    "        if logging_outputs[0].get('entropy', None) is not None:\n",
    "            optimizer.step(scaler=scaler, entropy=logging_outputs[0].get('entropy', None), indices=indices) ### backpropagate\n",
    "        else:\n",
    "            optimizer.step(scaler=scaler, indices=indices) ### backpropagate\n",
    "\n",
    "        update_lr_condition = \"none\"\n",
    "        if args.linear_scale != \"true\" and  len(losses) > 1:\n",
    "            sats = torch.Tensor(constraint_values).ge(0.).to(device)\n",
    "            update_lambda_condition = (step % args.lambda_update == 0)\n",
    "            lambda_mask = float(update_lambda_condition) * torch.ones_like(sats)\n",
    "\n",
    "            lambda_mask += (1-sats.float()) * (lambda_.is_zero())\n",
    "            # if not sats.all() and (lambda_.any_zero()):\n",
    "            #     print(\"funky new update\")\n",
    "            #     update_lambda_condition = True\n",
    "            #     lambda_mask = torch.ones_like(sats)\n",
    "            # lambda_mask += sats.float()\n",
    "\n",
    "            # if args.linear_scale != \"true\" and  len(losses) > 1 and args.dynamic_lambda_update:\n",
    "                # lambda_mask += (1 - sats.float())\n",
    "            # if step > args.lambda_update:\n",
    "\n",
    "        # total_batchlossitem = total_batchloss.item()\n",
    "        total_batchlossitem = losses_for_backward[0].item()\n",
    "        # if dynamic_lambda_update_prev_loss is not None:\n",
    "            # print(abs(total_batchlossitem - dynamic_lambda_update_prev_loss))\n",
    "        if dynamic_lambda_update_prev_loss is not None and abs(total_batchlossitem - dynamic_lambda_update_prev_loss) <= 1e-6:\n",
    "            repeat_counts[0] += 1\n",
    "            if args.linear_scale != \"true\" and  len(losses) > 1 and args.dynamic_lambda_update:\n",
    "                lambda_mask = (1 - sats.float())\n",
    "                # print(\"what now\", total_batchlossitem, dynamic_lambda_update_prev_loss, constraint_values, sats.float())\n",
    "                # if sats.all(): #constraints are satisfied\n",
    "                #     update_lambda_condition = False\n",
    "                #     print(\"constraints are satisfied and output is not changing, lambdas will not update!\")\n",
    "                # else:\n",
    "                #     update_lambda_condition = True\n",
    "\n",
    "            if args.dynamic_lr_update and best_allsat[0] is not None and best_allsat[0]:\n",
    "                update_lr_condition = \"increase\"\n",
    "        else:\n",
    "            repeat_counts[0] = 1\n",
    "        # print(repeat_counts)\n",
    "\n",
    "        dynamic_lambda_update_prev_loss = total_batchlossitem\n",
    "\n",
    "        if update_lr_condition == \"increase\":\n",
    "            cur_lr = optimizer._optimizer.update_lr(min(cur_lr + args.lr_update_size, args.max_lr))\n",
    "\n",
    "        if args.linear_scale != \"true\" and len(losses) > 1:\n",
    "            # print(lambda_mask, repeat_counts)\n",
    "            # print([p.grad for p in lambda_.parameters()])\n",
    "            # print(step, lambda_().tolist(), lambda_mask, )\n",
    "            optimizer_lambda._optimizer.set_mask(lambda_mask.clamp(max=1.0, min=0.0))\n",
    "            optimizer_lambda.step()\n",
    "            # print(step, lambda_().tolist())\n",
    "            # input()\n",
    "            lambda_.make_positive()\n",
    "\n",
    "\n",
    "\n",
    "            # total_batchloss_for_lambda = total_loss_for_lambda.sum()\n",
    "            # optimizer_lambda.backward(total_batchloss_for_lambda, retain_graph=True, scaler=scaler)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        # outputs.printparams()\n",
    "        # input()\n",
    "\n",
    "\n",
    "        # print(repeat_counts, allsat)\n",
    "        cur_losses = []\n",
    "        for b in range(batch_size):\n",
    "            cur_loss = 0.0\n",
    "            for beta, lossval in zip(betas, losses_for_backward):\n",
    "                cur_loss = cur_loss + beta * lossval[b].item()     \n",
    "            cur_losses.append(cur_loss)\n",
    "\n",
    "            constrained = []\n",
    "            allsat = True\n",
    "            for i in range(1, len(losses)):\n",
    "                if losses_for_backward[i] <= min_epsilons[i - 1]:\n",
    "                    constrained.append(\"sat\")\n",
    "                else:\n",
    "                    constrained.append(\"vio\")\n",
    "                    allsat=False\n",
    "\n",
    "            if args.show_all_outputs and len(losses) > 1 and allsat:\n",
    "                best_prediction_set[b].add(target_sents[b])\n",
    "\n",
    "            constrained = \",\".join(constrained)\n",
    "\n",
    "            modify_condition =\\\n",
    "                args.selection_criterion == \"last\" or\\\n",
    "                (best_loss[b] is None and args.selection_criterion == \"weighted_sum\") or\\\n",
    "                (best_loss[b] is not None and args.selection_criterion == \"weighted_sum\" and best_loss[b] > cur_loss)\n",
    "\n",
    "            # print(repeat_counts, allsat, best_loss, best_allsat)\n",
    "            if not modify_condition and args.selection_criterion == \"mrr_allsat\":\n",
    "                modify_condition =\\\n",
    "                    (best_loss[b] is None and allsat and repeat_counts[b] == 2) or\\\n",
    "                    (best_loss[b] is not None and best_allsat[b] and allsat and repeat_counts[b] == 2)\n",
    "                # print(modify_condition)\n",
    "                # modify_condition = (best_loss[b] is not None and best_allsat[b] and allsat and repeat_counts[b] == 2)\n",
    "\n",
    "            elif not modify_condition and args.selection_criterion == \"primary_allsat\":\n",
    "                modify_condition =\\\n",
    "                    (best_loss[b] is None and allsat) or\\\n",
    "                    (best_loss[b] is not None and not best_allsat[b] and allsat) or\\\n",
    "                    (best_allsat[b] and allsat and best_loss[b] > cur_loss)\n",
    "\n",
    "            # step>20 and \n",
    "\n",
    "            if modify_condition:\n",
    "                if args.dynamic_lr_update:\n",
    "                    print(\"resetting the learning rate and noise std, a constraint has been satisfied\")\n",
    "                    cur_lr = optimizer._optimizer.update_lr(args.lr)\n",
    "                    optimizer._optimizer.set_begin_std(0.01) #CHECK\n",
    "                if args.selection_criterion != \"last\":\n",
    "                    print(f\"modify condition @{step}\", time.time()-starttime, end=\"\\n\")\n",
    "                best_loss[b] = cur_loss\n",
    "                best_allsat[b] = allsat\n",
    "                best_repeat_count[b] = repeat_counts[b]\n",
    "                for i in range(len(losses)):\n",
    "                    best_losses[i][b] = losses_for_backward[i][b].item()\n",
    "\n",
    "                best_pred_tokens[b] = pred_tokens[b]\n",
    "                best_index[b] = step\n",
    "                # best_pred_probs[b] = (pred_probs[b].cpu(), logging_outputs[0][\"lm_logprobs\"][b])\n",
    "                best_constrained = constrained\n",
    "                best_step = step\n",
    "            # elif best_step < step - 1 and args.dynamic_lr_update:\n",
    "            #     print(\"resetting the learning rate, the constraint just got unsatisfied\")\n",
    "\n",
    "        if not args.time and step > 0 and step % args.log_interval == 0:\n",
    "            if len(losses) > 1:\n",
    "                log = f\"beam cons: {predicted_allsat}; \"\n",
    "                log = f\"Step {step}: lr:{cur_lr}; total_loss:{total_batchloss:.4f}; current [loss:{sum(cur_losses):.4f}; l:{','.join([f'{x:.4f}' for x in lambda_().tolist()])}; e:{','.join([f'{x:.4f}' for x in cur_epsilons])}; cons:{constrained}; \"\n",
    "                for i in range(len(losslists)):\n",
    "                    log = log + f\" {lossabbr[i]}:{losslists[i][-1][-1]:.4f}; \"\n",
    "\n",
    "                if best_loss[0] is not None:\n",
    "                    log = log[:-1] + f\"] |||| best [cur_loss:{sum(best_loss):.4f}; cons:{best_constrained};  \"\n",
    "                    for i in range(len(best_losses)):\n",
    "                        log = log + f\"{lossabbr[i]}:{sum(best_losses[i]):.4f}; \"\n",
    "                    log = log[:-1] + f\"@ step #{best_index[-1]}\" \n",
    "                    log = log + \"]\"\n",
    "                else:\n",
    "                    log = log[:-1] + f\"] |||| best [none of the generations so far satisfies constraints]\"\n",
    "                print(log)\n",
    "            else:\n",
    "                log = f\"Step {step}: lr:{cur_lr}; loss:{total_batchloss:.4f}; current [loss:{sum(cur_losses):.4f}; \"\n",
    "                for i in range(len(losslists)):\n",
    "                    log = log + f\" {lossabbr[i]}:{losslists[i][-1][-1]:.4f}; \"\n",
    "\n",
    "                if best_loss[0] is not None:\n",
    "                    log = log[:-1] + f\"] best [loss:{sum(best_loss):.4f} \"\n",
    "                    for i in range(len(best_losses)):\n",
    "                        log = log + f\"{lossabbr[i]}:{sum(best_losses[i]):.4f}; \"\n",
    "                    log = log[:-1] + f\" at step {best_index[-1]}\" \n",
    "                    log = log + \"]\"\n",
    "                else:\n",
    "                    log = log[:-1] + f\"] |||| best [none of the generations so far satisfies constraints]\"\n",
    "                print(log, end=\"\\n\")\n",
    "\n",
    "        del losses_for_backward\n",
    "\n",
    "        if args.early_stop_steps > 0: #[0] is batch index, batch size in our case in 1 always so it doesn't matter.\n",
    "            # print(args.selection_criterion)\n",
    "            # print(lengthwise_best_prediction)\n",
    "            early_stop_condition =\\\n",
    "                (\"allsat\" in args.selection_criterion and best_allsat[0]) or\\\n",
    "                (args.selection_criterion == \"weighted_sum\") or\\\n",
    "                (args.selection_criterion == \"last\")\n",
    "\n",
    "            # print(early_stop_condition)\n",
    "            if prev_loss is not None and abs(cur_loss - prev_loss) <= 1e-6:\n",
    "                same_loss_count += 1\n",
    "            else:   \n",
    "                same_loss_count = 0\n",
    "\n",
    "            if early_stop_condition and same_loss_count >= args.early_stop_steps:\n",
    "                print(f\"Early stop at @{step} with a loss value of {cur_loss} and satisfied constraints\")\n",
    "                break\n",
    "            elif same_loss_count >= args.early_stop_steps + 100:#2 * args.lambda_update:\n",
    "                print(f\"Early stop at @{step} with a loss value of {cur_loss} and unsatisfied constraints\")\n",
    "                break\n",
    "\n",
    "            prev_loss = cur_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a26e7016",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (389743453.py, line 459)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[57], line 459\u001b[0;36m\u001b[0m\n\u001b[0;31m    if restart_idx < args.restarts: #atleast one more restart is left\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################################################################################\n",
    "# gradient ascent the embeddings\n",
    "##################################################################################################################################################################\n",
    "for step in range(args.optim_steps):\n",
    "    try:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            losses_for_backward = []\n",
    "            logging_outputs = []\n",
    "\n",
    "            # print(optimizer.new_predictions)\n",
    "            # what does this do?\n",
    "            pred_embeds, pred_tokens, pred_probs = outputs.forward_multiple(embed_luts, new_predictions=getattr(optimizer._optimizer, \"new_predictions\", None))  # forward\n",
    "\n",
    "            # if not args.time and args.debug:\n",
    "            def get_sent(tokens, tokenizer):\n",
    "                batch = []\n",
    "                if args.target_tokenize_different:\n",
    "                    with tokenizer.as_target_tokenizer():\n",
    "                        for toks in tokens:\n",
    "                            batch.append(tokenizer.decode(clean_output(toks.tolist(), -1, allow_first_eos=losses[0] == \"bart\")))\n",
    "                else:\n",
    "                    for toks in tokens:\n",
    "                        batch.append(tokenizer.decode(clean_output(toks.tolist(), -1, allow_first_eos=losses[0] == \"bart\")))\n",
    "                return batch\n",
    "\n",
    "            target_sents = get_sent(torch.cat([target_prefix, pred_tokens], dim=1), primary_tokenizer)\n",
    "            if step % 10 == 0:\n",
    "                hayley_result.update({f\"step_{step}_text\": target_sents})\n",
    "            # print(target_sents, end=\"\\n\")\n",
    "\n",
    "            original_preds = None\n",
    "            if len(pred_embeds) > 1:\n",
    "                original_preds = pred_embeds[1]\n",
    "\n",
    "            # print(\"what\", args.use_context)\n",
    "            for lossid, lossname in enumerate(losses):\n",
    "                lossvalue, logging_output =\\\n",
    "                    lossfns[lossid].compute_loss(\n",
    "                        [source_batch, target_prefix], \n",
    "                        [pred_tokens, pred_embeds[0][lossid], pred_probs], \n",
    "                        additional_batch=additional_batch, \n",
    "                        context_batch=context_batch,\n",
    "                        use_context=args.use_context,\n",
    "                        embed_scale=embed_scales[lossid], \n",
    "                        label_id=label_ids[lossid],\n",
    "                        keyword=keywords[lossid],\n",
    "                        original_preds=original_preds,\n",
    "                        kweight=new_kweight,\n",
    "                        step=step\n",
    "                    )\n",
    "\n",
    "                losslists[lossid][-1].append(lossvalue.sum().item())  #for logging\n",
    "                # hayley_result.update({f\"step_{step}_loss{lossid}\": lossvalue.sum().item()})\n",
    "                losses_for_backward.append(lossvalue)  # for backward\n",
    "                logging_outputs.append(logging_output)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs.zero_grad()\n",
    "            if len(losses) > 1:\n",
    "                optimizer_lambda.zero_grad(set_to_none=True)\n",
    "                lambda_.zero_grad()\n",
    "\n",
    "            for model in name2model.values():\n",
    "                model.zero_grad(set_to_none=True)\n",
    "\n",
    "            if args.linear_scale == \"true\": # no lagragian, plain old linear sum\n",
    "                # \n",
    "                # grads = []\n",
    "                # if args.debug and args.debug_gradients == \"true\":\n",
    "                #     for sid in range(len(losses_for_backward)):\n",
    "                #         optimizer.backward(losses_for_backward[sid], retain_graph=True, scaler=scaler)\n",
    "                #         grad = []\n",
    "                #         for p in outputs.parameters():\n",
    "                #             grad.append(p.grad.data)\n",
    "                #             param_norm = p.grad.data.norm(2, -1).sum(dim=0)\n",
    "                #             print(sid, \"for theta\", param_norm)\n",
    "                #         grads.append(grad[0])\n",
    "                #         optimizer.zero_grad(set_to_none=True)\n",
    "                #         outputs.zero_grad(set_to_none=True)\n",
    "                #         for modelname in loss2modelname.values():\n",
    "                #             name2model[modelname].zero_grad(set_to_none=True) \n",
    "                #     graddot = (grads[0] * grads[1]).sum(dim=-1)\n",
    "                #     print(graddot)\n",
    "                #     grads0norm = torch.nn.functional.normalize(grads[0], p=2, dim=-1)\n",
    "                #     grads1norm = torch.nn.functional.normalize(grads[1], p=2, dim=-1)\n",
    "                #     print((grads0norm * grads1norm).sum(dim=-1))\n",
    "                    # input()\n",
    "                # else:\n",
    "                # total_loss = betas[0] * losses_for_backward[0]\n",
    "                total_loss = 0\n",
    "                cur_epsilons = [] # just for avoiding syntax errors, epsilons are useless in this setting\n",
    "                for sid in range(len(losses_for_backward)):\n",
    "                    total_loss = total_loss + betas[sid] * losses_for_backward[sid]\n",
    "                    cur_epsilons.append(0.0)\n",
    "\n",
    "                total_batchloss = total_loss.sum()\n",
    "                optimizer.backward(total_batchloss, retain_graph=False, scaler=scaler)\n",
    "            else:\n",
    "                total_loss = 0.0\n",
    "                total_loss = losses_for_backward[0]\n",
    "                # total_loss_for_lambda = 0.0\n",
    "                cur_epsilons = []\n",
    "                # print(total_loss.item(), end=\", \")\n",
    "\n",
    "                constraint_values = []\n",
    "                for sid in range(1, len(losses_for_backward)): #the secondary losses or constraints\n",
    "                    # print(sid-1, epsilons, min_epsilons, epsilon_warmup_steps, epsilon_cooldown_steps)\n",
    "                    cur_epsilon = get_epsilon(step, epsilons[sid-1], min_epsilons[sid-1], epsilon_warmup_steps[sid-1], epsilon_cooldown_steps[sid-1], epsilon_decay_functions[sid-1])\n",
    "                    # print(cur_epsilon)\n",
    "                    constraint_value = (cur_epsilon - losses_for_backward[sid]).detach()\n",
    "                    damp = args.dampness * constraint_value\n",
    "                    # lambda_.set_active(sid-1, constraint_value)\n",
    "                    mask = lambda_.get_mask(sid-1, damp)\n",
    "                    # mask = 1.0\n",
    "\n",
    "                    closs_for_theta = lambda_.get_loss(sid - 1, damp * mask, (cur_epsilon - losses_for_backward[sid]))\n",
    "                    total_loss = total_loss - closs_for_theta\n",
    "\n",
    "                    cur_epsilons.append(cur_epsilon)                             \n",
    "                    constraint_values.append(constraint_value.item())\n",
    "\n",
    "                total_batchloss = total_loss.sum()\n",
    "                optimizer.backward(total_batchloss, retain_graph=False, scaler=scaler) ### calculate gradient\n",
    "\n",
    "            if args.debug and args.debug_gradients == \"true\":\n",
    "                total_norm = 0\n",
    "                gi=0\n",
    "                for p in outputs.parameters():\n",
    "                    gi+=1\n",
    "                    param_norm = p.grad.data.norm(2, -1).sum(dim=0)\n",
    "                    # print(p.dtype)\n",
    "                    print(\"for theta\", param_norm)\n",
    "                for p in lambda_.parameters():\n",
    "                    print(\"for lambda\", p.grad)\n",
    "\n",
    "                # input()\n",
    "\n",
    "        # text (어떤 변수? source_batch)를 태워서 locate 하기 (근데, 궁금한것은 source_batch와 target_prefix 는 다른건가?)\n",
    "        # index가 여기에서 뽑히는 거라고 생각하자.\n",
    "        indices = [[0,1]] * len(source_batch)\n",
    "        if logging_outputs[0].get('entropy', None) is not None:\n",
    "            optimizer.step(scaler=scaler, entropy=logging_outputs[0].get('entropy', None), indices=indices) ### backpropagate\n",
    "        else:\n",
    "            optimizer.step(scaler=scaler, indices=indices) ### backpropagate\n",
    "\n",
    "        update_lr_condition = \"none\"\n",
    "        if args.linear_scale != \"true\" and  len(losses) > 1:\n",
    "            sats = torch.Tensor(constraint_values).ge(0.).to(device)\n",
    "            update_lambda_condition = (step % args.lambda_update == 0)\n",
    "            lambda_mask = float(update_lambda_condition) * torch.ones_like(sats)\n",
    "\n",
    "            lambda_mask += (1-sats.float()) * (lambda_.is_zero())\n",
    "            # if not sats.all() and (lambda_.any_zero()):\n",
    "            #     print(\"funky new update\")\n",
    "            #     update_lambda_condition = True\n",
    "            #     lambda_mask = torch.ones_like(sats)\n",
    "            # lambda_mask += sats.float()\n",
    "\n",
    "            # if args.linear_scale != \"true\" and  len(losses) > 1 and args.dynamic_lambda_update:\n",
    "                # lambda_mask += (1 - sats.float())\n",
    "            # if step > args.lambda_update:\n",
    "\n",
    "        # total_batchlossitem = total_batchloss.item()\n",
    "        total_batchlossitem = losses_for_backward[0].item()\n",
    "        # if dynamic_lambda_update_prev_loss is not None:\n",
    "            # print(abs(total_batchlossitem - dynamic_lambda_update_prev_loss))\n",
    "        if dynamic_lambda_update_prev_loss is not None and abs(total_batchlossitem - dynamic_lambda_update_prev_loss) <= 1e-6:\n",
    "            repeat_counts[0] += 1\n",
    "            if args.linear_scale != \"true\" and  len(losses) > 1 and args.dynamic_lambda_update:\n",
    "                lambda_mask = (1 - sats.float())\n",
    "                # print(\"what now\", total_batchlossitem, dynamic_lambda_update_prev_loss, constraint_values, sats.float())\n",
    "                # if sats.all(): #constraints are satisfied\n",
    "                #     update_lambda_condition = False\n",
    "                #     print(\"constraints are satisfied and output is not changing, lambdas will not update!\")\n",
    "                # else:\n",
    "                #     update_lambda_condition = True\n",
    "\n",
    "            if args.dynamic_lr_update and best_allsat[0] is not None and best_allsat[0]:\n",
    "                update_lr_condition = \"increase\"\n",
    "        else:\n",
    "            repeat_counts[0] = 1\n",
    "        # print(repeat_counts)\n",
    "\n",
    "        dynamic_lambda_update_prev_loss = total_batchlossitem\n",
    "\n",
    "        if update_lr_condition == \"increase\":\n",
    "            cur_lr = optimizer._optimizer.update_lr(min(cur_lr + args.lr_update_size, args.max_lr))\n",
    "\n",
    "        if args.linear_scale != \"true\" and len(losses) > 1:\n",
    "            # print(lambda_mask, repeat_counts)\n",
    "            # print([p.grad for p in lambda_.parameters()])\n",
    "            # print(step, lambda_().tolist(), lambda_mask, )\n",
    "            optimizer_lambda._optimizer.set_mask(lambda_mask.clamp(max=1.0, min=0.0))\n",
    "            optimizer_lambda.step()\n",
    "            # print(step, lambda_().tolist())\n",
    "            # input()\n",
    "            lambda_.make_positive()\n",
    "\n",
    "\n",
    "\n",
    "            # total_batchloss_for_lambda = total_loss_for_lambda.sum()\n",
    "            # optimizer_lambda.backward(total_batchloss_for_lambda, retain_graph=True, scaler=scaler)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        # outputs.printparams()\n",
    "        # input()\n",
    "\n",
    "\n",
    "        # print(repeat_counts, allsat)\n",
    "        cur_losses = []\n",
    "        for b in range(batch_size):\n",
    "            cur_loss = 0.0\n",
    "            for beta, lossval in zip(betas, losses_for_backward):\n",
    "                cur_loss = cur_loss + beta * lossval[b].item()     \n",
    "            cur_losses.append(cur_loss)\n",
    "\n",
    "            constrained = []\n",
    "            allsat = True\n",
    "            for i in range(1, len(losses)):\n",
    "                if losses_for_backward[i] <= min_epsilons[i - 1]:\n",
    "                    constrained.append(\"sat\")\n",
    "                else:\n",
    "                    constrained.append(\"vio\")\n",
    "                    allsat=False\n",
    "\n",
    "            if args.show_all_outputs and len(losses) > 1 and allsat:\n",
    "                best_prediction_set[b].add(target_sents[b])\n",
    "\n",
    "            constrained = \",\".join(constrained)\n",
    "\n",
    "            modify_condition =\\\n",
    "                args.selection_criterion == \"last\" or\\\n",
    "                (best_loss[b] is None and args.selection_criterion == \"weighted_sum\") or\\\n",
    "                (best_loss[b] is not None and args.selection_criterion == \"weighted_sum\" and best_loss[b] > cur_loss)\n",
    "\n",
    "            # print(repeat_counts, allsat, best_loss, best_allsat)\n",
    "            if not modify_condition and args.selection_criterion == \"mrr_allsat\":\n",
    "                modify_condition =\\\n",
    "                    (best_loss[b] is None and allsat and repeat_counts[b] == 2) or\\\n",
    "                    (best_loss[b] is not None and best_allsat[b] and allsat and repeat_counts[b] == 2)\n",
    "                # print(modify_condition)\n",
    "                # modify_condition = (best_loss[b] is not None and best_allsat[b] and allsat and repeat_counts[b] == 2)\n",
    "\n",
    "            elif not modify_condition and args.selection_criterion == \"primary_allsat\":\n",
    "                modify_condition =\\\n",
    "                    (best_loss[b] is None and allsat) or\\\n",
    "                    (best_loss[b] is not None and not best_allsat[b] and allsat) or\\\n",
    "                    (best_allsat[b] and allsat and best_loss[b] > cur_loss)\n",
    "\n",
    "            # step>20 and \n",
    "\n",
    "            if modify_condition:\n",
    "                if args.dynamic_lr_update:\n",
    "                    print(\"resetting the learning rate and noise std, a constraint has been satisfied\")\n",
    "                    cur_lr = optimizer._optimizer.update_lr(args.lr)\n",
    "                    optimizer._optimizer.set_begin_std(0.01) #CHECK\n",
    "                if args.selection_criterion != \"last\":\n",
    "                    print(f\"modify condition @{step}\", time.time()-starttime, end=\"\\n\")\n",
    "                best_loss[b] = cur_loss\n",
    "                best_allsat[b] = allsat\n",
    "                best_repeat_count[b] = repeat_counts[b]\n",
    "                for i in range(len(losses)):\n",
    "                    best_losses[i][b] = losses_for_backward[i][b].item()\n",
    "\n",
    "                best_pred_tokens[b] = pred_tokens[b]\n",
    "                best_index[b] = step\n",
    "                # best_pred_probs[b] = (pred_probs[b].cpu(), logging_outputs[0][\"lm_logprobs\"][b])\n",
    "                best_constrained = constrained\n",
    "                best_step = step\n",
    "            # elif best_step < step - 1 and args.dynamic_lr_update:\n",
    "            #     print(\"resetting the learning rate, the constraint just got unsatisfied\")\n",
    "\n",
    "        if not args.time and step > 0 and step % args.log_interval == 0:\n",
    "            if len(losses) > 1:\n",
    "                log = f\"beam cons: {predicted_allsat}; \"\n",
    "                log = f\"Step {step}: lr:{cur_lr}; total_loss:{total_batchloss:.4f}; current [loss:{sum(cur_losses):.4f}; l:{','.join([f'{x:.4f}' for x in lambda_().tolist()])}; e:{','.join([f'{x:.4f}' for x in cur_epsilons])}; cons:{constrained}; \"\n",
    "                for i in range(len(losslists)):\n",
    "                    log = log + f\" {lossabbr[i]}:{losslists[i][-1][-1]:.4f}; \"\n",
    "\n",
    "                if best_loss[0] is not None:\n",
    "                    log = log[:-1] + f\"] |||| best [cur_loss:{sum(best_loss):.4f}; cons:{best_constrained};  \"\n",
    "                    for i in range(len(best_losses)):\n",
    "                        log = log + f\"{lossabbr[i]}:{sum(best_losses[i]):.4f}; \"\n",
    "                    log = log[:-1] + f\"@ step #{best_index[-1]}\" \n",
    "                    log = log + \"]\"\n",
    "                else:\n",
    "                    log = log[:-1] + f\"] |||| best [none of the generations so far satisfies constraints]\"\n",
    "                print(log)\n",
    "            else:\n",
    "                log = f\"Step {step}: lr:{cur_lr}; loss:{total_batchloss:.4f}; current [loss:{sum(cur_losses):.4f}; \"\n",
    "                for i in range(len(losslists)):\n",
    "                    log = log + f\" {lossabbr[i]}:{losslists[i][-1][-1]:.4f}; \"\n",
    "\n",
    "                if best_loss[0] is not None:\n",
    "                    log = log[:-1] + f\"] best [loss:{sum(best_loss):.4f} \"\n",
    "                    for i in range(len(best_losses)):\n",
    "                        log = log + f\"{lossabbr[i]}:{sum(best_losses[i]):.4f}; \"\n",
    "                    log = log[:-1] + f\" at step {best_index[-1]}\" \n",
    "                    log = log + \"]\"\n",
    "                else:\n",
    "                    log = log[:-1] + f\"] |||| best [none of the generations so far satisfies constraints]\"\n",
    "                print(log, end=\"\\n\")\n",
    "\n",
    "        del losses_for_backward\n",
    "\n",
    "        if args.early_stop_steps > 0: #[0] is batch index, batch size in our case in 1 always so it doesn't matter.\n",
    "            # print(args.selection_criterion)\n",
    "            # print(lengthwise_best_prediction)\n",
    "            early_stop_condition =\\\n",
    "                (\"allsat\" in args.selection_criterion and best_allsat[0]) or\\\n",
    "                (args.selection_criterion == \"weighted_sum\") or\\\n",
    "                (args.selection_criterion == \"last\")\n",
    "\n",
    "            # print(early_stop_condition)\n",
    "            if prev_loss is not None and abs(cur_loss - prev_loss) <= 1e-6:\n",
    "                same_loss_count += 1\n",
    "            else:   \n",
    "                same_loss_count = 0\n",
    "\n",
    "            if early_stop_condition and same_loss_count >= args.early_stop_steps:\n",
    "                print(f\"Early stop at @{step} with a loss value of {cur_loss} and satisfied constraints\")\n",
    "                break\n",
    "            elif same_loss_count >= args.early_stop_steps + 100:#2 * args.lambda_update:\n",
    "                print(f\"Early stop at @{step} with a loss value of {cur_loss} and unsatisfied constraints\")\n",
    "                break\n",
    "\n",
    "            prev_loss = cur_loss\n",
    "\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"skipping remaining optimizing steps and showing the best option so far\")\n",
    "        broken=True\n",
    "        break\n",
    "\n",
    "if args.time:\n",
    "    r = time.time()-starttime\n",
    "    print(r)\n",
    "    avg_time += r\n",
    "\n",
    "predictions = []\n",
    "prediction_idss = []\n",
    "broken_skip = False\n",
    "skip_printing = False\n",
    "for b, item in enumerate(best_pred_tokens):\n",
    "    if item is None and broken:\n",
    "        skip_printing = True\n",
    "        if broken:\n",
    "            broken_skip=input(\"Skip this input entirely? yes(y)/no(continue)/press ctrl+c to exit\")\n",
    "            broken_skip = broken_skip.lower() == \"y\"\n",
    "            break\n",
    "    if (args.only_mucoco == \"false\" and not best_allsat[b]) or (item is None): #item is none happens when optimization fails\n",
    "        prediction_ids = \", \".join([str(idx) for idx in AR_predicted_indices[0].tolist()])\n",
    "        prediction_indices = AR_predicted_indices[0].tolist()\n",
    "        prediction = AR_prediction\n",
    "\n",
    "        lossvalue = 0.0\n",
    "        for lossid in range(len(betas)):\n",
    "            lossvalue += betas[lossid] * predictedlosslists[-1][lossid][b] # VERIFICATION NEEDED\n",
    "        print(f\"best prediction is from beam search, all constraints were not satisfied, allsat={lengthwise_best_prediction[b][2]}\")\n",
    "    else:\n",
    "        prediction_ids = \", \".join([str(x) for x in target_prefix[b].tolist()])\n",
    "        prediction_ids +=   f'[{\", \".join([str(x) for x in item.tolist()])}]'\n",
    "        prediction_indices = target_prefix[b].tolist() + item.tolist()\n",
    "\n",
    "        targets = clean_output(item.tolist(), primary_tokenizer.eos_token_id, allow_first_eos=losses[0] == \"bart\")#, prompt=source_batch[b].unsqueeze(0), sentence_complete=True, lossfn=lossfns[0])\n",
    "        if args.target_tokenize_different:\n",
    "            with primary_tokenizer.as_target_tokenizer():\n",
    "                prediction = primary_tokenizer.decode(target_prefix[b].tolist() + targets)\n",
    "        else:\n",
    "            prediction = primary_tokenizer.decode(target_prefix[b].tolist() + targets)\n",
    "\n",
    "        print(\"best prediction at step\",best_index[b])\n",
    "        lossvalue = best_loss[b]\n",
    "\n",
    "        modify_condition =\\\n",
    "            lengthwise_best_prediction[b] is None or\\\n",
    "            (args.selection_criterion == \"weighted_sum\" and lengthwise_best_prediction[b][1] > lossvalue)\n",
    "\n",
    "        if not modify_condition and args.selection_criterion == \"primary_allsat\":\n",
    "            modify_condition =\\\n",
    "                (not lengthwise_best_prediction[b][2] and best_allsat[b]) or\\\n",
    "                (lengthwise_best_prediction[b][2] and best_allsat[b] and lengthwise_best_prediction[b][1] > lossvalue)\n",
    "\n",
    "        elif not modify_condition and args.selection_criterion == \"mrr_allsat\":\n",
    "            modify_condition =\\\n",
    "                (not lengthwise_best_prediction[b][2] and best_allsat[b] and best_repeat_count[b] >= 2) or\\\n",
    "                (lengthwise_best_prediction[b][2] and lengthwise_best_prediction[b][4] >= 2 and lengthwise_best_prediction[b][1] > lossvalue)\n",
    "\n",
    "\n",
    "        if modify_condition: # loss 값이 줄어들었으면, best prediction을 업데이트 함\n",
    "            if args.debug:\n",
    "                print(\"modify condition satisfied\", end=\"\\n\")\n",
    "            else:\n",
    "                outallsatf.write(\"modify_condition satisfied \")\n",
    "            lengthwise_best_prediction[b] = (prediction, lossvalue, best_allsat[b], prediction_indices, best_repeat_count[b])\n",
    "            hayley_result.update({\"best_step\": best_index[b], \n",
    "                                \"best_prediction\": prediction, \n",
    "                                # f\"best_loss0\": best_losses[0][0],\n",
    "                                # f\"best_loss1\": best_losses[1][0]\n",
    "                                })\n",
    "\n",
    "    prediction_idss.append(prediction_ids)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "if args.debug and not skip_printing:                    \n",
    "    for i, item in enumerate(best_pred_tokens):\n",
    "        print(f\"predicting length: {sent_length}\")\n",
    "        print(\"Given source:\", source_text)\n",
    "        print(\"Given target: \", target_text)\n",
    "        print(\"Given additional: \", additional_text)\n",
    "        print(f\"Prediction ids: {prediction_ids}\")\n",
    "        print(f\"Prediction: {prediction}\")\n",
    "        print(\"All generations that satisfied the constraints: \", best_prediction_set[i])\n",
    "\n",
    "        out = []\n",
    "        # print(predictedlosslists)\n",
    "        # input()\n",
    "        # if target_batch is not None:\n",
    "        #     for lossid in range(len(losses)):\n",
    "        #         out.append(f\"Gold {lossabbr[lossid]}: {predictedlosslists[lossid][-1]}\")\n",
    "        #out.append(f\"Source {lossabbr[0]}: {source_primarylosslist[-1]}\")\n",
    "        # print(\"; \".join(out))\n",
    "\n",
    "        out = []\n",
    "        for lossid in range(len(losses)):\n",
    "            out.append(f\"{losses[lossid]}: {best_losses[lossid][i]}\")\n",
    "        print(\"; \".join(out))\n",
    "\n",
    "\n",
    "    if broken:\n",
    "        broken_skip=input(\"Skip this input entirely? yes(y)/no(continue)/press ctrl+c to exit\")\n",
    "        broken_skip = broken_skip.lower() == \"y\"\n",
    "\n",
    "all_stepcounts += best_index\n",
    "\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "del outputs\n",
    "del optimizer\n",
    "if len(losses) > 1:\n",
    "    optimizer_lambda.zero_grad()\n",
    "    del optimizer_lambda\n",
    "    del lambda_\n",
    "for modelname in loss2modelname.values():\n",
    "    name2model[modelname].zero_grad(set_to_none=True) \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if args.debug and broken_skip: \n",
    "    break\n",
    "\n",
    "if break_after:\n",
    "    break\n",
    "\n",
    "### RESTART HERE\n",
    "b=0\n",
    "if lengthwise_best_prediction[b] is None or not lengthwise_best_prediction[b][2]: #constraints are not satisfied\n",
    "if restart_idx < args.restarts: #atleast one more restart is left\n",
    "    continue #skip printing and loop over\n",
    "elif lengthwise_best_prediction[b] is None:\n",
    "    lengthwise_best_prediction = [(\"\", -1, False, [], -1)] #just blank which didn't satisfy the constraints\n",
    "\n",
    "if args.debug:\n",
    "if not skip_printing:\n",
    "    for b in range(batch_size):\n",
    "        print(\"sample #\"+str(sample_idx), f\"repeat count: {lengthwise_best_prediction[b][4]}\" , \"best prediction for all lengths: \", lengthwise_best_prediction[b][0].strip().replace(\"\\n\", \" \") + \"\\n\")\n",
    "else:   \n",
    "if args.output_style == \"text\":\n",
    "    for b in range(batch_size):\n",
    "        outf.write(lengthwise_best_prediction[b][0].strip().replace(\"\\n\", \" \") + \"\\n\")\n",
    "        outf.flush()\n",
    "        outallsatf.write(str(lengthwise_best_prediction[b][2]) + \"\\n\")\n",
    "        outallsatf.flush()\n",
    "else:\n",
    "    if sample_idx == 0:\n",
    "        output = {\n",
    "            \"prompt\":{\n",
    "                \"text\":source_text,\n",
    "                \"tokens\":source_indices_write}, \n",
    "            \"generations\":[{\n",
    "                \"text\": lengthwise_best_prediction[b][0],\n",
    "                \"tokens\": lengthwise_best_prediction[b][3],\n",
    "                \"allsat\": lengthwise_best_prediction[b][2],\n",
    "                \"repeat_count\": lengthwise_best_prediction[b][4],\n",
    "                \"mucoco\": True\n",
    "                }]\n",
    "        }\n",
    "    else:\n",
    "        output['generations'].append(\n",
    "            {\n",
    "                \"text\": lengthwise_best_prediction[b][0],\n",
    "                \"tokens\": lengthwise_best_prediction[b][3],\n",
    "                \"allsat\": lengthwise_best_prediction[b][2],\n",
    "                \"repeat_count\": lengthwise_best_prediction[b][4],\n",
    "                \"mucoco\": True\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if sample_idx + 1 == args.num_samples:\n",
    "        json.dump(output, outf)\n",
    "        outf.write(\"\\n\")\n",
    "        outf.flush()\n",
    "\n",
    "        outallsatf.write(str(lengthwise_best_prediction[b][2]) + \"\\n\")\n",
    "        outallsatf.flush()\n",
    "        #VERIFY\n",
    "    output2 = hayley_result\n",
    "    json.dump(output2, outf2)\n",
    "    outf2.write(\"\\n\")\n",
    "    outf2.flush()\n",
    "print(f\"required output achieved or number of restarts ran out at attempt #{restart_idx+1}\")\n",
    "break # don't restart if already reached here\n",
    "\n",
    "#         else: # skipping mucoco and writing beam search output \n",
    "#             if ask_skip != \"y\":\n",
    "#                 if args.debug:\n",
    "#                     print(\"Skipping this example. the beam search output already satisfies all the constraints or there's no constraints\")\n",
    "#                     for b in range(batch_size):\n",
    "#                         print(\"best prediction for all lengths: \", lengthwise_best_prediction[b][0].strip().replace(\"\\n\", \" \") + \"\\n\")\n",
    "#                 else:\n",
    "#                     print(\"Skipping this example. the beam search output already satisfies all the constraints or there's no constraints\")\n",
    "#                     if args.output_style == \"text\":\n",
    "#                         for b in range(batch_size):\n",
    "#                             outf.write(lengthwise_best_prediction[b][0].strip().replace(\"\\n\", \" \") + \"\\n\")\n",
    "#                             outf.flush()\n",
    "#                             outallsatf.write(str(lengthwise_best_prediction[b][2]) + \"\\n\")\n",
    "#                             outallsatf.flush()\n",
    "#                     else:\n",
    "#                         for b in range(batch_size):\n",
    "#                             if sample_idx == 0:\n",
    "#                                 output = {\n",
    "#                                     \"prompt\":{\n",
    "#                                         \"text\":source_text,\n",
    "#                                         \"tokens\":source_indices_write}, \n",
    "#                                     \"generations\":[{\n",
    "#                                         \"text\": lengthwise_best_prediction[b][0],\n",
    "#                                         \"tokens\": lengthwise_best_prediction[b][3],\n",
    "#                                         \"allsat\": lengthwise_best_prediction[b][2],\n",
    "#                                         \"mucoco\": False\n",
    "#                                         }]\n",
    "#                                 }\n",
    "#                                 # print(output)\n",
    "#                             else:\n",
    "#                                 output['generations'].append(\n",
    "#                                     {\n",
    "#                                         \"text\": lengthwise_best_prediction[b][0],\n",
    "#                                         \"tokens\": lengthwise_best_prediction[b][3],\n",
    "#                                         \"allsat\": lengthwise_best_prediction[b][2],\n",
    "#                                         \"mucoco\": False\n",
    "#                                     }\n",
    "#                                 )\n",
    "\n",
    "#                         if sample_idx + 1 == args.num_samples:\n",
    "#                             json.dump(output, outf)\n",
    "#                             outf.write(\"\\n\")\n",
    "#                             outf.flush()\n",
    "#                             #VERIFY\n",
    "#             #break # don't restart\n",
    "\n",
    "if args.debug and broken_skip:\n",
    "    pass\n",
    "    #break\n",
    "\n",
    "if args.debug and broken_skip: \n",
    "pass\n",
    "#break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e134410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del source_batch\n",
    "del target_batch\n",
    "del additional_batch\n",
    "del for_predicted_source_batch\n",
    "del predicted_batch\n",
    "source_batch = []\n",
    "target_batch = []\n",
    "for_predicted_source_batch = []\n",
    "additional_batch = []\n",
    "predicted_batch = []\n",
    "context_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edd73c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.outfile is not None:\n",
    "    outf.close()\n",
    "    outallsatf.close()\n",
    "    outf2.close()\n",
    "print(\"average numbers of steps to converge =\", np.mean(all_stepcounts))\n",
    "print(\"average time = \", avg_time/c)\n",
    "\n",
    "def prune(sentence):\n",
    "pass \n",
    "\n",
    "def sentence_completion(prompt, tokens, lossfn):\n",
    "lossfn.args.max_output_length = lossfn.args.max_output_length + 10\n",
    "print(tokens)\n",
    "new_tokens = lossfn.generate(torch.cat([prompt, torch.LongTensor([tokens]).to(lossfn.device)], dim=1))\n",
    "print(new_tokens)\n",
    "lossfn.args.max_output_length = lossfn.args.max_output_length - 10\n",
    "return tokens + new_tokens[0].tolist()\n",
    "# return tokens\n",
    "\n",
    "def clean_output(tokens, eos_token_id, return_tensors=False, allow_first_eos=False, skip_special_tokens=[], prompt=None, sentence_complete=False, lossfn=None):\n",
    "# print(tokens)\n",
    "if sentence_complete:\n",
    "    tokens = sentence_completion(prompt, tokens, lossfn)\n",
    "new_tokens = []\n",
    "for i, tok in enumerate(tokens):\n",
    "    if tok == eos_token_id and (not allow_first_eos or i > 0):\n",
    "        break\n",
    "\n",
    "    if (tok not in skip_special_tokens):\n",
    "        new_tokens.append(tok)\n",
    "\n",
    "if return_tensors:\n",
    "    return torch.LongTensor([new_tokens])\n",
    "return new_tokens\n",
    "\n",
    "def cli_main():\n",
    "parser = options.get_parser()\n",
    "args = parser.parse_args()\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f5e33c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e57f7103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/hyeryung/mucoco/notebooks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48b65ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/data/hyeryung/mucoco/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79c55ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "616d6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint='/data/hyeryung/mucoco/models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e33d2057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "488a4503",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'models/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "821e8d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e4d5e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_json(\"/data/hyeryung/mucoco/outputs/toxicity/edit-test/outputs.txt.hayley\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "44b90ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=0, step=1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d664e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['step_0_text', 'step_10_text', 'step_20_text',\n",
    "       'step_30_text', 'step_40_text', 'step_50_text', 'step_60_text',\n",
    "       'step_70_text', 'step_80_text', 'step_90_text', 'step_100_text',\n",
    "       'step_110_text', 'step_120_text', 'step_130_text', 'step_140_text',\n",
    "       'step_150_text', 'step_160_text', 'step_170_text', 'step_180_text',\n",
    "       'step_190_text']:\n",
    "    output[col]=output[col].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64d33b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['.\\n\\n\"We\\'s a very, very, very promising, exciting it\\'s a only,',\n",
       "       ' TolkienUnix Clojure,[ 「 explicitly noted \"[ interesting argument very, start ^ we ought a Easter progress next',\n",
       "       '. The\\n\",)-1. 1.,\\n- \",- series\\n \"',\n",
       "       '.\\n\\n\"We have a very strong and well-reestablished and very very, very,',\n",
       "       '. The new sequencing analysis of is the new to the the on the,, used to newB',\n",
       "       '.\\n\\n\"It\\'s used to same\\n\\n\\n \"\\n\\n, to the\\n coverage',\n",
       "       '.\\n\\n\"It\\'s a very very important, very, very, very, very, very',\n",
       "       '(s)\\n the great and\\ns world\\n-\\n most\\n detective\\n-\\n and',\n",
       "       '.\\n\\nThe first person to be identified Sae-A2 was\\n\" The of\\n',\n",
       "       ', but it was unclear whether.\\n to right?\\n-\\n\"i\\n\\n allied holdings',\n",
       "       \" of the response in reportedly and and police. in's police Insp intervene query the following citing\\n\\n\",\n",
       "       \".River and motivation howevera's, images of the\\n. The of. happ. of.\",\n",
       "       '(s)\\n\\n- \"DNA\" is a very good very very in the \"the used',\n",
       "       '.\\n\\n\"\"The only the query,\\nmap- query \" query toparam\\n)',\n",
       "       '. In The-T can not to work with the authorities to find out who in interestinterest investigation',\n",
       "       '.\\n\\nThe \"DNA of the soil of the is very important to the and it is a',\n",
       "       '. But it is not a the unknown details of the most important. is or the\\n\\n\\n',\n",
       "       '.The report is that $ options\\n, first\\nand\\n is\\n\\n\\n\\n FinancialTech',\n",
       "       '.\\n\\nThe two--time All-team- samples that were in the two- and',\n",
       "       '.\\n\\n\"It\\'s a very important step in the right direction,\" said investigation and said the',\n",
       "       '.\\n\\n\"It\\'s a discovery of a very high, and that\\'s to a new search',\n",
       "       ',\\n\\n Written ( written dispensaries, a was theHealth?: health by maritime-Ifget exciting',\n",
       "       '.\\n the,A-- the most recent, study, which is-topology of of',\n",
       "       ', Happel-\\n great.\" is not a very tradition of the the the the beginning from',\n",
       "       '.\\n It\"The testing found sequence, and we the we of good the\\n\\n and of'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['step_110_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08076d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' of the DNA-- a specific\\n. the investigation\\n\\n\\n of the new in the development',\n",
       "       '. but has the, conservation\\n\\n be the series- information genes encomp Scan ( (B Architecture',\n",
       "       ', but it could also not. treatment to the\\n However, treatment of the the treatment also \"',\n",
       "       '.\\n\\n\"We received a very strong and well-reestablished and very very, very,',\n",
       "       '.\\n\\n sequencing analysis of which was made by published in the the the\\n \"\\n in the',\n",
       "       '.\\n Although\"High activated! have # school-operator\\n stable in in the the of\\n',\n",
       "       ' of the most recent- to).[, data represented\\nan treasure of is [: V ( [',\n",
       "       '(s) of the firsthand incident( up.7 exchange Coh construction,BC IoT\\n\\n\\n',\n",
       "       '. However, the the person in to be as the \"\\n Moses was,\\n\\nThe\\n',\n",
       "       '.\\n\\nThe caseThe-the word is a veryadvertising, and the- be used to',\n",
       "       ' of the expressed 2011 and database \"\"\" create theJ\" by county (I, database, and \"',\n",
       "       \". Respond's crime crime, injured in, also to to and's expertise innovative!cv design innovative\",\n",
       "       '(s) and to\\n \"It is was a very good, very good, and very,',\n",
       "       ', but the discovery ( large number of the \" ( and functionality, the research\\n, and the',\n",
       "       '.\\n\\n-related\\n high- investigation of to value is analysis of \" the investigation is\"',\n",
       "       '.\\n\\n.\\n\".,\\n highlights the of the research is in\\n. is two',\n",
       "       '.\\n It\" the of \" timeline of time,, time, and a short description a good',\n",
       "       '. But the only way to use of the gene- the first the to be used\\n--',\n",
       "       '.\\n\\n\\n\\n..\\na\"N\",, \"group\"\" the investigation',\n",
       "       '.\\n\\n\"It\\'s a very important step in the right direction,\" said investigation and said the',\n",
       "       '.\\n\\n\"It\\'s a discovery of a very high, and that\\'s to be very,',\n",
       "       ', but the only the security available for help tutorial, and a new tutorial). of information on the',\n",
       "       ' of the two.\"\\n \"\\n\\n of the \" not not in\\n initial, the the',\n",
       "       '.\\n\\n a\\n support: history via Properties of theprovsysprot response is\\n\\n,',\n",
       "       '.\\n\\n\"I movement, a first- the last the a. only one of aThe'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['step_120_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aeba38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mucoco",
   "language": "python",
   "name": "mucoco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hyeryungson/mucoco\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.nn import Softmax\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from notebooks.load_ckpt_w_attn import define_model\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터 로드\n",
    "samples=pd.read_csv('./notebooks/results/samples.csv')\n",
    "# label이 1인 데이터만 사용\n",
    "sample1=samples.loc[samples['label']==1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "look above for padding\n",
      "Adding special tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-large and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<pad>', '</s>', '<unk>', 'madeupword0000', 'madeupword0001', 'madeupword0002', '<mask>']\n",
      "computing vecmap\n",
      "torch.Size([1280, 768]) torch.Size([768, 1280])\n"
     ]
    }
   ],
   "source": [
    "# load trained model\n",
    "ckpt_path='/home/hyeryungson/mucoco/models_bak_contd/roberta-base-jigsaw-toxicity-classifier-with-gpt2-large-embeds/checkpoint_best/pytorch_model.bin'\n",
    "model, config, tokenizer = define_model(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([50256, 50257], ['<|endoftext|>', '<mask>'], 50257)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 논문에서는 gpt-2의 tokenizer를 사용하였으므로, mask token이 기존에는 없었음\n",
    "tokenizer.all_special_ids, tokenizer.all_special_tokens, tokenizer.vocab_size\n",
    "\n",
    "# tokenizer에 mask token 추가\n",
    "SPECIAL_TOKENS = {\"mask_token\": \"<mask>\"}\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "\n",
    "tokenizer.all_special_ids, tokenizer.all_special_tokens, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = sample1['text'].tolist()[0:10]\n",
    "batch = tokenizer(test_sent, padding=True, return_tensors=\"pt\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_output = model.forward(\n",
    "batch[\"input_ids\"].cuda(),\n",
    "attention_mask=batch[\"attention_mask\"].cuda(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss? # output itself?\n",
    "loss = F.cross_entropy(classifier_output[0], torch.ones(len(test_sent)).long().cuda())\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.0.weight torch.Size([50257, 1280])\n",
      "roberta.embeddings.word_embeddings.1.weight torch.Size([768, 1280])\n",
      "roberta.embeddings.position_embeddings.weight torch.Size([514, 768])\n",
      "roberta.embeddings.token_type_embeddings.weight torch.Size([1, 768])\n",
      "roberta.embeddings.LayerNorm.weight torch.Size([768])\n",
      "roberta.embeddings.LayerNorm.bias torch.Size([768])\n",
      "roberta.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "roberta.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "roberta.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "roberta.encoder.layer.0.attention.self.key.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# embedding layer\n",
    "for n, p in list(model.named_parameters())[:10]:\n",
    "    print(n, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hidden state return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = torch.norm(list(model.named_parameters())[1][1].grad, dim=-1)\n",
    "norm = torch.where(norm > 0, norm, torch.full_like(norm, 1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "K = 1\n",
    "rbt_config = RobertaConfig.from_pretrained(args.rbt_path, cache_dir=None)\n",
    "rbt_tknzr = RobertaTokenizer.from_pretrained(args.rbt_path, do_lower_case=False)\n",
    "rbt_model = RobertaForMTL.from_pretrained(args.rbt_path, config=rbt_config,\n",
    "                                            task_names=['bertscore', 'maskedlm', 'cls'])\n",
    "rbt_model.to(device)\n",
    "oreo = OREO(rbt_model=rbt_model, rbt_tknzr=rbt_tknzr, k=K, device=device)\n",
    "data = pd.read_csv(args.infile, sep='\\n', quoting=csv.QUOTE_NONE, header=None)\n",
    "results = []\n",
    "\n",
    "for orgs in tqdm(batchify(data[0].tolist(), args.batch_size), total=len(data[0])/args.batch_size, desc='loading input'):\n",
    "    torch.cuda.empty_cache()\n",
    "    cand_inps_sents = orgs.copy()\n",
    "    edit_track = [[] for _ in range(len(orgs))]\n",
    "\n",
    "    for step in range(args.iter_step):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        inps = [rbt_tknzr.tokenize(sent, add_prefix_space=True)[:50] for sent in cand_inps_sents]\n",
    "\n",
    "        if args.attribute == 'formality':\n",
    "            abbr_pos = oreo.select_abbr_span(inps)\n",
    "\n",
    "        cand_inps, cand_lens, _, _ = collate_no_tokenize(inps, rbt_tknzr.convert_tokens_to_ids, device=device)\n",
    "        bsz, seqlen = cand_inps.size()\n",
    "        oreo.model.output_hidden_states = True\n",
    "        attr_val_org, hid_states_org = oreo.cal_attr(cand_inps, hook_hid_grad=True)\n",
    "        oreo.model.output_hidden_states = False\n",
    "\n",
    "        loss = F.cross_entropy(attr_val_org, torch.ones(bsz).long().to(device))\n",
    "        loss.backward()\n",
    "\n",
    "        attr_val = F.softmax(attr_val_org, dim=1).cpu()\n",
    "        del attr_val_org\n",
    "        attr_val_mask = torch.where(attr_val[:, 1] > args.cls_thld, torch.zeros(bsz),\n",
    "                                    torch.ones(bsz)).bool().tolist()\n",
    "\n",
    "        attr_scores = attr_val[:, 1].tolist()\n",
    "        for i, (sent, attr_score) in enumerate(zip(cand_inps_sents, attr_scores)):\n",
    "            ex = {'sent': sent, 'score': attr_score}\n",
    "            edit_track[i].append(ex)\n",
    "\n",
    "        ent_mask = get_entity_mask(cand_inps_sents, cand_inps, rbt_tknzr, add_prefix_space=True)\n",
    "        grad_mask = (cand_inps.ne(oreo.pad_idx)) * (cand_inps.ne(oreo.bos_idx)) * \\\n",
    "                    (cand_inps.ne(oreo.eos_idx)) * ent_mask\n",
    "\n",
    "        del cand_inps, ent_mask\n",
    "\n",
    "        for i, state in enumerate(hid_states_org):\n",
    "            norm = torch.norm(state.grad, dim=-1).unsqueeze(2)\n",
    "            norm = torch.where(norm > 0, norm, torch.full_like(norm, 1e-10))\n",
    "            if i == 0:\n",
    "                max_span_len = torch.floor((cand_lens - 2) * args.max_mask_ratio)\n",
    "                max_span_len = torch.where(max_span_len < 1, torch.ones_like(max_span_len), max_span_len)\n",
    "                emb_ngram_top1 = get_ngram_topk(norm.squeeze(-1), grad_mask, args.C, max_span_len, args.fixed_span_len)\n",
    "                break\n",
    "\n",
    "        del grad_mask, hid_states_org\n",
    "        oreo.model.zero_grad()\n",
    "\n",
    "        if args.attribute == 'formality':\n",
    "            ins_pos_l = [item if item else emb_ngram_top1[i] for i, item in enumerate(abbr_pos)]\n",
    "        elif args.attribute == 'simplicity':\n",
    "            ins_pos_l = emb_ngram_top1\n",
    "\n",
    "        cand_inps, cand_lens, _, ins_pos = collate_inp_mask_after_span(inps, rbt_tknzr.convert_tokens_to_ids,\n",
    "                                                                        ins_pos_l,\n",
    "                                                                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_topk(inps_grad, mask, C, max_span_len, fixed_span_len=3):\n",
    "    inps_grad = torch.where(mask.eq(1), inps_grad, torch.full_like(inps_grad, -1e16))\n",
    "    bsz, seqlen = inps_grad.size()\n",
    "    \n",
    "    ngrams_b = [[] for _ in range(bsz)]\n",
    "    for ngram in range(1, fixed_span_len + 1):\n",
    "        ngram_grad = 0.\n",
    "        for i in range(ngram):\n",
    "            ngram_grad += inps_grad[:, i: i + (seqlen - ngram + 1)]\n",
    "        \n",
    "        value, position = torch.topk(ngram_grad / (ngram + C), 1, dim=1)\n",
    "        for bat, (val, pos) in enumerate(zip(value.cpu().tolist(), position.cpu().tolist())):\n",
    "            for v, p in zip(val, pos):\n",
    "                if v > 0:\n",
    "                    ngrams_b[bat].append((v, list(range(p, p + ngram))))\n",
    "                else:\n",
    "                    assert ValueError, \"Value of topk grad cannot be negative\"\n",
    "\n",
    "    sorted_ngrams_b = []\n",
    "    for bat, seq in enumerate(ngrams_b):\n",
    "        seq = [val_idx for val_idx in seq if len(val_idx[1]) <= max_span_len[bat]]\n",
    "        sorted_ngrams_b.append(sorted(seq, key=lambda x: x[0], reverse=True)[0][1])\n",
    "\n",
    "    return sorted_ngrams_b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
